[
  {
    "objectID": "vignettes/MHW_MCS_horizonplots.html",
    "href": "vignettes/MHW_MCS_horizonplots.html",
    "title": "Horizon plots for MHW/MCS (Placeholder)",
    "section": "",
    "text": "This page is a placeholder in the standalone SAMOS R site.\n\n\n\nCitationBibTeX citation:@online{a._j.,\n  author = {A. J. , Smit},\n  title = {Horizon Plots for {MHW/MCS} {(Placeholder)}},\n  url = {http://samos-r.netlify.app/vignettes/MHW_MCS_horizonplots.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S Horizon plots for MHW/MCS (Placeholder). http://samos-r.netlify.app/vignettes/MHW_MCS_horizonplots.html."
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#questions-1",
    "href": "tasks/BCB744_Task_G.html#questions-1",
    "title": "BCB744 Task G",
    "section": "2.1 Questions 1",
    "text": "2.1 Questions 1\nWhy should we not just apply t-tests once per each of the pairs of comparisons we want to make? (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-2",
    "href": "tasks/BCB744_Task_G.html#question-2",
    "title": "BCB744 Task G",
    "section": "2.2 Question 2",
    "text": "2.2 Question 2\n\nWhat does the outcome say about the chicken masses? Which ones are different from each other? (/2)\nDevise a graphical display of this outcome. (/4)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-3",
    "href": "tasks/BCB744_Task_G.html#question-3",
    "title": "BCB744 Task G",
    "section": "2.3 Question 3",
    "text": "2.3 Question 3\nLook at the help file for the TukeyHSD() function to better understand what the output means.\n\nHow does one interpret the results? What does this tell us about the effect that that different diets has on the chicken weights at Day 21? (/3)\nFigure out a way to plot the Tukey HSD outcomes in ggplot. (/10)\nWhy does the ANOVA return a significant result, but the Tukey test shows that not all of the groups are significantly different from one another? (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-4",
    "href": "tasks/BCB744_Task_G.html#question-4",
    "title": "BCB744 Task G",
    "section": "2.4 Question 4",
    "text": "2.4 Question 4\n\nHow is time having an effect? (/3)\nWhat hypotheses can we construct around time? (/2)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-5",
    "href": "tasks/BCB744_Task_G.html#question-5",
    "title": "BCB744 Task G",
    "section": "2.5 Question 5",
    "text": "2.5 Question 5\n\nWhat do you conclude from the above series of ANOVAs? (/3)\nWhat problem is associated with running multiple tests in the way that we have done here? (/2)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-6",
    "href": "tasks/BCB744_Task_G.html#question-6",
    "title": "BCB744 Task G",
    "section": "2.6 Question 6",
    "text": "2.6 Question 6\n\nWrite out the hypotheses for this ANOVA. (/2)\nWhat do you conclude from the above ANOVA? (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-7",
    "href": "tasks/BCB744_Task_G.html#question-7",
    "title": "BCB744 Task G",
    "section": "2.7 Question 7",
    "text": "2.7 Question 7\n\nWhat question are we asking with the above line of code? (/3)\nWhat is the answer? (/2)\nWhy did we wrap Time in as.factor()? (/2)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-8",
    "href": "tasks/BCB744_Task_G.html#question-8",
    "title": "BCB744 Task G",
    "section": "2.8 Question 8",
    "text": "2.8 Question 8\nHow do these results differ from the previous set? (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_G.html#question-9",
    "href": "tasks/BCB744_Task_G.html#question-9",
    "title": "BCB744 Task G",
    "section": "2.9 Question 9",
    "text": "2.9 Question 9\nYikes! That is a massive amount of results. What does all of this mean, and why is it so verbose? (/5)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-1",
    "href": "tasks/BCB744_Task_E.html#question-1",
    "title": "BCB744 Task E",
    "section": "2.1 Question 1",
    "text": "2.1 Question 1\n\nExplain the output of dimnames() when applied to the penguins dataset. (/2)\nExplain the output of str() when applied to the penguins dataset. (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-2",
    "href": "tasks/BCB744_Task_E.html#question-2",
    "title": "BCB744 Task E",
    "section": "2.2 Question 2",
    "text": "2.2 Question 2\nHow would you manually calculate the mean value for the normal_data we generated in the lecture? (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-3",
    "href": "tasks/BCB744_Task_E.html#question-3",
    "title": "BCB744 Task E",
    "section": "2.3 Question 3",
    "text": "2.3 Question 3\nFind the faithful dataset and describe both variables in terms of their measures of central tendency. Include graphs in support of your answers (use ggplot()), and conclude with a brief statement about the data distribution. (/10)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-4",
    "href": "tasks/BCB744_Task_E.html#question-4",
    "title": "BCB744 Task E",
    "section": "2.4 Question 4",
    "text": "2.4 Question 4\nManually calculate the variance and SD for the normal_data we generated in the lecture. Make sure your answer is the same as those reported there. (/5)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-5",
    "href": "tasks/BCB744_Task_E.html#question-5",
    "title": "BCB744 Task E",
    "section": "2.5 Question 5",
    "text": "2.5 Question 5\nWrite a few lines of code to demonstrate that the \\((0-0.25]\\), \\((0.25-0.5]\\), \\((0.5-0.75]\\),\\((0.75-1]\\) quantiles of the normal_data we generated in the lecture indeed conform to the formal definition for what quantiles are. I.e., show manually how you can determine that 25% of the observations indeed fall below -0.66 for the normal_data. Explain the rationale to your approach. (/10)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-6",
    "href": "tasks/BCB744_Task_E.html#question-6",
    "title": "BCB744 Task E",
    "section": "2.6 Question 6",
    "text": "2.6 Question 6\nWhy is it important to consider the grouping structures that might be present within our datasets? (/2)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-7",
    "href": "tasks/BCB744_Task_E.html#question-7",
    "title": "BCB744 Task E",
    "section": "2.7 Question 7",
    "text": "2.7 Question 7\nExplain the output of summary() when applied to the penguins dataset. (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_E.html#question-8",
    "href": "tasks/BCB744_Task_E.html#question-8",
    "title": "BCB744 Task E",
    "section": "3.1 Question 8",
    "text": "3.1 Question 8\n\nUsing a tidy workflow, assemble a summary table of the palmerpenguins dataset that has a similar appearance as that produced by psych::describe(penguins). (/5)\n\nFor bonus marks (which will not count anything) of up to 10% added to Task E, apply a beautiful, creative styling to the table using the kable package. Try and make it as publication ready as possible. Refer to a few journal articles to see how to professionally typeset tables.\n\nStill using the palmerpenguins dataset, perform an exploratory data analysis to investigate the relationship between penguin species, their morphological traits (bill length and bill depth, flipper length, and body mass). Employ the tidyverse approaches learned earlier in the module to explore the data, account for the grouping structures present within the dataset. (/10)\nProvide visualisations (use Figure 4 as inspiration) and summary statistics to support your findings and elaborate on any observed patterns or trends. (/10)\nEnsure your presentation is professional and adhere to the standards required by scientific publications. State the major aims of your analysis and the patterns you seek. Using the combined findings from the EDA and the figures produced here, discuss the findings in a formal Results section. (/5)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task E"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_C.html#question-1",
    "href": "tasks/BCB744_Task_C.html#question-1",
    "title": "BCB744 Task C",
    "section": "2.1 Question 1",
    "text": "2.1 Question 1\n\n\n\n\n\n\n\n\nFigure 1: Styled Southern Africa map with annotations and scale.\n\n\n\n\n\nAesthetically improve the map you created in this lecture (above example) and add a title and subtitle. Also adjust it to show appropriately labelled axes. (/15)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task C"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_C.html#question-2",
    "href": "tasks/BCB744_Task_C.html#question-2",
    "title": "BCB744 Task C",
    "section": "2.2 Question 2",
    "text": "2.2 Question 2\nAdd a the capital city/town of each province to the map using geom_point() and ensure the place name is correctly associated with its point. (/5)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task C"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_C.html#question-3",
    "href": "tasks/BCB744_Task_C.html#question-3",
    "title": "BCB744 Task C",
    "section": "3.1 Question 3",
    "text": "3.1 Question 3\nWhy does the map region extend so far south of the southern tip of Africa when we requested only the countries South Africa, Mozambique, Namibia, Zimbabwe, Botswana, Lesotho, and Eswatini? (/1)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task C"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_C.html#question-4",
    "href": "tasks/BCB744_Task_C.html#question-4",
    "title": "BCB744 Task C",
    "section": "3.2 Question 4",
    "text": "3.2 Question 4\nHow do we fix this to plot a more sensible map of the region? (/1)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task C"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_C.html#question-5",
    "href": "tasks/BCB744_Task_C.html#question-5",
    "title": "BCB744 Task C",
    "section": "3.3 Question 5",
    "text": "3.3 Question 5\nWhat does st_buffer(0.4) do? (/1)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task C"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_C.html#question-6",
    "href": "tasks/BCB744_Task_C.html#question-6",
    "title": "BCB744 Task C",
    "section": "3.4 Question 6",
    "text": "3.4 Question 6\nWith the map that we created in the lecture with Natural Earth and sf (i.e., you can start with the script in the lecture), zoom into False Bay, the Cape Peninsula. Add the location of the Cape Town city centre to the map usinggeom_point(). Ensure the point is correctly associated with the city name. Ensure the map is correctly labelled and has a title, and is as close to publication quality as you can make it. Pay close attention to the axes (breaks, limits) and the map extent. Some marks are allocated to a pleasing and sensible aesthetic appearance. (/20)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task C"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-1",
    "href": "tasks/BCB744_Task_B.html#question-1",
    "title": "BCB744 Task B",
    "section": "Question 1",
    "text": "Question 1\nCreate a scatterplot of bill_length_mm against bill_depth_mm for Adelie penguins on Biscoe island. (/10)\nAnswer\n\nlibrary(palmerpenguins) # ✓ \nlibrary(tidyverse) # ✓ \ndata(penguins)\nhead(penguins)\n\nR&gt; # A tibble: 6 × 8\nR&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nR&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\nR&gt; 1 Adelie  Torgersen           39.1          18.7               181        3750\nR&gt; 2 Adelie  Torgersen           39.5          17.4               186        3800\nR&gt; 3 Adelie  Torgersen           40.3          18                 195        3250\nR&gt; 4 Adelie  Torgersen           NA            NA                  NA          NA\nR&gt; 5 Adelie  Torgersen           36.7          19.3               193        3450\nR&gt; 6 Adelie  Torgersen           39.3          20.6               190        3650\nR&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\npenguins %&gt;% # ✓ \n  filter(island == \"Biscoe\" & species == \"Adelie\") %&gt;%  # ✓ \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +  # ✓ \n  geom_point() +  # ✓ \n  labs(title = \"Adelie Penguins on Biscoe Island\",  # ✓ \n         x = \"Bill Length (mm)\", # ✓ \n         y = \"Bill Depth (mm)\") # ✓ \n\n\n\n\n\n\n\nFigure 1: Adelie bill length vs. bill depth on Biscoe Island.\n\n\n\n\n\nThe scatter reveals a moderately ordered morphometric relationship, i.e., individuals with longer bills tend, generally, to exhibit greater bill depth, though the association is not tightly constrained. The dispersion suggests intraspecific variability rather than discrete clustering, with no obvious sub-grouping. These groupings might be revealed once we take into acocunt the additional categorical structure in the data.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-2",
    "href": "tasks/BCB744_Task_B.html#question-2",
    "title": "BCB744 Task B",
    "section": "Question 2",
    "text": "Question 2\nCreate histograms of bill_length_mm for Adelie penguins on all three islands (one figure per island). Save each figure as a separate R object which you can later reuse. Again for Adelie penguins, create a boxplot for bill_length_mm showing all the data on one plot. Save it too as an R object. Combine the four saved figures into one figure using ggarrange(). (/25)\nAnswer\n\nlibrary(ggpubr) # ✓\n\n# Create histograms\nadelie_biscoe &lt;- penguins %&gt;% # ✓ x 5\n  filter(island == \"Biscoe\" & species == \"Adelie\") %&gt;% \n  ggplot(aes(x = bill_length_mm)) + \n  geom_histogram() + \n  labs(title = \"Adelie Penguins on Biscoe Island\", \n       x = \"Bill Length (mm)\", \n       y = \"Frequency\")\n\nadelie_dream &lt;- penguins %&gt;% # ✓ x 5\n  filter(island == \"Dream\" & species == \"Adelie\") %&gt;% \n  ggplot(aes(x = bill_length_mm)) + \n  geom_histogram() + \n  labs(title = \"Adelie Penguins on Dream Island\", \n       x = \"Bill Length (mm)\", \n       y = \"Frequency\")\n\nadelie_torgersen &lt;- penguins %&gt;% # ✓ x 5\n  filter(island == \"Torgersen\" & species == \"Adelie\") %&gt;% \n  ggplot(aes(x = bill_length_mm)) + \n  geom_histogram() + \n  labs(title = \"Adelie Penguins on Torgersen Island\", \n       x = \"Bill Length (mm)\", \n       y = \"Frequency\")\n\n# Create boxplot # ✓ x 5\nadelie_boxplot &lt;- penguins %&gt;% \n  filter(species == \"Adelie\") %&gt;% \n  ggplot(aes(x = island, y = bill_length_mm)) + \n  geom_boxplot() + \n  labs(title = \"Adelie Penguins Bill Length Boxplot\", \n       x = \"Island\", \n       y = \"Bill Length (mm)\")\n\n# Combine figures # ✓ x 1\nggarrange(adelie_biscoe, adelie_dream, adelie_torgersen, adelie_boxplot, \n          ncol = 2, nrow = 2)\n\n\n\n\n\n\n\nFigure 2: Adelie bill length histograms by island plus overall boxplot.\n\n\n\n\n\nThe histograms indicate island-level differentiation in bill length structure. Biscoe birds seem display a distribution shifted toward longer bills, whereas Dream and Torgersen populations centre on shorter modal values. Overlap is extensive across all islands, but the positional offsets imply geographic structuring (whether ecological or genetic) within Adelie morphology.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-3",
    "href": "tasks/BCB744_Task_B.html#question-3",
    "title": "BCB744 Task B",
    "section": "Question 3",
    "text": "Question 3\nCreate a scatter plot of flipper_length_mm against body_mass_g and use facet_wrap() to create separate panels for each island (combine all species). Plot the three species as distinct point shapes, and map a continuous colour scale to bill_length_mm. Add a best‑fit straight line with 95% confidence intervals through the points, ignoring the effect of species. Take into account which variable best belongs on x and y. Describe your findings. (/10)\nAnswer\n\npenguins %&gt;% # ✓ x 7\n  ggplot(aes(x = body_mass_g, y = flipper_length_mm)) + \n  geom_point(aes(shape = species, colour = bill_length_mm)) + \n  scale_colour_viridis_c() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  facet_wrap(~island) + \n  labs(title = \"Flipper Length vs Body Mass\", \n       x = \"Body Mass (g)\",\n       y = \"Flipper Length (mm)\",\n       colour = \"Bill length (mm)\")\n\n\n\n\n\n\n\nFigure 3: Flipper length vs. body mass faceted by island with species shapes and bill-length colour scale.\n\n\n\n\n\n\nThe body_mass_g variable is best suited to the x-axis as it is the independent variable. The flipper_length_mm variable is best suited to the y-axis as it is the dependent variable.\n✓ For all penguin species, the flipper_length_mm and body_mass_g variables show a positive correlation, with larger penguins having longer flippers, higher body masses.\n✓ The colour scale shows that bill length tends to increase with body mass, especially for Gentoo penguins.\n✓ Species are clearly separated by shape, and Gentoo penguins occupy the largest body‑mass and flipper‑length ranges.\n\nOr something like:\nAcross all islands, flipper length scales positively with body mass to produce an oblique band of points that points to a biomechanical relationship between flipper properties and body size. Species separation is legible through shapes assigned to the species: Gentoo individuals occupy the upper mass:flipper relationship, while Adelie cluster at lower magnitudes. The colour gradient adds a third dimension—bill length increases along the same mass trajectory, reinforcing the impression of integrated body scaling rather than isolated trait enlargement.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-4",
    "href": "tasks/BCB744_Task_B.html#question-4",
    "title": "BCB744 Task B",
    "section": "Question 4",
    "text": "Question 4\nCreate a scatter plot of bill_length_mm and body_mass_g and use facet_grid() to create separate panels for each species and island. Map bill_length_mm to a continuous colour scale that you customise yourself (do not use the default palette). (/10)\nAnswer\n\ngrid_plt &lt;-  penguins %&gt;% # ✓ x 7\n  ggplot(aes(x = body_mass_g, y = bill_length_mm, colour = bill_length_mm)) + \n  geom_point() + \n  scale_colour_gradient(low = \"lightyellow\", high = \"darkred\") +\n  facet_grid(species ~ island) + \n  labs(title = \"Bill Length vs Body Mass\", \n       x = \"Body Mass (g)\",\n       y = \"Bill Length (mm)\",\n       colour = \"Bill length (mm)\")\ngrid_plt\n\n\n\n\n\n\n\nFigure 4: Bill length vs. body mass faceted by species and island with a custom colour scale.\n\n\n\n\n\nThe faceted grid disentangles interspecific from geographic effects. Within each species panel, bill length rises with body mass, though the slope and spread vary. Gentoo shows the steepest scaling and the largest absolute values, while Adelie retains a compressed range. The customised colour gradient intensifies this visual interpretation since it visually thickens the upper mass–length zones and making trait amplification more legible.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-5",
    "href": "tasks/BCB744_Task_B.html#question-5",
    "title": "BCB744 Task B",
    "section": "Question 5",
    "text": "Question 5\nUsing the figure created in point 4, also show the effect of sex and add a best-fit straight line. Explain the findings. (/10)\nAnswer\n\ngrid_plt +  # ✓ x 7\n  geom_point(aes(shape = sex)) +\n  geom_smooth(method = \"lm\", se = TRUE, colour = \"black\")\n\n\n\n\n\n\n\n\n\n✓ The bill_length_mm and body_mass_g variables show a positive correlation, with larger penguins having longer bills, higher body masses.\n✓ The sex variable appears to have an effect on the relationship between bill_length_mm and body_mass_g, with male penguins tending to be heavier with longer bill lengths.\n✓ There also appears to be differences in the relationship between bill_length_mm and body_mass_g between the different species and islands.\n\nOr…\nIntroducing sex causes dimorphism to become visible within species–island panels. Males tend to populate the upper mass and bill-length ranges, producing a stratified layering of points. The fitted regression line, which was calculated without sex partitioning, still shows a positive incline, but the sex-coded distribution implies that part of the apparent size scaling is mediated through sexual size differentiation rather than uniform growth alone.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-6",
    "href": "tasks/BCB744_Task_B.html#question-6",
    "title": "BCB744 Task B",
    "section": "Question 6",
    "text": "Question 6\nWhat are the benefits of using faceting in data visualisation? (/3)\nAnswer\n\n✓ Faceting allows for the visualisation of multiple relationships in a single plot, making it easier to compare relationships between different groups.\n✓ Faceting can help to identify patterns and trends in the data that may not be immediately obvious when looking at the data as a whole.\n✓ Faceting can help to identify differences in relationships between different groups, such as species, islands or allowing for more detailed analysis of the data.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-7",
    "href": "tasks/BCB744_Task_B.html#question-7",
    "title": "BCB744 Task B",
    "section": "Question 7",
    "text": "Question 7\nUse the built-in ToothGrowth dataset (guinea pig tooth length) to create a scatter plot of len against dose, coloured by supp, and faceted by supp. Add a best‑fit straight line with a 95% confidence interval. (/10)\nAnswer\n\ndata(ToothGrowth) # ✓\n\nToothGrowth %&gt;% # ✓ x 6\n  ggplot(aes(x = dose, y = len, colour = supp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = TRUE) + \n  facet_wrap(~supp) + \n  labs(title = \"Tooth Length vs Dose by Supplement\",\n       x = \"Dose (mg/day)\",\n       y = \"Tooth length\")\n\n\n\n\n\n\n\nFigure 5: Tooth length vs. dose, coloured and faceted by supplement.\n\n\n\n\n\n\n✓ dose is the explanatory variable, so it belongs on the x‑axis; len is the response variable.\n✓ Tooth length increases with dose for both supplements.\n\nTooth length increases systematically with dose, producing increasing point clouds in both supplement panels. The regressions reinforce a dose-dependent growth response. But separation between supplements remains, such that at comparable doses, orange juice (OJ) tends to yield longer teeth than vitamin C (VC), implying differential bioavailability or metabolic uptake rather than a uniform pharmacological effect.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-8",
    "href": "tasks/BCB744_Task_B.html#question-8",
    "title": "BCB744 Task B",
    "section": "Question 8",
    "text": "Question 8\nCreate histograms of len for each dose (all supplements together) using facet_wrap(). (/6)\nAnswer\n\nToothGrowth %&gt;% # ✓ x 4\n  ggplot(aes(x = len)) + \n  geom_histogram(bins = 8) + \n  facet_wrap(~dose) + \n  labs(title = \"Tooth Length by Dose\",\n       x = \"Tooth length\",\n       y = \"Count\")\n\n\n\n\n\n\n\nFigure 6: Tooth length distributions by dose.\n\n\n\n\n\nDose stratification adds a new view of the distributions. The lowest dose clusters tightly around shorter tooth lengths, while intermediate and high doses shift the distribution to the right and broadens it. This progressive shift of modal peaks indicates a graded biological response rather than a threshold effect, and variance expansion at higher doses suggests individual heterogeneity in treatment uptake.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-9",
    "href": "tasks/BCB744_Task_B.html#question-9",
    "title": "BCB744 Task B",
    "section": "Question 9",
    "text": "Question 9\nCreate boxplots of len by dose and facet by supp (one panel per supplement). (/8)\nAnswer\n\nToothGrowth %&gt;% # ✓ x 5\n  ggplot(aes(x = factor(dose), y = len)) + \n  geom_boxplot() + \n  facet_grid(supp ~ .) + \n  labs(title = \"Tooth Length by Dose and Supplement\",\n       x = \"Dose (mg/day)\",\n       y = \"Tooth length\")\n\n\n\n\n\n\n\nFigure 7: Tooth length by dose, separated by supplement.\n\n\n\n\n\nMedian tooth length rises with dose in both supplements, but the vertical separation between OJ and VC remains visible. OJ panels show higher medians and often wider interquartile ranges at equivalent doses. The boxplot geometry thus reiterates the double structure: dose drives the primary vertical ascent, and supplement modulates its amplitude.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-10",
    "href": "tasks/BCB744_Task_B.html#question-10",
    "title": "BCB744 Task B",
    "section": "Question 10",
    "text": "Question 10\nCalculate the mean ± SD of len for each combination of dose and supp, then plot the means with error bars. (/10)\nAnswer\n\ntg_summary &lt;- ToothGrowth %&gt;% # ✓ x 7\n  group_by(dose, supp) %&gt;% \n  summarise(mean_len = mean(len),\n            sd_len = sd(len),\n            .groups = \"drop\")\n\ntg_summary %&gt;% \n  ggplot(aes(x = factor(dose), y = mean_len, colour = supp, group = supp)) + \n  geom_point(size = 2) + \n  geom_line() + \n  geom_errorbar(aes(ymin = mean_len - sd_len, ymax = mean_len + sd_len), width = 0.15) + \n  labs(title = \"Mean Tooth Length ± SD\",\n       x = \"Dose (mg/day)\",\n       y = \"Mean tooth length\")\n\n\n\n\n\n\n\nFigure 8: Mean tooth length ± SD by dose and supplement.\n\n\n\n\n\nThe lines show parallel increases across doses and confirms a consistent treatment response. Error bars widen modestly at higher doses, thus showing increased variability. OJ maintains an advantage across all doses, seen in its mean values sitting above those of VC. This reinforces the inference of supplement-specific efficacy layered atop the general dose effect.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-11",
    "href": "tasks/BCB744_Task_B.html#question-11",
    "title": "BCB744 Task B",
    "section": "Question 11",
    "text": "Question 11\nCreate a violin plot of len by dose, filled by supp, and facet by supp. (/8)\nAnswer\n\nToothGrowth %&gt;% # ✓ x 5\n  ggplot(aes(x = factor(dose), y = len, fill = supp)) + \n  geom_violin(trim = FALSE) + \n  facet_wrap(~supp) + \n  labs(title = \"Tooth Length Distributions by Dose\",\n       x = \"Dose (mg/day)\",\n       y = \"Tooth length\")\n\n\n\n\n\n\n\nFigure 9: Tooth length distributions by dose and supplement.\n\n\n\n\n\nThe violins reveal distributional nuances not visible in the boxplots. Lower doses produce narrow, compact shapes; higher doses widen and elongate. This suggests both upward displacement and variance inflation. OJ violins often extend further into higher length ranges, their density ridges thickening above those of VC, visually encoding supplement divergence.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-12",
    "href": "tasks/BCB744_Task_B.html#question-12",
    "title": "BCB744 Task B",
    "section": "Question 12",
    "text": "Question 12\nCreate a small summary table showing the number of observations for each combination of dose and supp. (/6)\nAnswer\n\nToothGrowth %&gt;% # ✓ x 3\n  count(dose, supp)\n\nR&gt;   dose supp  n\nR&gt; 1  0.5   OJ 10\nR&gt; 2  0.5   VC 10\nR&gt; 3  1.0   OJ 10\nR&gt; 4  1.0   VC 10\nR&gt; 5  2.0   OJ 10\nR&gt; 6  2.0   VC 10\n\n\nThe summary table shows neatly balanced sampling, such that each dose–supplement combination contains identical counts. This removes sample-size bias from visual comparisons and allows distributional and mean differences to be interpreted as biological rather than artefactual.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_B.html#question-13",
    "href": "tasks/BCB744_Task_B.html#question-13",
    "title": "BCB744 Task B",
    "section": "Question 13",
    "text": "Question 13\nBriefly describe two patterns you observe in any of the figures above. (/4)\nAnswer\n\n✓ Tooth length increases as dose increases for both supplements.\n✓ At the same dose, the OJ supplement tends to have higher tooth lengths than VC, especially at lower doses.\n\nTwo recurrent structures dominate the figures. First, dose exerts a monotonic positive effect on tooth growth across all representations (scatter, box, violin, and summary mean plots). Second, supplement type modulates this trajectory, so that OJ consistently produces greater tooth length than VC at equivalent doses, with the disparity most visible at lower concentrations where treatment sensitivity appears highest.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task B"
    ]
  },
  {
    "objectID": "pages/kaggle_earthquakes.html",
    "href": "pages/kaggle_earthquakes.html",
    "title": "Kaggle Earthquakes (Placeholder)",
    "section": "",
    "text": "This page is a placeholder in the standalone SAMOS R site.\nIf you need the full Kaggle earthquake workflow, we can port it into this repo later.\n\n\n\nCitationBibTeX citation:@online{a._j.,\n  author = {A. J. , Smit},\n  title = {Kaggle {Earthquakes} {(Placeholder)}},\n  url = {http://samos-r.netlify.app/pages/kaggle_earthquakes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S Kaggle Earthquakes (Placeholder). http://samos-r.netlify.app/pages/kaggle_earthquakes.html."
  },
  {
    "objectID": "pages/assessment_theory.html",
    "href": "pages/assessment_theory.html",
    "title": "Assessment Theory",
    "section": "",
    "text": "This page is a placeholder in the standalone SAMOS R site.\n\n\n\nCitationBibTeX citation:@online{a._j.,\n  author = {A. J. , Smit},\n  title = {Assessment {Theory}},\n  url = {http://samos-r.netlify.app/pages/assessment_theory.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S Assessment Theory. http://samos-r.netlify.app/pages/assessment_theory.html."
  },
  {
    "objectID": "intro_r/r_markdown_example.html#data",
    "href": "intro_r/r_markdown_example.html#data",
    "title": "R Markdown and Quarto Demo",
    "section": "2.1 Data",
    "text": "2.1 Data\nThe dataset used in this study is the airquality dataset from R, which contains daily air quality measurements in New York from May to September 1973. The dataset includes variables such as ozone levels, solar radiation, wind speed, and temperature."
  },
  {
    "objectID": "intro_r/r_markdown_example.html#analysis",
    "href": "intro_r/r_markdown_example.html#analysis",
    "title": "R Markdown and Quarto Demo",
    "section": "2.2 Analysis",
    "text": "2.2 Analysis\nThe R script in the code chunk further explores the impact of temperature on ozone level. All analyses were done in R (R Core Team 2017).\nThis is bold text. This is italicised text.\n\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\n\n\nFigure 1: Temperature and ozone level."
  },
  {
    "objectID": "intro_r/18-base_r.html",
    "href": "intro_r/18-base_r.html",
    "title": "18. Base R Primer",
    "section": "",
    "text": "Please note that the following chapter departs from the syntax employed by the tidyverse, as utilised throughout this workshop, in favour of the base R syntax. This may be changed in the future, but has been left for now in order to better highlight the fundamental machinations of the R language, upon which the tidyverse is based.\n\n1 Dataframes\nThe ‘workhorse’ data-containing structures you will use extensively in R are called dataframes. In fact, almost all of the work you do in R will be done directly with dataframes, will involve converting data into a dataframe. A dataframe is used for storing data as tables or with a table defined by a collection of vectors of similar or dissimilar data types but all of the same length. Do not worry if any of those terms are unknown or daunting. We will cover them in detail just now. But first we need to see what a dataframe looks like in order to provide context for all of the parts they consist of. After we have covered all of the terms used for data in R we will learn some methods of creating our own dataframes.\nTo load a dataframe into R is quite simple when the data are already in the ‘.Rdata’ format. Let us load a small dataframe that was prepared for this class and see. The file extension ‘.Rdata’ does not mean necessarily that the data are in a dataframe (table) format. This file extensions is actually a form of data compression unique to R and could hold anything from a single letter to the results of a complex species distribution model. For the following line of code to work we must make sure we are in the ‘Intro_R_Workshop’ project.\n\nload(here::here(\"data\", \"BCB744\", \"intro_data.Rdata\"))\n\nUpon loading the data frame we see in the Environment tab that there is a little blue circle next to our object. If we click on that we see a summary of each column. First it says what the data type for that column is and then shows the first several values therein.\nIf you click on the ‘intro_data’ word in your Environment tab it will open it in your Source Editor and allow you to click on the columns to organise them by ascending or descending order. Note that this does not change the dataframe, it is only a visual aid.\n\n\n2 Basic Data Types\nThere are several basic R data types that you frequently encounter in daily work. These include but are not limited to numeric, integer, logical, character, factor, date classes. All of these data types are present in our ‘intro_data’ dataframe for us to see practical examples. We will create our own examples as we go along.\n\n\n3 Numeric\nNumeric data with decimal values are called numeric in R. It is the default computational data type. If we look at our data frame we see that the following columns are numeric: lon, lat, NA.perc, mean, min, max. What sort of data are these?\nLet us create our own numeric object by assigning a decimal value to a variable x as follows, x will be of numeric type:\n\nx &lt;- 1.2 # assign 1.2 to x\nx # print the value of x\n\nR&gt; [1] 1.2\n\nclass(x) # what is the class of x?\n\nR&gt; [1] \"numeric\"\n\n\nFurthermore, even if we assign a number to a variable k that does not have a decimal place, it is still being saved as a numeric value:\n\nk &lt;- 1\nk\n\nR&gt; [1] 1\n\nclass(k)\n\nR&gt; [1] \"numeric\"\n\n\nIf we want to really be certain that k is or is not an integer we use is.integer():\n\nis.integer(k) # is k an integer?\n\nR&gt; [1] FALSE\n\n\n\n\n4 Integer\nAn integer in R is a numeric value that does not have a decimal place. It may only be a round whole number. Integers are often used for count data and when converting qualitative data to numbers for data analysis. In our dataframe we may see that we have two integer columns: depth and length. Why are these integers?\nIn order to create your own integer variable(s) in R, we use the as.integer(). We can be assured that y is indeed an integer by checking with is.integer():\n\ny &lt;- as.integer(13)\ny\n\nR&gt; [1] 13\n\nclass(y)\n\nR&gt; [1] \"integer\"\n\nis.integer(y) # is it an integer?\n\nR&gt; [1] TRUE\n\n\nIf we really have to, we can coerce a numeric value into an integer with the same as.integer() function:\n\nz &lt;- as.integer(pi)\nz\n\nR&gt; [1] 3\n\nclass(z)\n\nR&gt; [1] \"integer\"\n\nis.integer(z) # is it an integer?\n\nR&gt; [1] TRUE\n\n\n\n\n5 Logical\nThere are several logic values in R. We are mostly going to be concerned with the two main values we will be encountering: TRUE and FALSE. Note that all letters must be upper case. In our dataframe we see that only the ‘thermo’ column is logical. This column tells us whether or not the data were collected with a thermometer or not.\nLogical values (TRUE and FALSE) are often created via comparison between variables:\n\nx &lt;- 1; y &lt;- 2 # sample values\nz &lt;- x &gt; y\nz\n\nR&gt; [1] FALSE\n\nclass(z)\n\nR&gt; [1] \"logical\"\n\n\nIn order to perform logical operations we mostly use & (and), | (or), ! (negation):\n\nu &lt;- TRUE; v &lt;- FALSE; w &lt;- TRUE; x &lt;- FALSE\nu & v\n\nR&gt; [1] FALSE\n\nu & w\n\nR&gt; [1] TRUE\n\nv & x\n\nR&gt; [1] FALSE\n\nu | v\n\nR&gt; [1] TRUE\n\n!u\n\nR&gt; [1] FALSE\n\n\nAlthough these logical operators can be immensely useful in more advanced R programming, we will not go into too much detail in this introductory course. For more information on the logical operators, see the R help material:\n\nhelp(\"&\")\n\nOne final thing to note about logic in R is that it can be useful to perform arithmetic on logical values. TRUE has the value 1, while FALSE has value 0:\n\nas.integer(TRUE) # the numeric value of TRUE\n\nR&gt; [1] 1\n\nas.integer(FALSE) # the numeric value of FALSE\n\nR&gt; [1] 0\n\nsum(as.integer(intro_data$thermo))\n\nR&gt; [1] 10\n\n\nWhat is this telling us?\n\n\n6 Character\nIn our dataframe we see that only the ‘src’ column has the character values. This column is showing us which government body etc. collected the data in that row. At the use of a very familiar word, character, one may think this data type must be the most straightforward. This is not necessarily so as character values are used to represent string values in R. Because computers do not understand text the same way we do, they tend to handle this information differently. This allows us to do some pretty wild stuff with character values, but we will not be getting into that in this course as it quickly becomes very technical, generally speaking is not very useful in a daily application.\nIf however we wanted to convert an object to a character value we would do so with as.character():\n\nd &lt;- as.character(pi)\nclass(d)\n\nR&gt; [1] \"character\"\n\n\nThis can be useful if you have data that you want to be characters, but for one reason, another R has decided to make it a different data type.\nIf you want to join two character objects they can be concatenated with the paste() function:\n\na &lt;- \"fluffy\"; b &lt;- \"bunny\"\npaste(a, b)\n\nR&gt; [1] \"fluffy bunny\"\n\npaste(a, b, sep = \"-\")\n\nR&gt; [1] \"fluffy-bunny\"\n\n\nMore functions for string manipulation can be found in the R documentation — type help(\"sub\") at the command prompt. You may also wish to install Hadley Wickham’s nifty stringr package for more cool ways to work with character strings.\n\n\n7 Factor\nFactor values are somewhat difficult to explain and often even more difficult to understand. Factor values appear the same as character values when we look at them in a spreadsheet. But they are not the same. This will lead to much wailing and gnashing of teeth. So why then do factors exist and why would we use them? Factors allow us to numerically order names non-alphabetically, for example. This then allows one to order a list of research sites in geographical order.\nWe will see many examples of factors during this course but for now look at the ‘site’ column in our dataframe. If we click on this column a couple of times we see that it reorders all the data based on ascending or descending order of the sites. But that order is not alphabetical, it is based on the levels within the factor column. Each factor value in a column is assigned a level integer value (e.g. 1, 2, 3, 4, etc.). If multiple values in a factor column are the same, they receive the same level value as well.\nIf we want to see what the levels within a factor column are we use levels():\n\nlevels(intro_data$site)\n\nR&gt;  [1] \"Port Nolloth\"  \"St Helena Bay\" \"Saldanha Bay\"  \"Muizenberg\"   \nR&gt;  [5] \"Cape Agulhas\"  \"Mossel Bay\"    \"Tsitsikamma\"   \"Humewood\"     \nR&gt;  [9] \"Hamburg\"       \"Durban\"        \"Richards Bay\"  \"Sodwana\"\n\n\nWe will discuss in the next session what that $ means. But for now, are you able to see what the pattern is in the levels of the site listing?\nIf we want to create our own factors we will use as.factor():\n\nf &lt;- as.factor(letters[1:5])\nlevels(f)\n\nR&gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\nAnd if we want to change the order of our factor levels we use factor():\n\nf &lt;- factor(f, levels = c(\"b\", \"a\", \"c\", \"e\", \"d\"))\nlevels(f)\n\nR&gt; [1] \"b\" \"a\" \"c\" \"e\" \"d\"\n\n\nAnother reason for using factors to re-order our data, as we shall see tomorrow, is that this allows us to control the order in which values are plotted.\n\n\n8 Dates\nSee the next chapter about dates.\n\n\n\nDates.\n\n\n\n\n9 Vectors\nA vector, by definition, is a one-dimensional sequence of data elements of the same basic type (class). Members in a vector are officially called components. Basically, a vector is a column. Indeed, a dataframe is nothing more than a collection of vectors stuck together. If we wanted to create a vector from our dataframe we would do this:\n\nlonely_vector &lt;- intro_data$NA.perc\n\nNotice that we may not click on the object lonely_vector in our Environment tab. This is because it is no longer two-dimensional. If we want to visualise the data we need to enter it into the console or run it from our script:\n\nlonely_vector\n\nR&gt;  [1]  6 41 32  4 28 26  8  3  6 67 38 16\n\n\nLet us create some vectors of our own:\n\nprimes1 &lt;- c(3, 5, 7)\nprimes1\n\nR&gt; [1] 3 5 7\n\nclass(primes1)\n\nR&gt; [1] \"numeric\"\n\np1 &lt;- pi\np2 &lt;- 5\np3 &lt;- 7\n\nprimes2 &lt;- c(p1, p2, p3)\nprimes2\n\nR&gt; [1] 3.141593 5.000000 7.000000\n\nclass(primes2)\n\nR&gt; [1] \"numeric\"\n\nis.numeric(primes2)\n\nR&gt; [1] TRUE\n\nis.integer(primes2) # integers coerced into floating point numbers\n\nR&gt; [1] FALSE\n\n\nWe can also have vectors of logical values or character strings, and we can use the function length() to see how many components each has:\n\ntf &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE)\ntf\n\nR&gt; [1]  TRUE FALSE  TRUE FALSE FALSE\n\nlength(tf)\n\nR&gt; [1] 5\n\ncs &lt;- c(\"Mary\", \"has\", \"a\", \"silly\", \"lamb\")\ncs\n\nR&gt; [1] \"Mary\"  \"has\"   \"a\"     \"silly\" \"lamb\"\n\nlength(cs)\n\nR&gt; [1] 5\n\n\nOf course one would seldom enter data into R using the c() (combine) function, but it is useful for short calculations. More often than not one would import data from Excel (urgh!), something more reputable. The kinds of data one can read into R are remarkable. We will get to that later on.\nWe can also combine vectors in many ways, and the simplest way is the append one after the other:\n\nprimes12 &lt;- c(primes1, primes2)\nprimes12\n\nR&gt; [1] 3.000000 5.000000 7.000000 3.141593 5.000000 7.000000\n\nnonSense &lt;- c(primes12, cs)\nnonSense\n\nR&gt;  [1] \"3\"                \"5\"                \"7\"                \"3.14159265358979\"\nR&gt;  [5] \"5\"                \"7\"                \"Mary\"             \"has\"             \nR&gt;  [9] \"a\"                \"silly\"            \"lamb\"\n\nclass(nonSense)\n\nR&gt; [1] \"character\"\n\n\nIn the code fragment above, notice how the numeric values are being coerced into character strings when the two vectors of dissimilar class are combined. This is necessary so as to maintain the same primitive data type for members in the same vector.\n\n\n10 Vector Indices\nWhat if we want to extract one or a few components from the vector? Easy… We retrieve values in a vector by declaring an index inside a single square bracket [] operator. For example, the following shows how to retrieve a vector component. Since the vector index is 1-based (i.e., the first component in a vector is numbered 1), we use the index position 7 for retrieving the seventh member:\n\nnonSense[7] # find the seventh component in the vector\n\nR&gt; [1] \"Mary\"\n\n# or combine them in interesting ways...\npaste(nonSense[7], nonSense[8], nonSense[4], nonSense[10], \"bunnies\", sep = \" \")\n\nR&gt; [1] \"Mary has 3.14159265358979 silly bunnies\"\n\n\nIf the index given is negative, it will remove the value whose position has the same absolute value as the negative index. For example, the following creates a vector slice with the third member removed. However, if an index is out-of-range, a missing value will be reported via the symbol NA:\n\na &lt;- c(2, 6, 3, 8, 13)\na\n\nR&gt; [1]  2  6  3  8 13\n\na[-3]\n\nR&gt; [1]  2  6  8 13\n\na[10]\n\nR&gt; [1] NA\n\n\n\n\n11 Vector Creation\nR has many funky ways of creating vectors. This process is important to understand because we will need to build on it to create our own dataframes. Here are some examples of vector creation:\n\nseq(1:10) # assign them to a variable if you want to...\n\nR&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from = 0, to = 100, by = 10)\n\nR&gt;  [1]   0  10  20  30  40  50  60  70  80  90 100\n\nseq(0, 100, len = 10) # one may omit from and to\n\nR&gt;  [1]   0.00000  11.11111  22.22222  33.33333  44.44444  55.55556  66.66667\nR&gt;  [8]  77.77778  88.88889 100.00000\n\nseq(1, 9, by = pi)\n\nR&gt; [1] 1.000000 4.141593 7.283185\n\nrep(13, times = 13)\n\nR&gt;  [1] 13 13 13 13 13 13 13 13 13 13 13 13 13\n\nrep(seq(1:5), times = 6)\n\nR&gt;  [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\na &lt;- rnorm(20, mean = 13, sd = 0.13) # random numbers with known mean and sd\nrep(a, 5) # one may omit the times argument\n\nR&gt;   [1] 12.77928 12.80428 13.07617 12.97291 13.06123 13.08760 12.79702 13.04044\nR&gt;   [9] 13.10205 12.75365 13.03389 12.94164 12.86368 12.78633 13.07083 13.05517\nR&gt;  [17] 12.94809 13.08266 12.80896 12.98130 12.77928 12.80428 13.07617 12.97291\nR&gt;  [25] 13.06123 13.08760 12.79702 13.04044 13.10205 12.75365 13.03389 12.94164\nR&gt;  [33] 12.86368 12.78633 13.07083 13.05517 12.94809 13.08266 12.80896 12.98130\nR&gt;  [41] 12.77928 12.80428 13.07617 12.97291 13.06123 13.08760 12.79702 13.04044\nR&gt;  [49] 13.10205 12.75365 13.03389 12.94164 12.86368 12.78633 13.07083 13.05517\nR&gt;  [57] 12.94809 13.08266 12.80896 12.98130 12.77928 12.80428 13.07617 12.97291\nR&gt;  [65] 13.06123 13.08760 12.79702 13.04044 13.10205 12.75365 13.03389 12.94164\nR&gt;  [73] 12.86368 12.78633 13.07083 13.05517 12.94809 13.08266 12.80896 12.98130\nR&gt;  [81] 12.77928 12.80428 13.07617 12.97291 13.06123 13.08760 12.79702 13.04044\nR&gt;  [89] 13.10205 12.75365 13.03389 12.94164 12.86368 12.78633 13.07083 13.05517\nR&gt;  [97] 12.94809 13.08266 12.80896 12.98130\n\nrep(c(\"A\", \"B\", \"C\"), 3)\n\nR&gt; [1] \"A\" \"B\" \"C\" \"A\" \"B\" \"C\" \"A\" \"B\" \"C\"\n\nrep(c(\"A\", \"B\", \"C\"), each = 3)\n\nR&gt; [1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\nx &lt;- c(\"01-31-1960\", \"02-13-1960\", \"06-23-1977\", \"01-01-2013\")\nclass(x)\n\nR&gt; [1] \"character\"\n\nz &lt;- as.Date(x, \"%m-%d-%Y\")\nclass(z) # introducing the date class\n\nR&gt; [1] \"Date\"\n\nseq(as.Date(\"2013-12-30\"), as.Date(\"2014-01-04\"), by = \"days\")\n\nR&gt; [1] \"2013-12-30\" \"2013-12-31\" \"2014-01-01\" \"2014-01-02\" \"2014-01-03\"\nR&gt; [6] \"2014-01-04\"\n\nseq(as.Date(\"2013-12-01\"), as.Date(\"2016-01-31\"), by = \"months\")\n\nR&gt;  [1] \"2013-12-01\" \"2014-01-01\" \"2014-02-01\" \"2014-03-01\" \"2014-04-01\"\nR&gt;  [6] \"2014-05-01\" \"2014-06-01\" \"2014-07-01\" \"2014-08-01\" \"2014-09-01\"\nR&gt; [11] \"2014-10-01\" \"2014-11-01\" \"2014-12-01\" \"2015-01-01\" \"2015-02-01\"\nR&gt; [16] \"2015-03-01\" \"2015-04-01\" \"2015-05-01\" \"2015-06-01\" \"2015-07-01\"\nR&gt; [21] \"2015-08-01\" \"2015-09-01\" \"2015-10-01\" \"2015-11-01\" \"2015-12-01\"\nR&gt; [26] \"2016-01-01\"\n\nseq(as.Date(\"2000/1/1\"), by = \"month\", length.out = 12)\n\nR&gt;  [1] \"2000-01-01\" \"2000-02-01\" \"2000-03-01\" \"2000-04-01\" \"2000-05-01\"\nR&gt;  [6] \"2000-06-01\" \"2000-07-01\" \"2000-08-01\" \"2000-09-01\" \"2000-10-01\"\nR&gt; [11] \"2000-11-01\" \"2000-12-01\"\n\n# and many more...\n\n\n\n12 Vector Arithmetic\nArithmetic operations of vectors are performed component-by-component, i.e., componentwise. For example, suppose we have vectors a and b:\n\na &lt;- c(1, 3, 5, 7)\nb &lt;- c(1, 2, 4, 8)\n\nThen we multiply a by 5…\n\na * 5\n\nR&gt; [1]  5 15 25 35\n\n\n… and see that each component of a is multiplied by 5. In other words, the shorter vector (here 5) is recycled. Now multiply a with b…\n\na * b\n\nR&gt; [1]  1  6 20 56\n\n\n…and we see that the components in one vector matches those in the other one-for-one. Similarly for subtraction, addition, division and we get new vectors via componentwise operations. Try this here now a few times with your own vectors.\nBut what if one vector is somewhat shorter than the other? The recycling rule comes into play. If two vectors are of unequal length, the shorter one will be recycled in order to match the longer vector. For example, the following vectors u and v have different lengths, and their sum is computed by recycling values of the shorter vector u:\n\nv &lt;- rep(2, len = 13)\nu &lt;- rep(c(1, 20), len = 5)\nv + u\n\nR&gt; Warning in v + u: longer object length is not a multiple of shorter object\nR&gt; length\n\n\nR&gt;  [1]  3 22  3 22  3  3 22  3 22  3  3 22  3\n\n\n\n\n13 Dataframe Creation\nThe most rudimentary way to create a dataframe is to create several vectors and then assemble them into a dataframe using cbind() — this is a function that combines by column. For instance:\n\n# create three vectors of different types\nvec1 &lt;- rep(c(\"A\", \"B\", \"C\"), each = 5) # a character vector (a facctor)\nvec2 &lt;- seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                 length.out = length(vec1)) # date vector\nvec3 &lt;- rnorm(n = length(vec1), mean = 0, sd = 0.35) # numeric vector\n# now assemble dataframe\ndf1 &lt;- cbind(vec1, vec2, vec3)\nhead(df1)\n\nR&gt;      vec1 vec2   vec3                 \nR&gt; [1,] \"A\"  \"4018\" \"0.0244539781625587\" \nR&gt; [2,] \"A\"  \"4019\" \"0.444495543066824\"  \nR&gt; [3,] \"A\"  \"4020\" \"-0.0160541714112745\"\nR&gt; [4,] \"A\"  \"4021\" \"0.57907475506046\"   \nR&gt; [5,] \"A\"  \"4022\" \"-0.657917104092748\" \nR&gt; [6,] \"B\"  \"4023\" \"0.5170648471008\"\n\n\nAnother way to achieve the same thing is to use the data.frame() function that will allow you to achieve all of the above steps at once. Here is the example:\n\ndf2 &lt;- data.frame(vec1 = rep(c(\"A\", \"B\", \"C\"), each = 5),\n                  vec2 = seq.Date(from = as.Date(\"1981-01-01\"), by = \"day\", \n                                  length.out = length(vec1)),\n                  vec3 = rnorm(n = length(vec1), mean = 2, sd = 0.75))\nhead(df2, 2)\n\nR&gt;   vec1       vec2     vec3\nR&gt; 1    A 1981-01-01 2.280459\nR&gt; 2    A 1981-01-02 2.532540\n\n\nWhat about the names of the dataframe that you just created? Are you happy that they are descriptive enough? If you are not, do not fear. There are several different ways in which we can change it. We can assign the existing separate vectors vec1, vec2 and vec3 to more user-friendly names using the data.frame() function, like this:\n\ndf1 &lt;- data.frame(level = vec1,\n                  sample.date = vec2,\n                  measurement = vec3)\nhead(df1, 2)\n\nR&gt;   level sample.date measurement\nR&gt; 1     A  1981-01-01  0.02445398\nR&gt; 2     A  1981-01-02  0.44449554\n\n\nAnother way is to change the name after you have created the dataframe using the colnames() assignment function, as in:\n\ncolnames(df2) &lt;- c(\"level\", \"sample.date\", \"measurement\")\nhead(df2, 2)\n\nR&gt;   level sample.date measurement\nR&gt; 1     A  1981-01-01    2.280459\nR&gt; 2     A  1981-01-02    2.532540\n\nnames(df2)\n\nR&gt; [1] \"level\"       \"sample.date\" \"measurement\"\n\n\nDataframes are very versatile and we can do many operations on them. A common requirement is to add a column to a dataframe that contains the outcome of some calculation. We could create a new column in the dataframe ‘on the fly’, as in:\n\ndf2.1 &lt;- df1 # copy the dataframe\ndf2.1$meas.anom &lt;- df1$measurement - mean(df1$measurement)\ndf2.1$meas.diff &lt;- df2.1$measurement - df2.1$meas.anom\nhead(df2.1, 2)\n\nR&gt;   level sample.date measurement  meas.anom   meas.diff\nR&gt; 1     A  1981-01-01  0.02445398 0.01863298 0.005820999\nR&gt; 2     A  1981-01-02  0.44449554 0.43867454 0.005820999\n\n\nWe can also combine dataframes in different ways. Perhaps you have two (or more) dataframe that conform to the same layout, i.e., they have the same number of columns (although the length of the dataframes may differ), they have the same data type in those columns, the names of those columns are the same. Also and the order of the columns must be identical in all the dataframes. Two separate dataframe with the same structure may, for example, result from two identical experiments that were repeated at different times. We can then stack one on top (e.g., combine our experiments) of the other using the row bind function rbind(), as in:\n\nnrow(df1) # check the number of rows first\n\nR&gt; [1] 15\n\nnrow(df2)\n\nR&gt; [1] 15\n\ndf3 &lt;- rbind(df1, df2)\nnrow(df3) # number of rows in the combined dataframe\n\nR&gt; [1] 30\n\nhead(df3, 2)\n\nR&gt;   level sample.date measurement\nR&gt; 1     A  1981-01-01  0.02445398\nR&gt; 2     A  1981-01-02  0.44449554\n\n\nBut now how do we know how the portions of the stacked dataframe relate to the experiments that resulted in the data in the first place? There is no label to distinguish one experiment from the other. We can fix this by adding a new column to the stacked dataframe that contains the coding for the two experiments. We can achieve it like this:\n\ndf3$exp.no &lt;- rep(c(\"exp1\", \"exp2\"), each = nrow(df1))\nhead(df3, 2)\n\nR&gt;   level sample.date measurement exp.no\nR&gt; 1     A  1981-01-01  0.02445398   exp1\nR&gt; 2     A  1981-01-02  0.44449554   exp1\n\ntail(df3, 2)\n\nR&gt;    level sample.date measurement exp.no\nR&gt; 29     C  1981-01-14    2.022019   exp2\nR&gt; 30     C  1981-01-15    1.879616   exp2\n\n\nWe can combine dataframes in another way — that is, bind columns side-by-side using the function cbind(). We used it before to place vectors of the same length next to each other to create a dataframe. This function is similar to rbind(), but where rbind() fusses over the names of the columns, cbind() does not. What does concern cbind(), however, is that the number of rows in the two (or more) dataframes that will be ‘glued’ side-by-side is the same. Try it yourself with your own dataframes.\n\n\n14 Dataframe Indices\nRemember that weird $ symbol we saw a little while ago? That symbol tells R that you want to see a column (vector) within a dataframe. For example, if we wanted to perform an operation on only one column in intro_data in order to ascertain the mean depth (m) of sampling:\n\nround(mean(intro_data$depth),2)\n\nR&gt; [1] 1.33\n\n\nIf we want to subset only specific values in a dataframe, as we have seen how to do with vectors, we need to consider that we are now working with two dimensions, not one. We still use[] but now we must do a little extra. If we want to see how long the time series for Sodwana is we could do this in several ways, here are the three most common in an improving order:\n\n# Subset a dataframe using [,]\nintro_data[12,9]\n\nR&gt; [1] 4606\n\n# Subset only one column using []\nintro_data$length[12]\n\nR&gt; [1] 4606\n\n# Subset from one column using logic for another column\nintro_data$length[intro_data$site == \"Sodwana\"]\n\nR&gt; [1] 4606\n\n\nThe important thing to remember here is that when one needs to use a comma when subsetting, the row number is always on the left, and the column number is always on the right. Rows then columns! Tattoo that onto your brain. Or fore-arm if you are the adventurous type. We will go into the subsetting, analysis of dataframes in much more detail in the following session.\nOne must keep in mind that data in R can become substantially more complex than what we have covered, and the software also distinguishes several other kinds of data ‘containers’: in addition to vectors, dataframes and we also have lists, matrices, time series, arrays. The more complex ones such as arrays, may have more dimensions than the two (rows along dimension 1, columns along dimension 2) that most people are familiar with. We will not delve into these here as they are bit more advanced than the goals of this course.\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {18. {Base} {R} {Primer}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/18-base_r.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 18. Base R Primer. http://samos-r.netlify.app/intro_r/18-base_r.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "18. Base R Primer"
    ]
  },
  {
    "objectID": "intro_r/16-recap.html",
    "href": "intro_r/16-recap.html",
    "title": "16. Synthesis",
    "section": "",
    "text": "“The statistician’s task is not to discover the truth, but to measure uncertainty.”\n— Bradley Efron\n\n\n“Somewhere, something incredible is waiting to be known.”\n— Carl Sagan\n\n\n1 Workshop Recap, Assessment Alignment, and What Comes Next\nOver the course of this workshop, you have learned not just how to use R, but how to think in a tidy, reproducible, and analytical way. These skills are foundational for all subsequent assessments and for the Biostatistics component that follows.\n\n\n2 The One Workflow You Will Use Forever\nEvery analysis you do is a variation of the same loop:\nImport → Inspect → Tidy → Transform → Visualise → Summarise → Communicate → Repeat\nIf you can internalise that sequence, you can learn any new package or domain-specific dataset. The tools will change but the loop will not.\n\n\n3 Debugging Is a Core Skill\nWhat breaks is not your fault. Debugging is part of the job. Use this simple routine:\n\nRead the last line of the error (it usually says what failed).\nIdentify the function that failed (the first line after “Error in …”).\nCheck object class and structure with str() or glimpse().\nReproduce with a minimal example (smallest input that still fails).\n\nCommon failures to watch for:\n\nfactors vs characters\nNA propagation\nsilent recycling (length mismatch)\ngrouping that persists too long\n\n\n\n4 Predict Before You Execute\nBefore you run a pipeline, ask:\n\n“How many rows should I have now?”\n“What changed conceptually?”\n“What stayed the same?”\n\nThis habit is the difference between button-pushing and analysis.\n\n\n5 Object Hygiene and Naming Discipline\nYour objects are your memory so treat them carefully.\n\nOverwrite when you are confident you no longer need the old version.\nCreate new objects when you are exploring or unsure.\nUse names that scale (avoid df2, final_final, test).\nPeriodically restart R and run your script top-to-bottom. If it fails, your workflow is not yet reproducible.\n\n\n\n6 Reproducibility Beyond Quarto\nReproducibility is a mindset:\n\nScripts must run from a clean session.\nRelative paths are scientific hygiene.\nResults without code are not evidence.\n\nIf you cannot re-run it in six months, it does not exist.\n\n\n7 Light-Weight Statistical Instincts\nEven before formal statistics, cultivate these instincts:\n\nVariability vs central tendency (do not trust means alone).\nSample size matters (n() is critical to understanding the power of your data).\nPlots are models and offer first insights into your data.\n\n\n\n8 One Narrative Question\nWe have been asking the same question all along:\nHow does coastal temperature vary in space, time, and depth?\nYou saw this question early in simple plots, and later in grouped summaries and spatial maps. The question did not change but your ability to answer it did.\n\n\n9 Alignment with Assessments\nThe workshop has been structured to map directly onto your assessed work. Each assessment assumes that you can independently apply the following skills:\n\nAssessment readiness\n\nImport, inspect, and tidy real datasets\nApply a coherent workflow from raw data to final output\nWrite readable, reproducible R code using tidyverse principles\nProduce publication-quality figures using ggplot2\nManipulate data confidently using dplyr verbs and pipes\n\nWhat assessors will look for\n\nLogical data workflows (not trial-and-error code)\nClear transformation steps (filter(), mutate(), summarise(), group_by())\nAppropriate visualisation choices\nEvidence that results were derived, not manually curated\nLiberal application of comments to document your workflow, i.e., code that is understandable by someone else (including your future self)\n\n\nIf you can reproduce the analyses and figures from this workshop without following along line-by-line, you are well prepared for the assessments.\n\n\n10 Concept Map: How the Chapters Fit Together\nYou should be able to combine lessons learned in each chapter, because they play specific roles in a single, coherent analytical framework/workflow:\n\nR and RStudio Orientation — learning the environment, tools, and expectations of working in R.\nWorking with Data and Code Foundations — understanding scripts, objects, and how R thinks.\nR Markdown and Quarto Reproducibility — integrating code, results, and narrative into a single document.\nData Classes and Structures Literacy — knowing what your data is before deciding what to do with it.\nR Workflows Discipline — structuring analyses so they are repeatable and scalable.\nGraphics with ggplot2 Visual reasoning — learning to explore and communicate data visually.\nFaceting Figures Comparison — revealing patterns across groups and conditions.\nBrewing Colours Clarity and accessibility — making figures interpretable and professional.\nMapping with ggplot2 Spatial thinking — extending tidy principles to geographic data.\nMapping with Style Polish — producing maps suitable for reports and publications.\nMapping with Natural Earth / Applied Examples Integration — combining data sources, projections, and styling.\nTidy Data Structure — learning the rules that make analysis possible.\nTidier Data Transformation — filtering, mutating, selecting, and summarising.\nTidiest Data Power — grouping, pipelines, and complex workflows.\nSynthesis Synthesis — seeing the workflow as a single analytical language.\n\nTogether, these chapters teach you how to move from messy reality → structured data → insight → communication.\n\n\n11 What You Now Are\nYou are now someone who can:\n\nRead unfamiliar R code\nTidy real-world data\nAsk questions of data (not just plot it)\nLearn new packages independently\n\n\n\n12 Prelude to Biostatistics\nThe Biostatistics component builds directly on everything you have learned here. For those of you taking BCB743 as an elective, this work will also be foundational.\nIn this workshop, you focused on:\n\nHow to prepare data\nHow to explore patterns\nHow to visualise structure and variation\n\nIn Biostatistics, you will now ask:\n\nAre these patterns meaningful?\nHow much uncertainty is there?\nWhat conclusions are supported by the data?\n\nThe transition looks like this:\n\nTidy data → prerequisite for valid statistics\nGrouping and summarising → foundation of statistical models\nVisual exploration → guides hypothesis formulation\nReproducible workflows → ensures transparent inference\n\nStatistical tests, models, and confidence intervals only make sense when applied to well-structured, well-understood data. You now have the tools to ensure that this condition is met.\nThink of this workshop as learning the grammar of data analysis. Biostatistics is where you begin to write arguments.\n\n\n13 Final Note\nYou are not expected to memorise functions — that is what the help files are for. You are expected to know and implement workflows, patterns, and logic.\nConfidence in R comes from practice, patience, and clarity.\nIf your code reads like a story of what you did and why — you are doing it right.\n\n\n14 Session Info\n\ninstalled.packages()[names(sessionInfo()$otherPkgs), \"Version\"]\n\nR&gt; character(0)\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {16. {Synthesis}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/16-recap.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 16. Synthesis. http://samos-r.netlify.app/intro_r/16-recap.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "16. Synthesis"
    ]
  },
  {
    "objectID": "intro_r/14-tidier.html",
    "href": "intro_r/14-tidier.html",
    "title": "14. Tidier Data",
    "section": "",
    "text": "“Statistics cannot rescue a poorly designed study.”\n— Douglas Altman\n\n\n“The statistician’s task is not to discover the truth, but to measure uncertainty.”\n— Bradley Efron\n\n\n1 Introduction\nOn Day 1 already you worked through a tidy workflow. You saw how to import data, how to manipulate it, run a quick analysis, and create figures. In the previous session you filled in the missing piece of the workflow by also learning how to tidy up your data within R. For the remainder of today you will be revisiting the “transform” portion of the tidy workflow. In this session you are going to go into more depth on what you learned in Day 1, and in the last session you will learn some new tricks. Over these two sessions you will also become more comfortable with the pipe command %&gt;%, while practising writing tidy code.\nTransformation in the tidyverse is a small grammar, not a random list of tools. All five verbs share three properties:\n\nThe first argument is a data frame.\nThe output is a data frame.\nThe result stays tidy unless you deliberately collapse it (mainly with summarise()).\n\nThere are five primary data transformation functions that you will focus on here:\n\nArrange observations (rows) with arrange()\n\nFilter observations (rows) with filter()\n\nSelect variables (columns) with select()\n\nCreate new variables (columns) with mutate()\n\nSummarise variables (columns) with summarise()\n\n\n\n\n\n\n\nNoteQuick Decision Guide\n\n\n\n\nNeed fewer rows? → filter()\nNeed different row order? → arrange()\nNeed fewer or re-ordered columns? → select()\nNeed new columns (same number of rows)? → mutate()\nNeed fewer rows by aggregating? → summarise() (usually after group_by())\n\n\n\n\n\n\n\n\n\nTipRow/column Invariants\n\n\n\n\narrange() and filter() change rows, not columns.\nselect() and mutate() change columns, not rows.\nsummarise() changes rows on purpose (it collapses data).\n\n\n\nYou will use the full South African Coastal Temperature Network dataset for these exercises. Before you begin, however, you will need to cover two new concepts.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load the data from a .RData file\nload(here::here(\"data\", \"BCB744\", \"SACTNmonthly_v4.0.RData\"))\n\n# Copy the data as a dataframe with a shorter name\nSACTN &lt;- SACTNmonthly_v4.0\n\n# Remove the original\nrm(SACTNmonthly_v4.0)\n\n\n\n\n\n\n\nTipPipes as a Thinking Tool\n\n\n\nThe pipe is not just syntax. Each line should answer one question: “What rows do I want?” → “What columns do I need?” → “What new columns should I add?” If a pipeline feels confusing, split it into steps and inspect the intermediate outputs.\n\n\n\n\n2 Comparison and Logical (Boolean) Operators\nThe assignment operator (&lt;-) is a symbol that we use to assign some bit of code to an object in your environment. Likewise, comparison operators are symbols we use to compare different objects. This is how you tell R how to decide to do many different things. You will see these symbols often out in the “real world” so let us spend a moment now getting to know them better. Most of these should be very familiar to you already:\n\n&gt;: Greater than\n\n&gt;=: Greater than or equal to\n&lt;: Less than\n&lt;=: Less than or equal to\n==: Equal to\n!=: Not equal to\n\nIt is important here to note that == is for comparisons; = is for maths. They are not interchangeable, as we may see in the following code chunk. This is one of the more common mistakes one makes when writing code. Luckily the error message this creates should provide us with the clues we need to figure out that we have made this specific mistake.\n\nSACTN %&gt;% \n  filter(site = \"Amanzimtoti\")\n\nR&gt; Error in `filter()`:\nR&gt; ! We detected a named input.\nR&gt; ℹ This usually means that you've used `=` instead of `==`.\nR&gt; ℹ Did you mean `site == \"Amanzimtoti\"`?\n\n\n\n\n\n\n\n\nNoteWhy the Error Is Helpful\n\n\n\ndplyr inspects your arguments and refuses to guess what you meant. This is a feature: it prevents silent mistakes and makes debugging faster.\n\n\nThe comparison operators are often used together with Boolean operators. Boolean operators are used for logical operations and can compare values, resulting in either TRUE and FALSE. Here they are:\n\n!: NOT - Negates a true value to false, and a false value to true.\n&: AND (vectorised) - Returns TRUE if both operands are true, FALSE otherwise.\n|: OR (vectorised) - Returns TRUE if at least one of the operands is true.\n&&: AND (short-circuit) - Only evaluates the first element of each vector.\n||: OR (short-circuit) - Only evaluates the first element of each vector.\n\nThink of & as constraint tightening (both conditions must be true) and | as constraint loosening (either condition can be true).\nThe %in% operator in R is a special operator used to test if elements of a vector or data object are contained in another vector or data object. It returns a Boolean vector (TRUE and FALSE) indicating whether each element of the first vector is found in the second vector. This operator is particularly useful for subsetting or filtering data based on matching values. For example, x %in% y will check for each element of x if there is a match in y, and return a logical vector indicating the presence or absence of each x element in y.\nSo, comparison operators are used to make direct comparisons between specific things, but logical operators are used more broadly when making logical arguments. Logic is central to most computing so it is worth taking the time to cover these symbols explicitly here. R makes use of the same Boolean logic symbols as many other platforms, including Google, so some (or all) of these will likely be familiar.\nWhen writing a line of tidy code you tend to use these logical operator to combine two or more arguments that use comparison operators. For example, the following code chunk uses the filter() function to find all temperatures recorded at Pollock Beach during December or January. Do not worry if the following line of code is difficult to piece out, but make sure you can locate which symbols are comparison operators, which are logical operators. Please note that for purposes of brevity all of the outputs in this section are limited to ten lines but when you run these code chunks on your own computer they will be much longer.\n\nSACTN %&gt;% \n  filter(site == \"Pollock Beach\", month(date) == 12 | month(date) == 1)\n\n\n\nR&gt;             site  src       date     temp depth   type\nR&gt; 1  Pollock Beach SAWS 1999-12-01 19.95000     0 thermo\nR&gt; 2  Pollock Beach SAWS 2000-01-01 19.03333     0 thermo\nR&gt; 3  Pollock Beach SAWS 2000-12-01 19.20000     0 thermo\nR&gt; 4  Pollock Beach SAWS 2001-01-01 18.32667     0 thermo\nR&gt; 5  Pollock Beach SAWS 2001-12-01 20.59032     0 thermo\nR&gt; 6  Pollock Beach SAWS 2002-01-01 21.47097     0 thermo\nR&gt; 7  Pollock Beach SAWS 2002-12-01 19.78065     0 thermo\nR&gt; 8  Pollock Beach SAWS 2003-01-01 20.64516     0 thermo\nR&gt; 9  Pollock Beach SAWS 2003-12-01 20.48710     0 thermo\nR&gt; 10 Pollock Beach SAWS 2004-01-01 21.34839     0 thermo\n\n\nYou will look at the interplay between comparison and logical operators in more depth in the following session after you have reacquainted yourself with the main transformation functions you need to know.\n\n\n3 Arrange Observations (Rows) with arrange()\nFirst up in our greatest hits reunion tour is the function arrange(). This very simply arranges the observations (rows) in a dataframe based on the variables (columns) it is given. If you are concerned with ties in the ordering of our data you provide additional columns to arrange(). The importance of the columns for arranging the rows is given in order from left to right.\nWhen does arranging matter?\n\nChecking extremes (highest/lowest values).\nValidating temporal order before plotting.\nPreparing grouped summaries so you can inspect patterns.\n\n\nSACTN %&gt;% \n  arrange(depth, temp)\n\n\n\nR&gt;             site  src       date      temp depth   type\nR&gt; 1      Sea Point SAWS 1990-07-01  9.635484     0 thermo\nR&gt; 2     Muizenberg SAWS 1984-07-01  9.708333     0 thermo\nR&gt; 3     Doringbaai SAWS 2000-12-01  9.772727     0 thermo\nR&gt; 4  Hondeklipbaai SAWS 2003-06-01  9.775000     0 thermo\nR&gt; 5      Sea Point SAWS 1984-06-01 10.000000     0 thermo\nR&gt; 6     Muizenberg SAWS 1992-07-01 10.193548     0 thermo\nR&gt; 7  Hondeklipbaai SAWS 2005-07-01 10.333333     0 thermo\nR&gt; 8  Hondeklipbaai SAWS 2003-07-01 10.340909     0 thermo\nR&gt; 9      Sea Point SAWS 2000-12-01 10.380645     0 thermo\nR&gt; 10    Muizenberg SAWS 1984-08-01 10.387097     0 thermo\n\n\nIf you would rather arrange your data in descending order, as is perhaps more often the case, you simply wrap the column name you are arranging by with the desc() function as shown below.\n\nSACTN %&gt;% \n  arrange(desc(temp))\n\n\n\nR&gt;             site   src       date     temp depth type\nR&gt; 1        Sodwana   DEA 2000-02-01 28.34648    18  UTR\nR&gt; 2        Sodwana   DEA 1999-03-01 28.04890    18  UTR\nR&gt; 3        Sodwana   DEA 1998-03-01 27.87781    18  UTR\nR&gt; 4        Sodwana   DEA 1998-02-01 27.76452    18  UTR\nR&gt; 5        Sodwana   DEA 1996-02-01 27.73637    18  UTR\nR&gt; 6        Sodwana   DEA 2000-03-01 27.52637    18  UTR\nR&gt; 7        Sodwana   DEA 2000-01-01 27.52291    18  UTR\nR&gt; 8  Leadsmanshoal EKZNW 2007-02-01 27.48132    10  UTR\nR&gt; 9        Sodwana EKZNW 2005-01-01 27.45619    12  UTR\nR&gt; 10       Sodwana EKZNW 2007-02-01 27.44054    12  UTR\n\n\nIt must also be noted that when arranging data in this way, any rows with NA values will be sent to the bottom of the dataframe. This is not always ideal and so must be kept in mind.\n\n\n4 Filter Observations (Rows) with filter()\nWhen simply arranging data is not enough, and you need to remove rows of data you do not want, filter() is the tool to use. For example, you can select all monthly temperatures recorded at the site Humewood during the year 1990 with the following code chunk:\n\n\n\n\n\n\nWarningFiltering Is Destructive\n\n\n\nEvery filter() call removes rows. Stacking multiple filters shrinks your data quickly. If results look “too small,” check each step.\n\n\n\nSACTN %&gt;% \n  filter(site == \"Humewood\", year(date) == 1990)\n\n\n\nR&gt;        site  src       date     temp depth   type\nR&gt; 1  Humewood SAWS 1990-01-01 21.87097     0 thermo\nR&gt; 2  Humewood SAWS 1990-02-01 18.64286     0 thermo\nR&gt; 3  Humewood SAWS 1990-03-01 18.61290     0 thermo\nR&gt; 4  Humewood SAWS 1990-04-01 17.30000     0 thermo\nR&gt; 5  Humewood SAWS 1990-05-01 16.35484     0 thermo\nR&gt; 6  Humewood SAWS 1990-06-01 15.93333     0 thermo\nR&gt; 7  Humewood SAWS 1990-07-01 15.70968     0 thermo\nR&gt; 8  Humewood SAWS 1990-08-01 16.09677     0 thermo\nR&gt; 9  Humewood SAWS 1990-09-01 16.41667     0 thermo\nR&gt; 10 Humewood SAWS 1990-10-01 17.14194     0 thermo\n\n\nRemember to use the assignment operator (&lt;-, keyboard shortcut alt -) if you want to create an object in the environment with the new results.\n\nhumewood_90s &lt;- SACTN %&gt;% \n  filter(site == \"Humewood\", year(date) %in% seq(1990, 1999, 1))\n\nIt must be mentioned that filter() also automatically removes any rows in the filtering column that contain NA values. Should you want to keep rows that contain missing values, insert the is.na() function into the line of code in question. To illustrate this let us filter the temperatures for the Port Nolloth data collected by the DEA that were at or below 11°C or were missing values. You will put each argument on a separate line to help keep things clear. Note how R automatically indents the last line in this chunk to help remind you that they are in fact part of the same argument. Also note how I have put the last bracket at the end of this argument on its own line. This is not required, but I like to do so as it is a very common mistake to forget the last bracket.\n\nSACTN %&gt;% \n  filter(site == \"Port Nolloth\", # First give the site to filter\n         src == \"DEA\", # Then specify the source\n         temp &lt;= 11 | # Temperatures at or below 11°C OR\n           is.na(temp) # Include missing values\n         )\n\n\n\n5 Select Variables (Columns) with select()\nWhen you load a dataset that contains more columns than will be useful or required, it is preferable to shave off the excess. You do this with the select() function. In the following four examples you are going to remove the depth and type columns. There are many ways to do this and none are technically better or faster. So it is up to the user to find a favourite technique.\n\n\n\n\n\n\nTipTwo Mental Models for select()\n\n\n\n\nPositive selection: “What do I want to keep?”\nNegative selection: “What do I want to remove?”\n\nselect() never changes rows. It only changes columns.\n\n\n\n# Select columns individually by name\nSACTN %&gt;% \n  select(site, src, date, temp)\n\n# Select all columns between site and temp like a sequence\nSACTN %&gt;% \n  select(site:temp)\n\n# Select all columns except those stated individually\nSACTN %&gt;% \n  select(-date, -depth)\n\n# Select all columns except those within a given sequence\n  # Note that the '-' goes outside of a new set of brackets\n  # that are wrapped around the sequence of columns to remove\nSACTN %&gt;% \n  select(-(date:depth))\n\nYou may also use select() to reorder the columns in a dataframe. In this case the inclusion of the everything() function may be a useful shortcut as illustrated below.\n\n# Change up order by specifying individual columns\nSACTN %&gt;% \n  select(temp, src, date, site)\n\n# Use the everything function to grab all columns \n# not already specified\nSACTN %&gt;% \n  select(type, src, everything())\n\n# Or go bananas and use all of the rules at once\n  # Remember, when dealing with tidy data,\n  # everything may be interchanged\nSACTN %&gt;% \n  select(temp:type, everything(), -src)\n\n\n\n\n\n\n\nNoteThe Square Bracket [] Notation\n\n\n\nThe square bracket [] notation may also be used for indexing and subsetting data structures such as vectors, matrices, data frames, and lists. Before tidyverse existed, this was the only way to do so. Square brackets allow you to select elements from these data structures based on their positions, conditions, or names. The use of square brackets can vary slightly depending on the data structure being accessed. Here is a brief overview:\n\nVectors: When used with vectors, square brackets allow you to select elements by their numeric position, a logical vector indicating which elements to select. For examplevector[c(1, 3)] returns the first and third elements of the vector.\nMatrices: For matrices, square brackets take two dimensions [row, column] to select elements. You can select entire rows, columns, or individual elements. Specifying a row, column as empty (e.g., [,]) selects everything in that dimension.\nDataframes: Similar to matrices, square brackets can be used to subset data frames by row, column. However and since dataframes can have column names, you can also use these names for selection, e.g., df[1,] selects the first row of the dataframe, df[, \"columnName\"] selects all rows of a specific column.\nLists: Lists can be subsetted by numeric or character indices corresponding to their elements. For example, list[[1]] selects the first element of the list. Note the double brackets, which are used to extract elements from a list directly. Single brackets, e.g., list[1], return a sublist containing the first element.\n\n\n\n\n\n\n\n\n\nNoteBase R vs Tidyverse\n\n\n\nBase R subsetting is powerful but low-level. Tidyverse verbs are higher-level: they make your intent explicit and read like a sentence. Use the one that makes your reasoning clearest.\n\n\n\n\n\n\n\n\nImportantData Structures and Square Brackets\n\n\n\nDo this now: provide examples of i) the various data structures available in R, and ii) how to use square brackets to subset each of them. You may use any of the built-in datasets to do so.\n\n\n\n\n6 Create New Variables (Columns) with mutate()\nWhen you are performing data analysis/statistics in R this is likely because it is necessary to create some new values that did not exist in the raw data. The previous three functions you looked at (arrange(), filter(), select()) will prepare you to create new data, but do not do so themselves. This is when you need to use mutate(). You must however be very mindful that mutate() is only useful if we want to create new variables (columns) that are a function of one or more existing columns (well, that is how it is mainly used). Any new column you create with mutate() must always have the same number of rows as the dataframe you are working with. In order to create a new column you must first tell R what the name of the column will be, in this case let us create a column named kelvin. The second step is to then tell R what to put in the new column. As you may have guessed, you are going to convert the temp column which contains degrees Celsius (C) into Kelvin (K) by adding 273.15 to every row.\n\n\n\n\n\n\nWarningCommon Mutate Mistakes\n\n\n\n\nTrying to create a summary (fewer rows) inside mutate() → use summarise() instead.\nAccidentally overwriting an existing column when you meant to create a new one.\n\n\n\n\nSACTN %&gt;% \n  mutate(kelvin = temp + 273.15)\n\n\n\nR&gt;            site src       date     temp depth type   kelvin\nR&gt; 1  Port Nolloth DEA 1991-02-01 11.47029     5  UTR 284.6203\nR&gt; 2  Port Nolloth DEA 1991-03-01 11.99409     5  UTR 285.1441\nR&gt; 3  Port Nolloth DEA 1991-04-01 11.95556     5  UTR 285.1056\nR&gt; 4  Port Nolloth DEA 1991-05-01 11.86183     5  UTR 285.0118\nR&gt; 5  Port Nolloth DEA 1991-06-01 12.20722     5  UTR 285.3572\nR&gt; 6  Port Nolloth DEA 1991-07-01 12.53810     5  UTR 285.6881\nR&gt; 7  Port Nolloth DEA 1991-08-01 11.25202     5  UTR 284.4020\nR&gt; 8  Port Nolloth DEA 1991-09-01 11.29208     5  UTR 284.4421\nR&gt; 9  Port Nolloth DEA 1991-10-01 11.37661     5  UTR 284.5266\nR&gt; 10 Port Nolloth DEA 1991-11-01 10.98208     5  UTR 284.1321\n\n\nThis is a very basic example; mutate() is capable of much more than simple addition. You will get into some more exciting examples during the next session.\n\n\n7 Summarise Variables (Columns) with summarise()\nFinally this brings you to the last tool for this section. To create new columns you use mutate(), but to calculate any sort of summary/statistic from a column that will return fewer rows than the dataframe has you will use summarise(). This makes summarise() much more powerful than the other functions in this section, but because it is able to do more, it can also be more unpredictable, making its use potentially more challenging. You will almost always end up using this function in our workflows. The following chunk very simply calculates the overall mean temperature for the entire SACTN.\nThink of summarise() as a controlled rupture: it intentionally breaks the one-observation-per-row structure to produce aggregate values. This is why it is almost always paired with group_by() later.\n\nSACTN %&gt;% \n  summarise(mean_temp = mean(temp, na.rm = TRUE))\n\nR&gt;   mean_temp\nR&gt; 1  19.26955\n\n\nNote how the above chunk created a new dataframe. This is done because it cannot add this one result to the previous dataframe due to the mismatch in the number of rows. If you were to create additional columns with other summaries, you may do so within the same summarise() function. These multiple summaries are displayed on individual lines in the following chunk to help keep things clear.\n\nSACTN %&gt;% \n  summarise(mean_temp = mean(temp, na.rm = TRUE),\n            sd_temp = sd(temp, na.rm = TRUE),\n            min_temp = min(temp, na.rm = TRUE),\n            max_temp = max(temp, na.rm = TRUE)\n            )\n\nR&gt;   mean_temp  sd_temp min_temp max_temp\nR&gt; 1  19.26955 3.682122 9.136322 28.34648\n\n\nCreating summaries of the entire SACTN dataset in this way is not appropriate as you should not be combining time series from such different parts of the coast. In order to calculate summaries within variables you will need to learn how to use group_by(), which in turn will first require you to learn how to chain multiple functions together within a pipe (%&gt;%). That is how you will begin the next session for today. You will finish with several tips on how to make your data the tidiest that it may be.\n\n\n\n\n\n\nTipDebugging Pipelines\n\n\n\nIf a result looks wrong:\n\nRun the pipeline stepwise and inspect intermediate objects.\nCheck row counts before and after each verb.\nUse count() or summarise(n = n()) to confirm expectations.\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {14. {Tidier} {Data}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/14-tidier.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 14. Tidier Data. http://samos-r.netlify.app/intro_r/14-tidier.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "14. Tidier Data"
    ]
  },
  {
    "objectID": "intro_r/12-mapping_quakes.html",
    "href": "intro_r/12-mapping_quakes.html",
    "title": "12. The Fiji Earthquake Data",
    "section": "",
    "text": "Here I will plot the built-in earthquake dataset (datasets::quakes).\n\n\n\n\n\n\nNoteGlobal Earthquake Distribution\n\n\n\nFor a global map of earthquakes, see my plot of the Kaggle earthquake data.\n\n\nTwo new concepts will be introduced in the chapter:\n\nGeographic Coordinate Systems\nProjected Coordinate Systems\n\nThese are specified to the mapping / plotting functions via the Coordinate Reference System (CRS) through functionality built into the sf package.\nYou will also learn how to deal with polygons that cross the dateline (0° wrapping back onto 360°) or the anti-meridian (-180° folding back onto +180°).\n\n1 What Will go Wrong, and Why That’s Useful\nBefore we write any code, here is the promise of this chapter: things will break, and that breakage is the lesson. We will:\n\nStart with a sensible map that still hides problems.\nReproject it and watch polygons break at the anti-meridian.\nFix the topology before projecting, so the fix sticks.\nCompare three zooming strategies and learn when each one helps or hurts.\nEnd with a projection choice that is justified by the question, not habit or because it looks pretty.\n\nIf a map looks strange along the way, assume that it is revealing a rule, not a mistake.\n\n\n2 Load Packages and Data\nI will use the Natural Earth data and manipulate it with the sf package functions. This package integrates well with the tidyverse.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthhires)\n\n\n\n3 Load the Natural Earth Map\nThe rnaturalearth package has the ne_countries() function which we use to load borders of all the countries in the world. I load the medium resolution dataset and make sure the data are of class sf, i.e. a simple features collection.\n\n\n\n\n\n\nNoteI’m Assuming (but Should Probably Tell you)\n\n\n\n\nMedium resolution is a tradeoff: faster plotting, smaller files, and good enough for regional maps.\nCountries are used here instead of coastlines so that later grouping and region selection are easy.\nsf is chosen over sp because it plays well with the tidyverse and ggplot2.\nsf_use_s2(FALSE) opts out of spherical geometry because we will do explicit projection steps and want predictable topology fixes.\n\n\n\n\nsf_use_s2(FALSE)\n\nworld &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3)  \nhead(world[c('continent')])\n\nR&gt; head(world[c('continent')])\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.4537 ymin: -55.9185 xmax: 140.9776 ymax: 7.35578\nCRS:           +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n      continent                       geometry\n0          Asia MULTIPOLYGON (((117.7036 4....\n1          Asia MULTIPOLYGON (((117.7036 4....\n2 South America MULTIPOLYGON (((-69.51009 -...\n3 South America MULTIPOLYGON (((-69.51009 -...\n4 South America MULTIPOLYGON (((-69.51009 -...\n5 South America MULTIPOLYGON (((-67.28475 -...\nMake this a habit: always ask “What CRS am I in right now?” You can check any object with st_crs(world). If you skip this, you will eventually crop or measure in the wrong units.\nNote that for the Natural Earth data the coordinate reference system (CRS) is longitude / latitude in degrees, and it is stored as WGS84. This is the World Geodetic System 1984 commonly used in most GPS devices. The default unit of this CRS is in degrees longitude and latitude.\nFor more information on CRS, see:\n\nThe PROJ system. The proj-string specified with every map can be used in sf; it can be retrieved using st_crs() and one can transform between various projections uing st_transform(). PROJ is written in C++ and loaded automagically with sf.\nThe EPSG coordinate codes, which provide a convenient shortcut to the longer proj-string. Navigating the a page for a projection — WGS84 known as EPSG:4326 — gives one the various relevant details, and the proj-string can be located in the PROJ.4 tab under Exports.\n\nMore information about the map data is available with the head(world) function (as seen above), namely that the longitude goes from -180° (180° west of the prime meridian) to +180° (180° east of the prime meridian). This means that the anti-meridian cuts some of the polygons in the Pacific along the line where -180° wraps back onto +180°, and this can be problematic for maps of the Pacific. We will get to this later. The latitude goes from -90° (90° south of the equator) to +90° (90° north of the equator).\nA very quick map looks like this:\n\nggplot() +\n  geom_sf(data = world, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nFigure 1: World map in WGS84 using Natural Earth countries.\n\n\n\n\n\nYou can see above that Africa is centrally positioned. However, I want to focus on the western Pacific region. I am also going to apply a new projection (ESRI:53077, the Natural Earth projection) to it. The “rotation” is accomplished by setting lon_0=170 in the proj-string.\n\nNE_proj &lt;- \"+proj=natearth +lon_0=170 \"\n\nworld_0 &lt;- world |&gt; \n  st_transform(NE_proj)\n\nggplot() +\n  geom_sf(data = world_0, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nFigure 2: Natural Earth projection centred on 170E showing anti-meridian artefacts.\n\n\n\n\n\nThe western Pacific is now focal, but the map looks strange to say the least. This is a topological failure caused by how polygons cross the anti-meridian.\nTo separate the ideas:\n\nProjection choice: Natural Earth is reasonable for global context.\nCentral meridian choice: setting lon_0 = 170 is intentional to centre the Pacific.\nTopology failure: polygons that cross the anti-meridian are now stitched together incorrectly.\n\nThis is why we must fix topology before projection. I can fix it using st_break_antimeridian(), but to do so I must start with unprojected data, and only then apply this function.\n\nworld_1 &lt;- ne_countries(returnclass = 'sf',\n  scale = 10, type = \"countries\") |&gt; \n  select(continent, sovereignt, iso_a3) |&gt; \n  st_break_antimeridian(lon_0 = 170) |&gt; \n  st_transform(NE_proj)\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nFigure 3: Natural Earth projection after anti-meridian repair.\n\n\n\n\n\nReusable rule: If polygons cross the anti-meridian, repair topology in geographic coordinates before you project.\nSilent failure warning: many maps will still look “fine” after a bad projection, but the geometry will be wrong. Always fix topology first, or you will eventually crop, dissolve, or measure the wrong shapes.\n\n\n4 Zooming In\nThere are three options for focusing in on a particular area of the map (zooming in):\n\nselecting only certain areas of interest from the spatial dataset (e.g., only certain countries / continent(s) / etc.);\ncropping the geometries in the spatial dataset using sf_crop();\nrestricting the display window via coord_sf().\n\nI will look at each in some detail.\nHere is the quick decision logic:\n\n\n\n\n\n\n\n\nStrategy\nBest when…\nRisk / downside\n\n\n\n\nSelect features\nYou want to keep whole countries/regions intact.\nMay exclude necessary context (e.g., partial islands).\n\n\nCrop geometry (st_crop)\nYou want a tight, data-driven extent.\nYou are modifying the geometry (not just the view).\n\n\nView window (coord_sf)\nYou want to preserve original geometries.\nEasy to hide geometry problems and mislead readers.\n\n\n\n\n\n5 Selecting Areas of Interest\nI am interested only in the “continent” called Oceania, which includes the Pacific Islands, Australia, and New Zealand. More correctly, it is a geographical region, not a continent. It is comprised of Australasia and Melanesia, Micronesia, and Polynesia which span the Eastern and Western Hemispheres.\n\noceania &lt;- world_1[world_1$continent == 'Oceania',]\nggplot() +\n  geom_sf(data = oceania, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nFigure 4: Oceania subset from the repaired world map.\n\n\n\n\n\nZooming in on only the group of nations included with Oceania reveals another problem. This is, only part of New Guinea is displayed: Papua New Guinea appears on the map but the western side of New Guinea, called Western New Guinea, is absent. This is because Papua New Guinea is part of Micronesia whereas West New Guinea is part of Indonesia (part of the South-eastern Asia region).\nThere is no easy way to select the countries that constitute Australasia, Melanesia, Micronesia, Indonesia, and Polynesia — unless I create an exhaustive list of these small island nations. But I can use the countrycode package to insert an attribute with the geographic regional classification of the United Nations in the world_1 map. Now all the constituent countries belonging to these regions will be correctly classified to the UN regional classification scheme. Note that I also collapse the countries into their continents by merging all polygons belonging to the same continent (the group_by() and summarise() functions) — I do this because I do not want to display individual countries.\n\n\n\n\n\n\nTipAdvanced Pattern (recognition &gt; memorisation)\n\n\n\nThis region + dissolve workflow is advanced. You do not need to memorise it. Learn to recognise when you need:\n\na classification table (UN regions),\nan attribute join,\nand a dissolve (group_by() + summarise()).\n\n\n\n\nlibrary(countrycode)\n\nworld_1$region &lt;- countrycode(world_1$iso_a3, origin = \"iso3c\",\n  destination = \"un.regionsub.name\")\n\nsw_pacific &lt;- world_1 |&gt; \n  filter(region %in% c(\"Australia and New Zealand\", \"Melanesia\", \"Micronesia\",\n    \"Indonesia\", \"Polynesia\", \"South-eastern Asia\")) |&gt; \n  group_by(continent) |&gt;\n  summarise()\n\nggplot() +\n  geom_sf(data = sw_pacific, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nFigure 5: Southwest Pacific regions dissolved using UN classifications.\n\n\n\n\n\nAs we can see above, by including the South-eastern Asia region I complete the mass of land that is New Guinea.\n\n\n6 Cropping\nThe above map is good but not perfect. I have in mind zooming in a bit more into the region around Fiji where the earthquake monitoring network is located. One way to do this is to crop the extent of the study region using a bounding box whose boundaries are defined by the extent of the quakes datapoints.\nTo start, I use the earthquake data, extract from there the study domain and increase the edges all round by a given margin — this is so that the stations are not plotted right on the map’s edges.\nImportant! The coordinates in the quakes data are provided in WGS84, so I need to first specify them as such, then transform them to the same coordinate system used by the map.\nWhy this is a good habit: a data-derived bounding box keeps your map honest. It is reproducible, it adapts if the data change, and it prevents you from “hand-drawing” an extent that hides inconvenient points.\n\nquakes &lt;- as_tibble(datasets::quakes)\nmargin &lt;- 15.0\nxmin &lt;- min(quakes$long) - margin; xmax &lt;- max(quakes$long) + margin\nymin &lt;- min(quakes$lat) - margin; ymax &lt;- max(quakes$lat) + margin\n\nWGS84_proj &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\nbbox &lt;- st_sfc(st_point(c(xmin, ymin)), st_point(c(xmax, ymax)),\n                         crs = WGS84_proj)\nbbox_trans &lt;- st_transform(bbox, NE_proj)\n\nsw_pacific_cropped &lt;- sw_pacific |&gt; \n  st_crop(bbox_trans)\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"black\", fill = \"grey70\")\n\n\n\n\n\n\n\nFigure 6: Data-derived bounding box used to crop the Southwest Pacific map.\n\n\n\n\n\nThat looks decent enough, but there is another way to accomplish the same.\n\n\n7 Setting the Mapping Limits in ggplot2\nThe third approach to get closer to the region of interest is to use the full map extent (the world) loaded at the beginning, but to set the limits of the view window within the coord_sf() function in ggplot().\nTo do this, I start with the bbox_sf_trans object, which was obtained by first specifying the coordinates marking the extent of the map in the WGS84 coordinate system, then transforming them to Natural Earth. We can extract the transformed limits withst_coordinates() and supply them to the map.\n\nbbox_trans_coord &lt;- st_coordinates(bbox_trans)\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\") +\n  coord_sf(xlim = bbox_trans_coord[,'X'], ylim = bbox_trans_coord[,'Y'],\n    expand = FALSE)\n\n\n\n\n\n\n\nFigure 7: World map windowed by transformed bounding box coordinates.\n\n\n\n\n\nGreat! This works well. Note that the coordinates on the graticule are in degrees longitude and latitude, the default for WGS84. By setting datum = NE_proj ensures the graticule is displayed in Natural Earth coordinate system units, which is meters. This might look strange at first, but it is not wrong.\n\n\n\n\n\n\nWarningPitfall: Correct Can Still Be Confusing\n\n\n\ncoord_sf() can show correct graticule units that are unfamiliar to readers. If your audience expects degrees, a meter-based graticule can mislead even if it is technically correct. Always decide for communicative clarity, not just mathematical correctness.\n\n\n\nggplot() +\n  geom_sf(data = world_1, colour = \"black\", fill = \"grey70\") +\n  coord_sf(xlim = bbox_trans_coord[,'X'], ylim = bbox_trans_coord[,'Y'],\n    expand = FALSE, datum = NE_proj)\n\n\n\n\n\n\n\nFigure 8: Natural Earth map with projected graticule via coord_sf.\n\n\n\n\n\n\n\n8 Adding the quakes Data as Points\nIn order to plot the quakes data, I need to create a sf object from the quakes data. When converting the dataframe to class sf, I must also assign to it the CRS associated of the original dataset. This would be WGS84. Afterwards I will transform it to the Natural Earth CRS.\n\nquakes_sf &lt;- quakes |&gt; \n  st_as_sf(coords = c(\"long\", \"lat\"),\n    crs = WGS84_proj)\nquakes_sf_trans &lt;- st_transform(quakes_sf, NE_proj)\nhead(quakes_sf_trans)\n\nR&gt; head(quakes_sf_trans)\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1047820 ymin: -2923232 xmax: 1361900 ymax: -2017755\nCRS:           +proj=natearth +lon_0=170 \n# A tibble: 6 × 4\n  depth   mag stations           geometry\n  &lt;int&gt; &lt;dbl&gt;    &lt;int&gt;        &lt;POINT [m]&gt;\n1   562   4.8       41 (1104307 -2293735)\n2   650   4.2       15 (1047820 -2316276)\n3    42   5.4       43 (1323082 -2923232)\n4   626   4.1       19 (1113132 -2017755)\n5   649   4         11 (1136619 -2293735)\n6   195   4         12 (1361900 -2210349)\nI am going to make a map of the Fiji region and plot the spatial location of the earthquakes, and scale the points indicating the earthquakes’ magnitude by their intensity (mag).\nStylistic choices (these are not defaults):\n\nStar shapes draw attention and visually separate points from land polygons.\nColour and size both map to magnitude to reinforce the same message (redundant encoding).\nThe size legend is suppressed to avoid repeating the same information twice.\n\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"black\", fill = \"grey70\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = mag, size = mag),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.4) +\n  scale_colour_continuous(type = \"viridis\") +\n  guides(size = \"none\") +\n  coord_sf(expand = FALSE) +\n  labs(x = NULL, y = NULL,\n    title = \"The Fiji Earthquake Data\",\n    subtitle = \"Natural Earth\")\n\n\n\n\n\n\n\nFigure 9: Fiji region with earthquake points sized and coloured by magnitude.\n\n\n\n\n\nNow I apply a more appropriate CRS to the map. This is the WGS 84 / NIWA Albers projection (EPSG:9191), which is designed for the southwestern Pacific and the Southern Ocean around New Zealand.\nWhy this is a better choice here:\n\nIt is an equal-area projection, so area comparisons are meaningful.\nIt centres the region of interest, reducing distortion where we care most.\nIt matches the question (regional patterns in the SW Pacific), not just global aesthetics.\n\n\nNIWA_Albers_proj &lt;- \"+proj=aea +lat_0=-22 +lon_0=175 +lat_1=-20 +lat_2=-30 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs\"\n\nggplot() +\n  geom_sf(data = sw_pacific_cropped, colour = \"indianred\", fill = \"beige\") +\n  geom_sf(data = quakes_sf_trans, aes(colour = mag, size = mag),\n    stat = \"sf_coordinates\",\n    shape = \"*\", alpha = 0.6) +\n  scale_colour_continuous(type = \"viridis\") +\n  guides(size = \"none\",\n    colour = guide_colourbar(\"Magnitude\")) +\n  coord_sf(expand = FALSE,\n    crs = NIWA_Albers_proj) +\n  labs(x = NULL, y = NULL,\n    title = \"The Fiji Earthquake Data\",\n    subtitle = \"WGS 84 / NIWA Albers\") +\n  theme_minimal() \n\n\n\n\n\n\n\nFigure 10: Fiji region in NIWA Albers projection with earthquake magnitudes.\n\n\n\n\n\n\n\n9 CRS Decision Checklist\nBefore writing mapping code, ask:\n\nWhat is my data’s native CRS? Check with st_crs().\nDo I need metric units? If yes, project before measuring/cropping.\nWhat should be preserved? Area, distance, or shape?\nWhere is my region of interest? Choose a projection that minimises distortion there.\n\n\n\n10 Explicit Failure Modes\nThese are the mistakes that quietly ruin maps:\n\nMismatched CRS between layers (data align but are actually offset).\nCropping in the wrong CRS (degree-based bounds applied to projected data).\nProjection before topology repair (anti-meridian polygons break).\n\n\n\n11 Conceptual Summary\nThis chapter is not just about making maps; it is about reasoning with spatial data. You learned to:\n\ncheck CRS as a habit,\ntreat projection artefacts as topological problems,\nchoose zooming strategies intentionally,\nand justify a projection based on the question you are asking.\n\n\n\n12 CRS and Spatial Workflow Decision Tree\nUse this before you write a single line of mapping code:\nStart\n |\n |-- What CRS is my data in? ----&gt; st_crs()\n |        |\n |        |-- Geographic (degrees)? --&gt; If you need metric units, project.\n |        |-- Projected (meters)? ----&gt; Ensure all layers match.\n |\n |-- Do polygons cross the anti-meridian?\n |        |\n |        |-- Yes --&gt; Fix topology in geographic CRS (st_break_antimeridian)\n |        |           THEN project.\n |        |-- No  --&gt; Continue.\n |\n |-- What is my goal?\n |        |\n |        |-- Preserve area? ----&gt; Choose an equal-area projection.\n |        |-- Preserve distance? -&gt; Choose an equidistant projection.\n |        |-- Preserve shape? ----&gt; Choose a conformal projection.\n |\n |-- How to zoom?\n |        |\n |        |-- Need whole features? -&gt; Select features.\n |        |-- Need tight extent? ----&gt; st_crop() (geometry changes).\n |        |-- Need full geometry? ---&gt; coord_sf() (view only).\n |\n |-- Final check\n          |\n          |-- Are layers aligned? Are units clear to readers?\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {12. {The} {Fiji} {Earthquake} {Data}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/12-mapping_quakes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 12. The Fiji Earthquake Data. http://samos-r.netlify.app/intro_r/12-mapping_quakes.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "12. The Fiji Earthquake Data"
    ]
  },
  {
    "objectID": "intro_r/10-mapping_style.html",
    "href": "intro_r/10-mapping_style.html",
    "title": "10. Mapping with Style",
    "section": "",
    "text": "A reminder that maps can mislead without context.\n\n\n\n“The world is complex, dynamic, multidimensional; the paper is static, flat. How are we to represent the rich visual world of experience and measurement on mere flatland?”\n— Edward Tufte\n\n\n“Science flies you to the moon. Religion flies you into buildings.”\n— Victor Stenger\n\nNow that you have learned the basics of creating a beautiful map in ggplot2 it is time to look at some of the more particular things you will need to make your maps extra stylish. There are also a few more things you need to learn how to do before your maps can be truly publication quality.\nAt this point, it is important to remember that style is not decoration; it is constraint. Maps are arguments, not illustrations. Every stylistic choice should help the reader orient themselves, interpret the data honestly, and see the intended message without confusion.\nIf we have not yet loaded the tidyverse let us do so.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggspatial)\n\n# Load Africa map\nload(here::here(\"data\", \"BCB744\", \"africa_map.RData\"))\n\n\n1 Default Maps\nIn order to access the default maps included with the tidyverse we will use the function borders().\n\n\n\n\n\n\nNoteBorders vs Shapefiles\n\n\n\nborders() is convenient and quick, but it is coarse and imprecise. Use it for rough context or exploratory work. For publishable maps or analysis, load proper spatial data (e.g., shapefiles or sf objects) with correct boundaries and metadata.\n\n\n\nggplot() +\n  borders(col = \"black\", fill = \"cornsilk\", size = 0.2) + # The global shape file\n  coord_equal() # Equal sizing for lon/lat\n\n\n\n\n\n\n\nFigure 1: The built in global shape file.\n\n\n\n\n\nJikes! It is as simple as that to load a map of the whole planet. Usually you are not going to want to make a map of the entire planet, so let us see how to focus on just the area around South Africa.\n\n\n\n\n\n\nWarningAspect Ratio Matters\n\n\n\ncoord_equal() preserves a 1:1 relationship between x and y units. Without it, the map can look fine but be geometrically misleading.\n\n\nHere is the next iteration. What is different about the code that causes the map to look very different?\n\nsa_1 &lt;- ggplot() +\n  borders(size = 0.2, fill = \"cornsilk\", colour = \"black\") +\n  coord_equal(xlim = c(12, 36), ylim = c(-38, -22), expand = 0) # Force lon/lat extent\nsa_1\n\n\n\n\n\n\n\nFigure 2: A better way to get the map of South Africa.\n\n\n\n\n\nThat is a very tidy looking map of South(ern) Africa without needing to load any files. The extent values (xlim, ylim) are a design decision: too tight and you clip context; too loose and you lose focus. Use the scientific question to guide the bounds.\n\n\n2 Specific Labels\nA map is almost always going to need some labels and other visual cues. You saw in the previous section how to add site labels. The following code chunk shows how this differs if you want to add just one label at a time. This can be useful if each label needs to be different from all other labels for whatever reason. You may also see that the text labels we are creating have \\n in them. When R sees these two characters together like this it reads this as an instruction to return down a line. Let us run the code to make sure you see what this means.\nLabel placement is one of the hardest problems in cartography. A few rules you should break only deliberately:\n\nAvoid overlaps and do not obscure the data.\nRespect visual hierarchy: water bodies should not visually overpower land, and context labels should not dominate the data.\nUse style (size, colour, angle) only to encode meaning, not decoration.\n\nAlso note the distinction between manual labels (a small number of contextual labels) and data-driven labels (labels generated from your data). Manual labels are appropriate for oceans, regions, and context; data-driven labels should be automated and consistent.\n\nsa_2 &lt;- sa_1 +\n  annotate(\n    \"text\",\n    label = \"Atlantic\\nOcean\",\n    x = 15.1,\n    y = -32.0,\n    size = 5.0,\n    angle = 30,\n    colour = \"navy\"\n  ) +\n  annotate(\n    \"text\",\n    label = \"Indian\\nOcean\",\n    x = 33.2,\n    y = -34.2,\n    size = 5.0,\n    angle = 330,\n    colour = \"red4\"\n  )\nsa_2\n\n\n\n\n\n\n\nFigure 3: Map of southern Africa with specific labels.\n\n\n\n\n\n\n\n3 Scale Bars\nWith your fancy labels added, let us insert a scale bar next. There is no default scale bar function in the tidyverse, which is why you have loaded ggspatial. This package is devoted to adding scale bars and North arrows to ggplot2 figures. There are heaps of options so you will just focus on one of them for now. It is a bit finicky so to get it looking exactly how you want it requires some guessing and checking. Please feel free to play around with the coordinates below.\nDecision rules:\n\nInclude a scale bar when distances matter to interpretation.\nOmit it on very small-scale maps or when it adds clutter without value.\nUse a north arrow only if orientation is ambiguous or non-standard.\n\n\nsa_3 &lt;- sa_2 +\n  annotation_scale(\n    location = \"bl\", # bottom left; you can also specify \"tl\", \"tr\", etc.\n    width_hint = 0.2, # relative width of the scale bar\n    height = unit(0.3, \"cm\"), # scale bar height\n    text_cex = 0.8, # text size\n    pad_x = unit(0.5, \"cm\"), # horizontal offset\n    pad_y = unit(0.5, \"cm\"), # vertical offset\n    bar_cols = c(\"black\", \"white\")\n  ) +\n  annotation_north_arrow(\n    location = \"bl\",\n    which_north = \"true\", # true north, not grid north\n    pad_x = unit(3, \"cm\"), # adjust to approximate your original x.min/x.max\n    pad_y = unit(2, \"cm\"), # adjust to approximate your original y.min/y.max\n    style = north_arrow_fancy_orienteering\n  )\n\nsa_3\n\n\n\n\n\n\n\nFigure 4: Map of southern Africa with labels and a scale bar.\n\n\n\n\n\n\n\n4 Advanced Styling: Insets and Coordinate Labels\nInset maps, coordinate labels, and export are not optional niceties — they are often what distinguishes a competent map from a careless one. If your main map needs geographic context, use an inset. Here we build a simple Africa inset using borders() so it stays lightweight and reliable.\n\nafrica_inset &lt;- ggplot() +\n  borders(colour = \"black\", fill = \"cornsilk\", size = 0.2) +\n  coord_equal(xlim = c(-20, 55), ylim = c(-36, 38), expand = 0)\nafrica_inset\n\n\n\n\nInset map of Africa (context).\n\n\n\n\nWe can add the inset using a grob. This is a controlled way to place an entire plot inside another plot.\n\nafrica_grob &lt;- ggplotGrob(africa_inset)\n\nsa_4 &lt;- sa_3 +\n  annotation_custom(\n    grob = africa_grob,\n    xmin = 20.9, xmax = 26.9,\n    ymin = -30, ymax = -24\n  )\nsa_4\n\n\n\n\nMap of southern Africa with labels, scale bar, and an inset map of Africa.\n\n\n\n\nNow tweak coordinate labels for cartographic conventions:\n\nsa_final &lt;- sa_4 +\n  scale_x_continuous(\n    breaks = seq(16, 32, 4),\n    labels = c(\"16°E\", \"20°E\", \"24°E\", \"28°E\", \"32°E\"),\n    position = \"bottom\"\n  ) +\n  scale_y_continuous(\n    breaks = seq(-36, -24, 4),\n    labels = c(\"36.0°S\", \"32.0°S\", \"28.0°S\", \"24.0°S\"),\n    position = \"right\"\n  ) +\n  labs(x = \"\", y = \"\")\nsa_final\n\n\n\n\nThe final map with coordinate labels.\n\n\n\n\nExport with intention — size, aspect ratio, and resolution must match the medium.\n\nggsave(\n  plot = sa_final,\n  filename = \"figures/southern_africa_final.pdf\",\n  height = 6,\n  width = 8,\n  dpi = 300\n)\n\n\n\n\n\n\n\nNoteMap Sanity Checklist\n\n\n\n\nClear title and legend?\nCoordinate system and aspect ratio appropriate?\nLabels readable and not obscuring data?\nScale bar or north arrow included only when needed?\nExport size and resolution appropriate for the audience?\n\n\n\n\n\n\n\n\n\nWarningCommon Student Errors\n\n\n\n\nOver‑labelling and visual clutter\nDecorative colours that imply meaning\nCropping boundaries in ways that mislead\nUsing default borders when precision is required\n\n\n\n\n\n\n\n\n\nNoteAnalytic vs Illustrative Maps\n\n\n\nAnalytic maps emphasise data integrity and scale; illustrative maps prioritise narrative and clarity. Decide which you are making before you style.\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {10. {Mapping} with {Style}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/10-mapping_style.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 10. Mapping with Style. http://samos-r.netlify.app/intro_r/10-mapping_style.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "10. Mapping with Style"
    ]
  },
  {
    "objectID": "intro_r/08-brewing.html",
    "href": "intro_r/08-brewing.html",
    "title": "8. Brewing Colours",
    "section": "",
    "text": "Colour palette inspiration.\n\n\n\n“Microbiology and meteorology now explain what only a few centuries ago was considered sufficient cause to burn women to death.”\n— Carl Sagan\n\n\n“Knowledge is not a resource we simply stumble upon. It’s not something that we pluck out of the air. Knowledge is created. It is coaxed into existence by thoughtful, creative people. It is not a free good. It comes only to the prepared mind.”\n— Frank H. T. Rhodes\n\nNow that you have seen the basics of ggplot2, let us take a moment to delve further into the beauty of our figures. It may sound vain at first, but the colour palette of a figure is actually very important. This is for two main reasons. The first being that a consistent colour palette looks more professional. But most importantly it is necessary to have a good colour palette because it makes the information in our figures easier to understand. The communication of information to others is central to good science.\nBefore we touch the tools, note the following: colour can encode magnitude or category. If the variable is numeric and ordered, use a continuous scale that shows gradients. If the variable is categorical, use a discrete scale that separates groups. Everything that follows is an application of this single decision.\n\n\n\n\n\n\nNoteColour Scale Decision Tree\n\n\n\n\nIs the variable ordered with meaningful distance?\nYes → use a continuous scale.\nNo → use a discrete scale.\nIs there a meaningful midpoint (e.g. above vs below zero)?\nYes → use a diverging palette.\nNo → use a sequential palette.\nHow many categories do you need to distinguish?\nMore categories require more separable hues — keep the legend visible.\n\n\n\n\n\n\n\n\n\nWarningAccessibility and Reproducibility\n\n\n\nColour choices are part of your evidence. Aim for palettes that remain legible for colour‑blind viewers and print well in greyscale. Keep your palette definitions in code so others can reproduce the exact mapping.\n\n\n\n1 R Data (Choosing a Dataset for Colour)\nThis chapter is about colour, so we need a dataset where colour matters.\nFor SAMOS, we will use the WOA18 core dataset because oceanographic variables are inherently spatial and gradients are often the point of the figure.\n\n\n\n\n\n\nNoteAbout the dataset used in this chapter (World Ocean Atlas 2018)\n\n\n\nIn this chapter we use a small, tidy extract of World Ocean Atlas 2018 (WOA18) climatologies for the broader Southern Africa region.\nWhy WOA matters in ocean science:\n\nTemperature and salinity are the fundamental state variables of seawater, and together shape density and stratification.\nDissolved oxygen is a key indicator of ventilation, productivity, and habitat suitability.\nNutrients (nitrate, phosphate, silicate) constrain primary production and structure ecosystems.\n\nThese variables are not “just numbers”: they encode the physical and biogeochemical structure of the ocean.\n\n\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(here)\n\n# Load the core teaching dataset (WOA18 climatology extract)\nwoa &lt;- readr::read_csv(\n  here::here(\"data\", \"SAMOS\", \"processed\", \"woa18_sa_core_1deg_monthly.csv\"),\n  show_col_types = FALSE\n)\n\n# Quick look\nglimpse(woa)\n\nR&gt; Rows: 200,382\nR&gt; Columns: 8\nR&gt; $ lat      &lt;dbl&gt; -44.5, -44.5, -44.5, -44.5, -44.5, -44.5, -44.5, -44.5, -44.5…\nR&gt; $ lon      &lt;dbl&gt; 6.5, 7.5, 9.5, 12.5, 14.5, 15.5, 19.5, 20.5, 22.5, 24.5, 26.5…\nR&gt; $ depth_m  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\nR&gt; $ value    &lt;dbl&gt; NA, 295.308, 295.840, NA, 280.251, NA, 270.377, 270.764, 289.…\nR&gt; $ month    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\nR&gt; $ variable &lt;chr&gt; \"dissolved_oxygen\", \"dissolved_oxygen\", \"dissolved_oxygen\", \"…\nR&gt; $ unit     &lt;chr&gt; \"umol/kg\", \"umol/kg\", \"umol/kg\", \"umol/kg\", \"umol/kg\", \"umol/…\nR&gt; $ source   &lt;chr&gt; \"WOA18 decav 1.00° CSV\", \"WOA18 decav 1.00° CSV\", \"WOA18 deca…\n\n\n\n\n\n\n\n\nTipData dictionary\n\n\n\nSee: data/SAMOS/processed/woa18_sa_core_1deg_monthly_DICTIONARY.md\n\n\nWe will use:\n\na continuous variable (e.g. temperature or dissolved oxygen) to demonstrate continuous palettes;\na categorical variable (e.g. depth bin) to demonstrate discrete palettes.\n\n\n# To create a list of ALL available data\n  # Not really recommended as the output is overwhelming\ndata(package = .packages(all.available = TRUE))\n\n# To look for datasets within a single known package\n  # type the name of the package followed by '::'\n  # This tells R you want to look in the specified package\n  # When the autocomplete bubble comes up you may scroll\n  # through it with the up and down arrows\n  # Look for objects that have a mini spreadsheet icon\n  # These are the datasets\n\n# Try typing the following code and see what happens...\ndatasets::\n\nYou have an amazing amount of data available to you. So the challenge is not to find a dataframe that works for you, but to just decide on one. My preferred method is to read the short descriptions of the dataframes, pick the one that sounds the funniest. But please use whatever method makes the most sense to you. One note of caution and in R there are generally two different forms of data: wide OR long. You will see in detail what this means on Day 4, and what to do about it. For now you need to know that ggplot2 works much better with long data. To look at a dataframe of interest, you use the same method you would use to look up a help file for a function.\nWe start with a continuous colour scale. Below we map dissolved oxygen (continuous) to colour in a temperature–salinity plot (surface February climatology).\n\nwoa %&gt;%\n  filter(month == 2, depth_m == 0, variable %in% c(\"temperature\", \"salinity\", \"dissolved_oxygen\")) %&gt;%\n  select(lon, lat, variable, value) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  ggplot(aes(x = salinity, y = temperature)) +\n  geom_point(aes(colour = dissolved_oxygen), alpha = 0.65, size = 0.8) +\n  scale_colour_viridis_c(name = \"Oxygen (µmol/kg)\") +\n  labs(x = \"Salinity (PSU)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 1: WOA18: temperature vs salinity coloured by dissolved oxygen (continuous palette).\n\n\n\n\n\nWhat is important to note here is that the colour scale is continuous. How can we know this by looking at the figure? Look for a smooth gradient (ordered transitions) and a bar-like legend rather than separate keys.\nRule of thumb: if your variable has a meaningful order and distance, treat it as continuous; otherwise treat it as discrete.\nNow we use a discrete variable for colouring. We do this by converting depth to a factor (categorical bins).\n\nwoa %&gt;%\n  filter(month == 2, variable == \"temperature\", depth_m %in% c(0, 50, 100, 200, 500)) %&gt;%\n  ggplot(aes(x = lat, y = value)) +\n  geom_point(aes(colour = factor(depth_m)), alpha = 0.35, size = 0.7) +\n  labs(x = \"Latitude (°N)\", y = \"Temperature (°C)\", colour = \"Depth (m)\")\n\n\n\n\n\n\n\nFigure 2: WOA18: temperature vs latitude coloured by depth (discrete palette).\n\n\n\n\n\nWhat is the first thing you notice about the difference in the colours? Notice the distinct hues and the key-style legend (one swatch per category).\n\n\n\n\n\n\nWarningBad Colour Choices (and Why They Fail)\n\n\n\n\nCategorical data shown with a gradient → implies a false order.\nOrdered magnitudes shown with random hues → hides trends and distances.\nToo many categories in one palette → colours become indistinguishable and the legend stops working.\n\n\n\n\n\n2 RColorBrewer\nCentral to the purpose of ggplot2 is the creation of beautiful figures. For this reason there are many built in functions that you may use in order to have precise control over the colours, as well as additional packages that extend your options even further. The RColorBrewer package should have been installed on your computer, activated automatically when you installed and activated the tidyverse. You will use this package for its colour palettes.\nRColorBrewer groups palettes by purpose: sequential (magnitude), diverging (deviation from a midpoint), and qualitative (categories). This matters more than style. Also remember that some palettes are not colour-blind safe and may not reproduce well in print. When in doubt, favour clarity over prettiness.\nIn ggplot2, scale functions are the bridge between data and appearance. To this end, scale_colour_*() tells ggplot how to translate values into colours. Let us modify and pretty-up the previous continuous colour scale figure now.\n\n\n\n\n\n\nNoteScale Functions (Quick Map)\n\n\n\n\nContinuous: scale_colour_gradient(), scale_colour_gradientn(), scale_colour_distiller()\nDiscrete: scale_colour_brewer(), scale_colour_manual()\nFill versions: replace colour with fill (e.g. scale_fill_brewer()). Think of these as a family of translators between data and appearance.\n\n\n\n\nwoa %&gt;%\n  filter(month == 2, depth_m == 0, variable %in% c(\"temperature\", \"salinity\", \"dissolved_oxygen\")) %&gt;%\n  select(lon, lat, variable, value) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  ggplot(aes(x = salinity, y = temperature)) +\n  geom_point(aes(colour = dissolved_oxygen), alpha = 0.65, size = 0.8) +\n  scale_colour_distiller(palette = \"YlGnBu\", name = \"Oxygen (µmol/kg)\") +\n  labs(x = \"Salinity (PSU)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 3: WOA18: oxygen-coloured T–S plot using a sequential distiller palette.\n\n\n\n\n\nDoes this look different? If so, how? The second page of the colour cheat sheet we included in the course material shows some different colour brewer palettes. Let us look at how to use those here.\n\nwoa %&gt;%\n  filter(month == 2, depth_m == 0, variable %in% c(\"temperature\", \"salinity\", \"dissolved_oxygen\")) %&gt;%\n  select(lon, lat, variable, value) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  ggplot(aes(x = salinity, y = temperature)) +\n  geom_point(aes(colour = dissolved_oxygen), alpha = 0.65, size = 0.8) +\n  scale_colour_distiller(palette = \"Spectral\", name = \"Oxygen (µmol/kg)\") +\n  labs(x = \"Salinity (PSU)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 4: WOA18: same plot with a different distiller palette (Spectral).\n\n\n\n\n\nDoes that help you to see a pattern in the data? What do you see? Does it look like there are any significant relationships here? How would you test that?\nIf you want to use RColorBrewer with a discrete variable, you use a slightly different function.\n\nwoa %&gt;%\n  filter(month == 2, variable == \"temperature\", depth_m %in% c(0, 50, 100, 200, 500)) %&gt;%\n  ggplot(aes(x = lat, y = value)) +\n  geom_point(aes(colour = factor(depth_m)), alpha = 0.35, size = 0.7) +\n  scale_colour_brewer(palette = \"Set1\", name = \"Depth (m)\") +\n  labs(x = \"Latitude (°N)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 5: WOA18: temperature vs latitude coloured by depth (discrete brewer palette).\n\n\n\n\n\nThe default colour scale here is not helpful at all. So let us pick a better one. If you look at our cheat sheet you will see a list of different continuous and discrete colour scales. All you need to do is copy and paste one of these names into your colour brewer function with inverted commas.\n\nwoa %&gt;%\n  filter(month == 2, variable == \"temperature\", depth_m %in% c(0, 50, 100, 200, 500)) %&gt;%\n  ggplot(aes(x = lat, y = value)) +\n  geom_point(aes(colour = factor(depth_m)), alpha = 0.35, size = 0.7) +\n  scale_colour_brewer(palette = \"Dark2\", name = \"Depth (m)\") +\n  labs(x = \"Latitude (°N)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 6: Same plot but with a different discrete palette (Dark2).\n\n\n\n\n\nNotice the pattern: scale_colour_distiller() is typically used for continuous variables, while scale_colour_brewer() is typically used for discrete variables. They are not interchangeable because they encode different kinds of meaning.\n\n\n\n\n\n\nWarningA Common Misuse\n\n\n\nUsing a continuous palette for categorical data suggests a false order. Using a discrete palette for ordered magnitudes hides gradients. If the palette and the data type do not match, the figure becomes misleading.\n\n\n\n\n3 Worked Examples With a New Dataset (Iris)\nLet us reinforce the ideas using a fresh dataset. The built-in iris data give us both continuous variables (e.g. petal length) and categories (species), which makes it perfect for colour decisions.\n\n# Load data\niris_df &lt;- datasets::iris\n\n# Continuous colour: petal length (magnitude)\nggplot(data = iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(colour = Petal.Length)) +\n  labs(x = \"Sepal length\", y = \"Sepal width\", colour = \"Petal length\") +\n  scale_colour_distiller(palette = \"YlGnBu\")\n\n\n\n\n\n\n\nFigure 7: Iris sepal scatterplot coloured by petal length (sequential palette).\n\n\n\n\n\nNotice the smooth gradient and the bar-style legend — this signals an ordered, numeric scale. If you want to emphasise departures above and below a midpoint, use a diverging palette:\n\n# Diverging colour around a midpoint (mean petal length)\nggplot(data = iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(colour = Petal.Length)) +\n  labs(x = \"Sepal length\", y = \"Sepal width\", colour = \"Petal length\") +\n  scale_colour_distiller(palette = \"RdBu\")\n\n\n\n\n\n\n\nFigure 8: Iris sepal scatterplot with a diverging palette for petal length.\n\n\n\n\n\nNow compare a discrete scale where colour represents species (categories):\n\nggplot(data = iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(colour = Species)) +\n  labs(x = \"Sepal length\", y = \"Sepal width\", colour = \"Species\") +\n  scale_colour_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\nFigure 9: Iris sepal scatterplot coloured by species (discrete palette).\n\n\n\n\n\nFinally, here is a bad choice and the fix. A continuous palette for species implies ordering that does not exist:\n\n# Misleading: forcing categories into a continuous scale\nggplot(data = iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(colour = as.numeric(Species))) +\n  scale_colour_distiller(palette = \"Spectral\")\n\n\n\n\n\n\n\nFigure 10: Iris sepal scatterplot with a misleading continuous palette for species.\n\n\n\n\n\n\n# Fix: discrete palette for categorical data\nggplot(data = iris_df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(colour = Species)) +\n  scale_colour_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 11: Iris sepal scatterplot with a correct discrete palette for species.\n\n\n\n\n\nThe rule remains the same: match the data type to the scale type, and make the legend explain the mapping.\n\n\n4 Make Your Own Palettes\nThis is all well and good. But did not I claim that this should give you complete control over our colours? So far it looks like it has just given you a few more palettes to use. And that is nice, but it is not “infinite choices”. That is where the Internet comes to your rescue. There are many places you may go to for support in this regard. The following links, in descending order, are very useful. And fun!\n\nhttp://tristen.ca/hcl-picker/#/hlc/6/0.95/48B4B6/345363\nhttp://tools.medialab.sciences-po.fr/iwanthue/index.php\nhttp://jsfiddle.net/d6wXV/6/embedded/result/\n\nI find the first link the easiest to use. But the second and third links are better at generating discrete colour palettes. Take several minutes playing with the different websites and decide for yourself which one(s) you like.\n\n\n5 Use Your Own Palettes\nBefore you commit to a custom palette, run a quick checklist:\n\nHow many categories do you need to distinguish?\nIs there a meaningful order or magnitude?\nWho is your audience, and will this be printed or viewed on screen?\nDo the legend labels and title communicate what the colours mean?\n\nNow that you have had some time to play around with the colour generators let us look at how to use them with our figures. I have used the first web link to create a list of five colours. I then copy and pasted them into the code below, separating them with commas, placing them inside of c() and inverted commas. Be certain that you insert commas and inverted commas as necessary or you will get errors. Note also that you are using a new function to use our custom palette.\n\nwoa %&gt;%\n  filter(month == 2, depth_m == 0, variable %in% c(\"temperature\", \"salinity\", \"dissolved_oxygen\")) %&gt;%\n  select(lon, lat, variable, value) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  ggplot(aes(x = salinity, y = temperature)) +\n  geom_point(aes(colour = dissolved_oxygen), alpha = 0.65, size = 0.8) +\n  scale_colour_gradientn(\n    colours = c(\"#A5A94D\", \"#6FB16F\", \"#45B19B\", \"#59A9BE\", \"#9699C4\", \"#CA86AD\"),\n    name = \"Oxygen (µmol/kg)\"\n  ) +\n  labs(x = \"Salinity (PSU)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 12: WOA18: custom continuous gradient for dissolved oxygen.\n\n\n\n\n\nTo use your custom colour palettes with a discrete colour scale, you use a different function as seen in the code below. While you are at it, also see how to correct the title of the legend, its text labels. Sometimes the default output is not what you want for our final figure and especially if you are going to be publishing it. Also note in the following code chunk that rather than using hexadecimal character strings to represent colours in your custom palette, you are simply writing in the human name for the colours you want. This will work for the continuous colour palettes above, too.\n\nwoa %&gt;%\n  filter(month == 2, variable == \"temperature\", depth_m %in% c(0, 50, 100, 200, 500)) %&gt;%\n  mutate(depth_bin = factor(depth_m)) %&gt;%\n  ggplot(aes(x = lat, y = value)) +\n  geom_point(aes(colour = depth_bin), alpha = 0.35, size = 0.7) +\n  scale_colour_manual(\n    values = c(\"0\" = \"navy\", \"50\" = \"dodgerblue3\", \"100\" = \"turquoise4\", \"200\" = \"goldenrod2\", \"500\" = \"firebrick3\"),\n    labels = c(\"0\" = \"0 m\", \"50\" = \"50 m\", \"100\" = \"100 m\", \"200\" = \"200 m\", \"500\" = \"500 m\")\n  ) +\n  labs(x = \"Latitude (°N)\", y = \"Temperature (°C)\", colour = \"Depth\")\n\n\n\n\n\n\n\nFigure 13: WOA18: custom discrete palette for depth bins, with legend labels.\n\n\n\n\n\n\n\n\n\n\n\nNotePalette Length and Meaning\n\n\n\nFor discrete palettes, the number of colours should match the number of categories. For continuous palettes, colours should change smoothly and monotonically. Also consider cultural or semantic associations (e.g., red for danger), especially if your audience is non-technical.\n\n\nSo now you have seen how to control the colour palettes in your figures. I know it is a bit much. Four new functions just to change some colours! That is a bummer. Do not forget that one of the main benefits of R is that all of your code is written down, annotated, saved. You do not need to remember which button to click to change the colours and you just need to remember where you saved the code that you will need. And that is pretty great in my opinion.\n\n\n6 Worked Examples With the colorspace Package\nLet us add one more worked example using the colorspace package. It is useful because it provides palettes that are designed to be perceptually uniform and more robust for colour‑blind viewing and print. The dataset below (mtcars) gives us a continuous variable (horsepower) and a categorical variable (number of cylinders).\n\n# Load libraries\nlibrary(colorspace)\n\n# Load data\ncars_df &lt;- datasets::mtcars\n\n# Continuous colour: horsepower (magnitude)\nggplot(data = cars_df, aes(x = wt, y = mpg)) +\n  geom_point(aes(colour = hp)) +\n  labs(x = \"Weight\", y = \"Fuel efficiency (mpg)\", colour = \"Horsepower\") +\n  scale_colour_continuous_sequential(palette = \"BluGrn\")\n\n\n\n\n\n\n\nFigure 14: mtcars weight vs. mpg coloured by horsepower (sequential palette).\n\n\n\n\n\nHere the sequential palette reinforces magnitude (low → high) without sudden jumps. Now map a categorical variable with a qualitative palette:\n\nggplot(data = cars_df, aes(x = wt, y = mpg)) +\n  geom_point(aes(colour = as.factor(cyl))) +\n  labs(x = \"Weight\", y = \"Fuel efficiency (mpg)\", colour = \"Cylinders\") +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\")\n\n\n\n\n\n\n\nFigure 15: mtcars weight vs. mpg coloured by cylinders (qualitative palette).\n\n\n\n\n\nNotice the legend structure again: a continuous bar for horsepower and discrete keys for cylinders. This is the same diagnostic pattern we used earlier.\n\n# Diverging palette for deviations around a midpoint\nggplot(data = cars_df, aes(x = wt, y = mpg)) +\n  geom_point(aes(colour = hp - mean(hp))) +\n  labs(x = \"Weight\", y = \"Fuel efficiency (mpg)\", colour = \"HP deviation\") +\n  scale_colour_continuous_diverging(palette = \"Blue-Red 3\")\n\n\n\n\n\n\n\nFigure 16: mtcars weight vs. mpg coloured by horsepower deviation (diverging palette).\n\n\n\n\n\nUse diverging palettes when the midpoint is meaningful (e.g. above vs below average). If the midpoint is not meaningful, use a sequential palette instead.\n\n\n\n\n\n\nImportantDo This Now\n\n\n\nToday we learned the basics of ggplot2, how to facet, how to brew colours, and how to plot some basic summary statistics. Sjog, that is a lot of stuff to remember… which is why we will now spend the rest of Day 3 putting our new found skills to use.\nPlease group up as you see fit to produce your very own ggplot2 figures. We have not yet learned how to manipulate/tidy up our data so it may be challenging to grab any ol’ dataset and make a plan with it. But try! Explore some of the other built-in datasets and find two or three you like. Or use your own data!\nThe goal by the end of today is to have created four figures and join them together via faceting and the options offered by ggarrange(). We will be walking the room to help with any issues that may arise.\nSuccess criteria:\n\nAt least one figure uses a continuous colour scale and one uses a discrete scale.\nLegends are correctly titled and interpretable.\nColour choices match the data type (no false ordering).\nThe grid or facet layout helps comparison rather than obscures it.\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {8. {Brewing} {Colours}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/08-brewing.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 8. Brewing Colours. http://samos-r.netlify.app/intro_r/08-brewing.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "8. Brewing Colours"
    ]
  },
  {
    "objectID": "intro_r/06-graphics.html#geom_-the-pipe-or-and-the-sign",
    "href": "intro_r/06-graphics.html#geom_-the-pipe-or-and-the-sign",
    "title": "6. Graphics with ggplot2",
    "section": "3.1 geom_*(), the Pipe (%>% or |>), and the + Sign",
    "text": "3.1 geom_*(), the Pipe (%&gt;% or |&gt;), and the + Sign\nTransition: we now move from ggplot as a system to ggplot() as syntax. Keep the first mental model in mind, that is, ggplot builds plots by accumulation.\nAs part of the tidyverse (as we saw briefly on Day 1, and will go into in depth on Day 4), the ggplot2 package endeavours to use a clean, easy for humans to understand syntax that relies heavily on functions that do what they say. For example, the function geom_point() makes points on a figure. Need a line plot? geom_line() is the way to go! Need both at the same time? No problem. In ggplot we may seamlessly merge a nearly limitless number of objects together to create startlingly sophisticated figures.\nBefore we go over the code below, it is very important to note the use of the + signs. This is different from the pipe symbol (|&gt; or %&gt;%) used elsewhere in the tidyverse. The + sign indicates that one set of geometric features is added to another, each building on top of what came before. In other words, we add one geometry on top of the next, and in such a way we can arrive at complex graphical representations of data. Effectively, each line of code represents one new geometric feature with its own aesthetic appearance of the figure. It is designed this way so as to make it easier for the human eye to read through the code.\n\n\n\n\n\n\nNote+ Signs in ggplot() Code\n\n\n\nOne may see below that the code naturally indents itself if the previous line ended with a + sign. This is because R knows that the top line is the parent line and the indented lines are its children. This is a concept that will come up again when we learn about tidying data. What we need to know now is that a block of code that has + signs, like the one below, must be run together. As long as lines of code end in +, R will assume that you want to keep adding lines of code (more geometric features). If we are not mindful of what we are doing we may tell R to do something it cannot and we will see in the console that R keeps expecting more + signs. If this happens, click inside the console window and press the Esc button to cancel the chain of code you are trying to enter.\n\n\n\n\n\n\n\n\nNoteDebugging Ggplot Messages\n\n\n\nMost ggplot2 errors and warnings point to a specific layer or aesthetic. Read them as clues about which layer failed and why. If the message mentions an unknown aesthetic or object, check spelling and whether the variable exists in your data. If it mentions a problem with a scale, check whether you accidentally mapped a constant inside aes() or mixed mapped and constant aesthetics across layers.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "6. Graphics with **ggplot2**"
    ]
  },
  {
    "objectID": "intro_r/06-graphics.html#aes",
    "href": "intro_r/06-graphics.html#aes",
    "title": "6. Graphics with ggplot2",
    "section": "3.2 aes()",
    "text": "3.2 aes()\nTransition: we now move from accumulation to aesthetic mapping. Keep the second mental model in view: aesthetics are data-driven or constant.\nAnother recurring function within the parent ggplot() function or the associated geom_*() is aes(). The aes() function in ggplot2 is used to specify the mapping between variables in a dataframe and visual properties of a plot. aes() stands for ‘aesthetic,’ which refers to the visual elements of a plot, such as colour, size, shape, etc. In ggplot, the aesthetics of a plot are defined inside the aes() function, which is passed as an argument to the base ggplot() function or its associated geometry.\nIt helps to separate positional aesthetics from non-positional aesthetics. Positional aesthetics (x, y) place data in the coordinate system and always create scales. Non-positional aesthetics (colour, size, shape, alpha, group) change how the geometry is drawn, and they may or may not create a scale depending on whether you map them to data.\nFor example, if you have a dataframe with two variables x and y, you can create a scatterplot of x against y by calling ggplot(data, aes(x, y)) + geom_point(). The aes(x, y) function maps the variables (columns) in the dataframe to the x and y positions of the points in the scatterplot. Similarly, we can map variables in the dataframe to non-positional aesthetics, such as colour (e.g., a colour might be more intense as the magnitude of the values in a column increase), size (larger symbols for bigger values), transparency, or grouping.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "6. Graphics with **ggplot2**"
    ]
  },
  {
    "objectID": "intro_r/06-graphics.html#how-to-read-this-code",
    "href": "intro_r/06-graphics.html#how-to-read-this-code",
    "title": "6. Graphics with ggplot2",
    "section": "4.2 How to Read This Code",
    "text": "4.2 How to Read This Code\nRead the plot like a sentence with grammar: subject → verb → modifiers. The subject is the data (here: a filtered slice of woa), the verb is the geometric action (geom_point()), and the modifiers are the aesthetic mappings (aes(...)) that tell ggplot how to draw. The ggplot(...) line sets the stage (data + core mappings), and each geom_*() line adds a new clause. The + sign means “and then add another clause.”\nSo what is that code doing? We may see from the figure that it is creating a dot for every grid cell, placing salinity on the x‑axis and temperature on the y‑axis (a very common oceanography view).\nAs a workflow, it is perfectly normal to build plots incrementally: start with a minimal plot, confirm the axes are correct, then add layers one at a time. Partial plots are not failures; they are thinking tools.\nThe first line of code is telling R that we want to create a ggplot figure. We know this because we are using the ggplot() function. Inside of that function we are telling R which dataframe (or tibble) we want to create a figure from. Lastly, with the aes() function we tell R what the necessary parts of the figure will be. This is also known as ‘mapping’ (variables map to the visual appearance and arrangement of figure elements).\nThe second line of code then takes all of that information and makes points (dots) out of it, added as a layer on the set of axes created by the aes() argument provided within ggplot(...) — in other words, we add a ‘geometry’ layer, and hence the name of the kind of ‘shape’ we want to plot the data as is prefixed by geom_.\nIn many cases (especially time series) you will add a geom_line() layer. With the WOA climatology we typically do not connect points with lines because they are not ordered in time within each location.\nHowever, we can add an extra aesthetic mapping to include a third variable. Below we colour points by dissolved oxygen (surface, February). This is an example of mapping a continuous variable to a continuous colour scale.\n\nwoa %&gt;%\n  filter(month == 2, depth_m == 0, variable %in% c(\"temperature\", \"salinity\", \"dissolved_oxygen\")) %&gt;%\n  select(lon, lat, variable, value) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  ggplot(aes(x = salinity, y = temperature)) +\n  geom_point(aes(colour = dissolved_oxygen), alpha = 0.6, size = 0.8) +\n  scale_colour_viridis_c(name = \"Oxygen (µmol/kg)\") +\n  labs(x = \"Salinity (PSU)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 2: Surface February climatology: T–S scatter coloured by dissolved oxygen.\n\n\n\n\n\nDo any patterns appear to emerge? Typically oxygen is higher in cooler surface waters (and lower in warmer waters), but the relationship is not purely linear because circulation and biology both matter.\nWe can still add a simple best‑fit line through the points to demonstrate geom_smooth():\n\nwoa %&gt;%\n  filter(month == 2, depth_m == 0, variable %in% c(\"temperature\", \"salinity\", \"dissolved_oxygen\")) %&gt;%\n  select(lon, lat, variable, value) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  ggplot(aes(x = temperature, y = dissolved_oxygen)) +\n  geom_point(alpha = 0.35, size = 0.8) +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Temperature (°C)\", y = \"Dissolved oxygen (µmol/kg)\")\n\n\n\n\n\n\n\nFigure 3: Same plot with a simple linear trend (for demonstration).",
    "crumbs": [
      "Home",
      "Introduction to R",
      "6. Graphics with **ggplot2**"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#numeric-variables",
    "href": "intro_r/04-data-in-R.html#numeric-variables",
    "title": "4. Data Classes and Structures",
    "section": "2.1 Numeric Variables",
    "text": "2.1 Numeric Variables\nNumeric data in the context of biostatistics refers to quantitative data that can be expressed in numerical form, typically obtained from field and laboratory measurements or from field sampling campaigns. Examples of numeric data in biostatistics include the height and mass of animals, concentrations of nutrients, laboratory test results such as respiration rates, or the number of limpets in a quadrat. Numeric data can be further categorised as discrete and continuous.\n\n2.1.1 Discrete variables\nDiscrete data are whole (integer) numbers that represent counts of items or events. Integer data usually answer the question, “how many?” For example, in the biological and Earth sciences, discrete data are commonly encountered in the form of counts or integers that represent the presence or absence of certain characteristics or events. For example, the number of individuals of some species in a population, the number of chromosomes in a cell, or the number of earthquakes occurring in a region within a given time frame. Other examples of discrete data in these sciences include the number of mutations in a gene, the number of cells in a tissue sample, or the number of species present in an ecosystem. These types of data are often analysed using statistical techniques such as frequency distributions, contingency tables, and chi-square tests.\n\n\n2.1.2 Continuous variables\nContinuous data, on the other hand, are measured on a continuous scale. These usually represent measured quantities such as something’s heat content (temperature, measured in degrees Celsius) or distance (measured in metres or similar). They can be rational numbers including integers and fractions, but typically they have an infinite number of ‘steps’ that depend on rounding (they can even be rounded to whole integers), considerations such as measurement precision and accuracy. Often, continuous data have upper and lower bounds that depend on the characteristics of the phenomenon being studied or the measurement being taken.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#dates",
    "href": "intro_r/04-data-in-R.html#dates",
    "title": "4. Data Classes and Structures",
    "section": "2.2 Dates",
    "text": "2.2 Dates\n\n\n\nDates as a data class.\n\n\nWe often encounter date data when dealing with time-related data. For example, in ecological research, data collection may involve recording the date of a particular observation, sampling event such as the date when a bird was sighted, or when water samples were taken from a stream. The purpose of using date (or time) data in biology, ecology is to enable us to understand and analyse temporal patterns and relationships in their response variables. This can include exploring seasonal trends and understanding the impact of environmental changes over time, or tracking the growth, development of organisms.\nBy analysing date data, we can gain insights into long-term trends, patterns that may not be apparent when looking at the data in aggregate. They can also use this information to make predictions about future trends and develop more effective management strategies, and identify potential areas for further research.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#character-data",
    "href": "intro_r/04-data-in-R.html#character-data",
    "title": "4. Data Classes and Structures",
    "section": "2.3 Character Data",
    "text": "2.3 Character Data\nCharacter data are used to describe qualitative variables or descriptive text that are not numerical in nature. Character data can be entered as descriptive character strings, and internally, they are translated into a vector of characters in R. They are often used to represent categorical variables, such as the type of plant species, the colour of a bird’s feathers, or the name of a some gene. Social scientists will sometimes use character data fields to record the names of people, places or other descriptive information, such as a narrative that will later be subjected to, for example, a sentiment analysis. For convenience, I will call these data narrative style data to distinguish them from the qualitative data that are the main focus of the present discussion.\nSince narrative style data are not directly amenable to statistical analsysis, in this module, we will mainly concern ourselves with qualitative data which are typically names of things, or categories of objects, classes of behaviours, properties, characteristics, and so on. Qualitative data typically refer to non-numeric data collected from observations, experimental treatment groups, or other sources. They tend to be textual, are often used to describe characteristics or properties of living organisms and ecosystems, or other biological phenomena. Examples may include the colour of flowers, the type of habitat where an animal is found, the behaviour of animals, or the presence, absence of certain traits or characteristics in a population.\nQualitative data can be further classified into nominal or ordinal data types. Ordinal and nominal data are both amenable to statistical interpretation.\n\n2.3.1 Nominal variables\nNominal data are used to describe qualitative variables that do not have any inherent order or ranking. Examples of nominal data in biology may include the type of plant or animal species, or the presence, absence of certain genetic traits. Another term for nominal data is categorical data. Because there are well-defined categories or the number of members belonging to each of the category can be counted. For example, there are three red flowers, 66 purple flowers, and 13 yellow flowers.\n\n\n2.3.2 Ordinal variables\nOrdinal data refer to a type of data that can be used to describe qualitative categorical variables that have a natural order or ranking. It is used when we need to arrange things in a particular order, such as from worst to best, from least to most. However, the differences between the values cannot be measured or quantified exactly, making them somewhat subjective. Examples of ordinal data include the different stages of development of an organism, the performance of a species to different fertilisers. Ordinal data can be entered as descriptive character strings and internally, they are translated into an ordered vector of integers in R. For example, we can use a scale of 1 for terrible, 2 for ‘so-so’, 3 for average, 4 for good, 5 for brilliant.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#binary-variables",
    "href": "intro_r/04-data-in-R.html#binary-variables",
    "title": "4. Data Classes and Structures",
    "section": "2.4 Binary Variables",
    "text": "2.4 Binary Variables\nLife can be boiled down to a series of binary decisions: should I have pizza for dinner, yes or no? Should I go to bed early or TRUE or FALSE? Should I start that new series on Netflix, accept or reject? Am I present or absent? You get the gist… This kind of binary decision-making is known as ‘logical’ and in R they can only take on the values of TRUE or FALSE (remember to mind your case!). In the computing world, logical data are often represented by 1 for TRUE and 0 for FALSE. So basically, your life’s choices can be summarised as a string of 1s and 0s. Who knew it was that simple?\nWhen it comes down to it, everything in life is either black or white, right or wrong, good or bad. It is like a cosmic game of “Would You Rather?” — and we are all just playing along.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#missing-values",
    "href": "intro_r/04-data-in-R.html#missing-values",
    "title": "4. Data Classes and Structures",
    "section": "2.5 Missing Values",
    "text": "2.5 Missing Values\nIt is unfortunate to admit that one of the most reliable aspects of any biological dataset is the presence of missing data (the presence of something that is missing?!). It is a stark reminder of the fragility of life. How can we say that something contains missing data? It seems counter intuitive, as if the data were never there in the first place. However, as we remember the principles of tidy data, we see that every observation must be documented in a row, and each column in that row must contain a value. This organisation allows us to create a matrix of data from multiple observations. Since the data are presented in a two-dimensional format, any missing values from an observation will leave a gaping hole in the matrix. We call these ‘missing values.’ It is a somber reality that even the most meticulous collection of data can be marred by the loss of information.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#complex-numbers",
    "href": "intro_r/04-data-in-R.html#complex-numbers",
    "title": "4. Data Classes and Structures",
    "section": "2.6 Complex Numbers",
    "text": "2.6 Complex Numbers\n\n“And if you gaze long enough into an abyss, the abyss will gaze back into you.”\n— Friedrich Nietzsche\n\nI mention complex numbers just to be complete; you will rarely encounter them in applied biological analysis, but knowing about their existence prevents confusion when they emerge indirectly in modelling or numerical methods.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#numeric-core",
    "href": "intro_r/04-data-in-R.html#numeric-core",
    "title": "4. Data Classes and Structures",
    "section": "3.1 numeric (core)",
    "text": "3.1 numeric (core)\nIn R, the numeric data class represents either integers or floating point (decimal) values. Numerical data are quantitative in nature as they represent things that can be objectively counted, measured, or calculated. More often than not, these represent the measured variables.\nNumeric datasets are therefore some of the most common types of data used in statistical and mathematical analysis. In R, numeric data are represented by the class numeric, which includes both integers and floating-point numbers. Numeric data can be used in a variety of operations and calculations, including arithmetic operations, statistical analyses, and visualisations. One important feature of the numeric data class in R is that it supports vectorisation, which allows for efficient, concise operations on large sets of numeric data. Additionally, R provides a wide range of built-in functions for working with numeric data, including functions for calculating basic statistical measures such as mean, median, and standard deviation.\nIn R integer (discrete) data are called int and &lt;int&gt; while continuous data are denoted num and &lt;dbl&gt;.\nExample of integer data Suppose you have a dataset of the number of rats in different storm water drains in a neighbourhood. The number of rats is a discrete variable because it can only take on integer values (you cannot own a fraction of a rat).\nHere is how you could create a vector of this data in R:\n\n# Create a vector of the number of pets owned by each household\nnum_rats &lt;- c(0, 1, 2, 2, 3, 1, 4, 0, 2, 1, 2, 2, 0, 3, 2, 1, 1, 4, 2, 0)\nnum_rats\n\nR&gt;  [1] 0 1 2 2 3 1 4 0 2 1 2 2 0 3 2 1 1 4 2 0\n\nclass(num_rats)\n\nR&gt; [1] \"numeric\"\n\n\nIn this example, the data are represented as a vector called num_rats of class numeric (as revealed by class(num_rats)). Each element of the vector represents the number of rats in one storm water drain. For example, the first element of the vector (num_rats[1]) is 0, which means that the first drain in the dataset is free of rats. The fourth element of the vector (num_rats[4]) is 2, indicating that the fourth drain in the dataset is occupied by 2 rats.\nOne can also explicitly create a vector of integer using the as.integer() function. Here is a simple example of coercion; in this case, R is not preserving meaning, only enforcing a representational rule (it represents the floating point numbers specifically as integers):\n\nnum_rats_int &lt;- as.integer(num_rats)\nnum_rats_int\n\nR&gt;  [1] 0 1 2 2 3 1 4 0 2 1 2 2 0 3 2 1 1 4 2 0\n\nclass(num_rats_int)\n\nR&gt; [1] \"integer\"\n\n\nAbove we coerced the class numeric data to class integer. But we can take floating point numeric and convert them to integers too with the as.integer() function. As we see, the effect is that the whole part of the number is retained, the rest discarded:\n\npies &lt;- pi * seq(1:5)\npies\n\nR&gt; [1]  3.141593  6.283185  9.424778 12.566371 15.707963\n\nclass(pies)\n\nR&gt; [1] \"numeric\"\n\nas.integer(pies)\n\nR&gt; [1]  3  6  9 12 15\n\n\nEffectively, what happened above is more-or-less equivalent to what the floor() function would return:\n\nfloor(pies)\n\nR&gt; [1]  3  6  9 12 15\n\n\nBe careful when coercing floating point numbers to integers. If rounding is what you expect, this is not what you will get. For rounding, use round() instead:\n\nround(pies, 0)\n\nR&gt; [1]  3  6  9 13 16\n\n\n\n\n\n\n\n\nNoteUse class() to Troubleshoot\n\n\n\nWhenever an operation yields an unexpected result, inspect the class before inspecting the values; coercion almost always precedes confusion.\n\n\nExample of continuous data Here are some randomly generated temperature data assigned to an object called temp_data:\n\n# Generate a vector of 50 normally distributed temperature values\ntemp_data &lt;- round(rnorm(n = 50, mean = 15, sd = 3), 2)\ntemp_data\n\nR&gt;  [1] 14.88 14.08 18.63 19.36 15.31 16.55 15.21 13.27 14.49  9.74 17.24  7.81\nR&gt; [13] 11.12  9.65 12.97 11.06 13.89 14.89 10.28 15.43 17.28 15.45 18.97 17.70\nR&gt; [25] 17.73 14.79 14.86 11.81  8.23 21.37 13.96 15.43 15.60 15.98 16.92 15.07\nR&gt; [37] 13.58 14.42 14.17 15.16 14.64 17.85 14.34 10.80 21.10 14.00 15.01 12.85\nR&gt; [49] 13.26 15.64\n\nclass(temp_data)\n\nR&gt; [1] \"numeric\"",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#character",
    "href": "intro_r/04-data-in-R.html#character",
    "title": "4. Data Classes and Structures",
    "section": "3.2 character",
    "text": "3.2 character\nIn R, the character data class represents textual data such as words, sentences, and paragraphs. Character data can be created using either single or double quotes, and it can include letters, numbers, and other special characters. In addition, character data can be concatenated using the paste() function or other string manipulation functions.\nOne important feature of the character data class in R is its versatility in working with textual data. For instance, it can be used to store and manipulate text data, including text-based datasets, text-based files, and text-based visualisations. Additionally, R provides a wide range of built-in functions for working with character data, including functions for manipulating strings, searching for patterns, and formatting output. Overall, the character data class in R is a fundamental data type that is critical for working with textual data in a variety of contexts. You will most frequently use character values to represent labels, names, or descriptions.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#factor-core",
    "href": "intro_r/04-data-in-R.html#factor-core",
    "title": "4. Data Classes and Structures",
    "section": "3.3 factor (core)",
    "text": "3.3 factor (core)\nIn R, the factor data class is used to represent discrete categorical variables. Factors are often used in statistical analyses to represent class or group belonging. Factor values are categorical data, such as levels and categories of a variable. Factor variables are most commonly also character data, but they can be numeric too if coded correctly as factors. Factor values can be ordered (ordinal) or unordered (categorical or nominal).\nCategorical variables take on a limited number of distinct values, often corresponding to different groups and levels. For example, a categorical variable might represent different colours, size classes, or species. Factors in R are represented as integers with corresponding character levels, where each level corresponds to a distinct category. The levels of a factor can be defined explicitly using the factor() function or automatically using the cut() function. One important feature of the factor data class in R is that it allows for efficient and effective data manipulation and analysis, particularly when working with large datasets. For instance, factors can be used in statistical analyses such as regression models and ANOVA, and they can also be used to create visualisations such as bar or pie graphs. The factor data class in R is a fundamental data type that is critical for representing and working with categorical variables in data analysis and visualisation.\nThe factor data class in an R data.frame structure (or in a tibble) is indicated by Factor (&lt;fctr&gt;). Ordered factors are denoted by columns named Ord.factor (&lt;ord&gt;).\nNominal data One example of nominal factor data that ecologists might encounter is the type of vegetation in a particular area, such as ‘grassland’, ‘forest’, or ‘wetland’. Here is an example of how to generate a vector of nominal data in R using the sample() function:\n\n# Generate a vector of vegetation types\nvegetation &lt;- sample(c(\"grassland\", \"forest\", \"wetland\"), size = 50, replace = TRUE)\n\n# View the vegetation data\nvegetation\n\nR&gt;  [1] \"wetland\"   \"grassland\" \"grassland\" \"wetland\"   \"grassland\" \"grassland\"\nR&gt;  [7] \"forest\"    \"grassland\" \"grassland\" \"grassland\" \"forest\"    \"forest\"   \nR&gt; [13] \"grassland\" \"forest\"    \"wetland\"   \"wetland\"   \"forest\"    \"wetland\"  \nR&gt; [19] \"forest\"    \"grassland\" \"wetland\"   \"wetland\"   \"forest\"    \"grassland\"\nR&gt; [25] \"wetland\"   \"grassland\" \"forest\"    \"wetland\"   \"grassland\" \"wetland\"  \nR&gt; [31] \"wetland\"   \"grassland\" \"forest\"    \"wetland\"   \"grassland\" \"wetland\"  \nR&gt; [37] \"forest\"    \"forest\"    \"forest\"    \"wetland\"   \"wetland\"   \"forest\"   \nR&gt; [43] \"forest\"    \"forest\"    \"grassland\" \"wetland\"   \"forest\"    \"wetland\"  \nR&gt; [49] \"grassland\" \"forest\"\n\nclass(vegetation)\n\nR&gt; [1] \"character\"\n\n\n\n\n\n\n\n\nNoteThe sample() Function\n\n\n\nNote that the sample() function is not made specifically for nominal data; it can be used on any kind of data class.\n\n\nOrdinal data Here is an example vector of ordinal data in R that could be encountered by ecologists:\n\n# Vector of ordinal data representing the successional stage of a forest\nsuccession &lt;- c(\"Early Pioneer\", \"Late Pioneer\",\n                \"Young Forest\", \"Mature Forest\",\n                \"Old Growth\")\nsuccession\n\nR&gt; [1] \"Early Pioneer\" \"Late Pioneer\"  \"Young Forest\"  \"Mature Forest\"\nR&gt; [5] \"Old Growth\"\n\nclass(succession)\n\nR&gt; [1] \"character\"\n\n# Convert to ordered factor\nsuccession &lt;- factor(succession, ordered = TRUE,\n                     levels = c(\"Early Pioneer\", \"Late Pioneer\",\n                                \"Young Forest\", \"Mature Forest\",\n                                \"Old Growth\"))\nsuccession\n\nR&gt; [1] Early Pioneer Late Pioneer  Young Forest  Mature Forest Old Growth   \nR&gt; 5 Levels: Early Pioneer &lt; Late Pioneer &lt; Young Forest &lt; ... &lt; Old Growth\n\nclass(succession)\n\nR&gt; [1] \"ordered\" \"factor\"\n\n\nThe ordering here reflects biological reasoning, but R will only respect that ordering if it is made explicit in the data structure.\nIn this example, the successional stage of a forest is represented by an ordinal scale with five levels ranging from ‘Early Pioneer’ to ‘Old Growth’. The factor() function is used to convert the vector to an ordered factor, with the ordered argument set to TRUE and the levels argument set to the same order as the original vector. This ensures that the levels are properly represented as an ordered factor.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#logical-core",
    "href": "intro_r/04-data-in-R.html#logical-core",
    "title": "4. Data Classes and Structures",
    "section": "3.4 logical (core)",
    "text": "3.4 logical (core)\nIn R, the logical data class represents binary or Boolean data. Logical data are used to represent variables that can take on only two possible values, TRUE and FALSE. In addition to TRUE and FALSE, logical data can also take on the values of NA and NULL, which represent missing or undefined values.\nBiologically this is presence or absence; in R it is a logical or numeric encoding, and the distinction matters because R responds to the encoding, not the intention.\nLogical data can be created using logical operators such as ==, !=, &gt;, &lt;, &gt;=, &lt;=. Logical data are commonly used in R for data filtering and selection, conditional statements, and logical operations. For example, logical data can be used to filter a dataset to include only observations that meet certain criteria, to perform logical operations such as AND (&) or (|). The logical data class in R is a fundamental data type that is critical for representing and working with binary or Boolean variables in data analysis and programming.\nExample logical (binary) data Here is an example of generating a vector of binary or logical data in R, which represents the presence, absence of a particular species in different ecological sites:\n\n# Generate a vector of 1s and 0s to represent the presence\n# or absence of a species in different ecological sites\nspecies_presence &lt;- sample(c(0,1), 10, replace = TRUE)\nspecies_presence\n\nR&gt;  [1] 0 0 0 0 0 0 1 1 0 0\n\n\nWe can also make a formal logical class data:\n\nspecies_presence_logi &lt;- as.logical(species_presence)\nclass(species_presence_logi)\n\nR&gt; [1] \"logical\"\n\n\nIn this example, we again use the sample() function to randomly generate a vector of 10 values, each either 0, 1 or to represent the presence or absence of a species in 10 different ecological sites. However, it is often not necessary to coerce to class logical, as we see in the presence-absence datasets we will encounter in BCB743: Quantitative Ecology.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#date",
    "href": "intro_r/04-data-in-R.html#date",
    "title": "4. Data Classes and Structures",
    "section": "3.5 date",
    "text": "3.5 date\nDate — time handling can become intricate, but most introductory analyses rely on a small number of basic conventions; the distinctions introduced here are intended to make those conventions intelligible… there is no need to master them at this point.\nIn R, the POSIXct, POSIXlt, Date classes are commonly used to represent date and time data. These classes each have unique characteristics that make them useful for different purposes.\nThe POSIXct class is a date/time class that represents dates and times as a numerical value, typically measured in seconds since January 1st, 1970. This class provides a high level of precision, with values accurate to the second. It is useful for performing calculations, data manipulation involving time such as finding the difference between two dates or adding a certain number of seconds to a given time. An example of how to generate a POSIXct object in R is as follows:\n\nmy_time &lt;- as.POSIXct(\"2022-03-10 12:34:56\")\nclass(my_time)\n\nR&gt; [1] \"POSIXct\" \"POSIXt\"\n\nmy_time\n\nR&gt; [1] \"2022-03-10 12:34:56 SAST\"\n\n\nThe POSIXlt class, on the other hand, typically represents dates, times in a more human-readable format. It stores date and time information as a list of named elements including year, month, day, hour, minute, second. This format is useful for displaying data in a more understandable way and for extracting specific components of a date or time. An example of how to generate a POSIXlt object in R is as follows:\n\nmy_time &lt;- as.POSIXlt(\"2022-03-10 12:34:56\")\nclass(my_time)\n\nR&gt; [1] \"POSIXlt\" \"POSIXt\"\n\nmy_time\n\nR&gt; [1] \"2022-03-10 12:34:56 SAST\"\n\n\nThe Date class is used to represent dates only, without any time information. Dates are typically stored as the number of days since January 1st, 1970. This class provides functions for performing arithmetic operations, comparisons between dates. It is useful for working with time-based data that is only concerned with the date component such as daily sales or stock prices. An example of how to generate a Date object in R is as follows:\n\nmy_date &lt;- as.Date(\"2022-03-10\")\nclass(my_date)\n\nR&gt; [1] \"Date\"\n\nmy_date\n\nR&gt; [1] \"2022-03-10\"\n\n\nTo generate a vector of dates in R with daily intervals, we can use the seq() function to create a sequence of dates, specifying the start, end dates and the time interval. Here is an example:\n\n# Generate a vector of dates from January 1, 2022 to December 31, 2022\ndates &lt;- seq(as.Date(\"2022-01-01\"), as.Date(\"2022-12-31\"), by = \"day\")\n\n# View the first 10 dates in the vector\nhead(dates, 10)\n\nR&gt;  [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\nR&gt;  [6] \"2022-01-06\" \"2022-01-07\" \"2022-01-08\" \"2022-01-09\" \"2022-01-10\"\n\nclass(dates)\n\nR&gt; [1] \"Date\"\n\n\nUnderstanding the characteristics of these date and time classes in R is essential for effective data analysis and manipulation in fields where time-based data is a critical component.\nDate and time data in R can be manipulated using various built-in functions and packages such as lubridate and chron. Additionally, date, time data can be visualised using different types of graphs such as time series plots and heatmaps, and Hovmöller diagrams. The date, time data classes in R are essential for working with temporal data and conducting time-related analyses in various biological and environmental datasets.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#missing-values-na",
    "href": "intro_r/04-data-in-R.html#missing-values-na",
    "title": "4. Data Classes and Structures",
    "section": "3.6 Missing Values, NA",
    "text": "3.6 Missing Values, NA\nMissing values can be encountered in vectors of all data classes. To demonstrate some data that contains missing values, I will generate a data sequence containing 5% missing values. We can use the rnorm() function to generate a sequence of random normal numbers and then randomly assign 5% of the values as missing using the sample() function. The indices of the missing values are stored in missing_indices, and we use them to assign NA to the corresponding elements of the data sequence. Here is some code to achieve this:\n\n# Set the length of the sequence\nn &lt;- 100\n\n# Generate a sequence of random normal numbers with\n# mean 0 and standard deviation 1\ndata &lt;- rnorm(n, mean = 0, sd = 1)\n\n# Randomly assign 5% of the values as missing\nmissing_indices &lt;- sample(1:n, size = round(0.05*n))\ndata[missing_indices] &lt;- NA\nlength(data)\n\nR&gt; [1] 100\n\ndata\n\nR&gt;   [1]  1.152237774 -0.667328568           NA -1.850952067 -0.721923806\nR&gt;   [6] -0.981985032 -1.135630208  0.030440904  0.622795783  0.476833620\nR&gt;  [11] -0.865448996 -0.472195059  1.384917512  0.702545293 -1.049620899\nR&gt;  [16]  1.893503557 -1.298133747  0.111635744 -1.267554570 -0.095934748\nR&gt;  [21] -0.365817663 -0.181088769  0.132233338  0.165198338  2.172448608\nR&gt;  [26] -1.798911519 -0.167937633  0.763907049  0.070589449 -0.395254280\nR&gt;  [31]  0.218581202 -0.465530510 -1.789608333 -1.634126760  0.113546497\nR&gt;  [36] -0.599806704 -0.262279671 -0.870901155 -0.220440960  1.478744991\nR&gt;  [41]  0.712444878 -0.804968205  0.044875590  1.529184214  1.508587866\nR&gt;  [46]  0.186427188  1.754091107 -0.795160030 -1.520418233  0.221287935\nR&gt;  [51] -0.632398892 -0.829183077  1.999855800 -1.638169161 -2.492915317\nR&gt;  [56] -0.510795396 -0.869918607 -0.285946264  1.561695444  1.474560596\nR&gt;  [61]  1.290586036  1.001304423 -0.446670836           NA -0.387541658\nR&gt;  [66]  0.446772758 -0.376466273 -0.187040880  0.318782726  0.464729649\nR&gt;  [71]  2.994934901  0.661983179  0.382983320 -0.284840756 -0.252235849\nR&gt;  [76]  0.906344362  1.218122584  0.749751862           NA -0.117889207\nR&gt;  [81] -1.070015597 -2.289978032 -0.037274156 -1.502065851  0.079408115\nR&gt;  [86]           NA  0.322848286 -0.589367744 -0.941435502  0.939456904\nR&gt;  [91]  1.420685785 -1.057554488  0.065426839           NA  2.916757050\nR&gt;  [96]  0.280590612  0.006933513 -2.497883924  0.712900894  0.154831091\n\n\nTo remove all NAs from the vector of data we can use na.omit():\n\ndata_sans_na &lt;- na.omit(data)\nlength(data_sans_na)\n\nR&gt; [1] 95\n\ndata_sans_na\n\nR&gt;  [1]  1.152237774 -0.667328568 -1.850952067 -0.721923806 -0.981985032\nR&gt;  [6] -1.135630208  0.030440904  0.622795783  0.476833620 -0.865448996\nR&gt; [11] -0.472195059  1.384917512  0.702545293 -1.049620899  1.893503557\nR&gt; [16] -1.298133747  0.111635744 -1.267554570 -0.095934748 -0.365817663\nR&gt; [21] -0.181088769  0.132233338  0.165198338  2.172448608 -1.798911519\nR&gt; [26] -0.167937633  0.763907049  0.070589449 -0.395254280  0.218581202\nR&gt; [31] -0.465530510 -1.789608333 -1.634126760  0.113546497 -0.599806704\nR&gt; [36] -0.262279671 -0.870901155 -0.220440960  1.478744991  0.712444878\nR&gt; [41] -0.804968205  0.044875590  1.529184214  1.508587866  0.186427188\nR&gt; [46]  1.754091107 -0.795160030 -1.520418233  0.221287935 -0.632398892\nR&gt; [51] -0.829183077  1.999855800 -1.638169161 -2.492915317 -0.510795396\nR&gt; [56] -0.869918607 -0.285946264  1.561695444  1.474560596  1.290586036\nR&gt; [61]  1.001304423 -0.446670836 -0.387541658  0.446772758 -0.376466273\nR&gt; [66] -0.187040880  0.318782726  0.464729649  2.994934901  0.661983179\nR&gt; [71]  0.382983320 -0.284840756 -0.252235849  0.906344362  1.218122584\nR&gt; [76]  0.749751862 -0.117889207 -1.070015597 -2.289978032 -0.037274156\nR&gt; [81] -1.502065851  0.079408115  0.322848286 -0.589367744 -0.941435502\nR&gt; [86]  0.939456904  1.420685785 -1.057554488  0.065426839  2.916757050\nR&gt; [91]  0.280590612  0.006933513 -2.497883924  0.712900894  0.154831091\nR&gt; attr(,\"na.action\")\nR&gt; [1]  3 64 79 86 94\nR&gt; attr(,\"class\")\nR&gt; [1] \"omit\"\n\n\n\n\n\n\n\n\nNoteDealing with NAS in Functions\n\n\n\nMany functions have specific arguments to deal with NAs in data. See for example the na.rm = TRUE argument given to mean(), median(), min(), lm(), etc.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#vector-array-matrix",
    "href": "intro_r/04-data-in-R.html#vector-array-matrix",
    "title": "4. Data Classes and Structures",
    "section": "4.1 vector, array, matrix",
    "text": "4.1 vector, array, matrix\nVectors In R, a vector is a one-dimensional array-like data structure that can hold a sequence of values of the same atomic mode, such as numeric, character, logical values, or Date values. A vector can be created using the c() function, which stands for ‘combine’ or ‘concatenate’ and is used to combine a sequence of values into a vector. Vectors can also be created by using the seq() function to generate a sequence of numbers, or the rep() function to repeat a value or sequence of values. Here is an example of a numeric vector:\n\n# create a numeric vector\nmy_vector &lt;- c(1, 2, 3, 4, 5)\n\n# coerce to vector\nmy_vector &lt;- as.vector(c(1, 2, 3, 4, 5))\nclass(my_vector) # but it does not change the class from numeric\n\nR&gt; [1] \"numeric\"\n\n# print the vector\nmy_vector\n\nR&gt; [1] 1 2 3 4 5\n\n\n\n\n\n\n\n\nNoteCoercion to Vector\n\n\n\nThe behaviour is such that the output of coercion to vector is that one the atomic modes (the basic data types) is returned.\n\n\nOne of the advantages of using vectors in R is that many of the built-in functions and operations work on vectors, allowing us to easily manipulate, analyse large amounts of data. Additionally, R provides many functions specifically designed for working with vectors, such as mean(), median(), sum(), min(), max(), and many others.\nMatrices A matrix (again, this terminology may be different for other languages), on the other hand, is a special case of an array that has two dimensions (rows, columns). It is also a multi-dimensional data structure that can hold elements of the same data type but it is specifically designed for handling data in a tabular format. A matrix can be created using the matrix() function in R.\n\n# create a numeric matrix\nmy_matrix &lt;- matrix(1:6, nrow = 2, ncol = 3)\n\n# print the matrix\nmy_matrix\n\nR&gt;      [,1] [,2] [,3]\nR&gt; [1,]    1    3    5\nR&gt; [2,]    2    4    6\n\nclass(my_matrix)\n\nR&gt; [1] \"matrix\" \"array\"\n\n\nWe can query the size or dimensions of the matrix as follows:\n\ndim(my_matrix)\n\nR&gt; [1] 2 3\n\nncol(my_matrix)\n\nR&gt; [1] 3\n\nnrow(my_matrix)\n\nR&gt; [1] 2\n\n\nCoercion of matrices to vectors A matrix can be coerced to a vector:\n\nas.vector(my_matrix)\n\nR&gt; [1] 1 2 3 4 5 6\n\n\nArrays In R (as opposed to in python or some other languages), an array specifically refers to a multi-dimensional data structure that can hold elements of the same data type. It can have any number of dimensions (1, 2, 3, etc.), and its dimensions can be named.\nMulti-dimensional arrays are common in modelling and spatial data, but you are unlikely to manipulate them directly in this course.\nAn array can be created using the array() function in R.\n\n# create a 2-dimensional array\nmy_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n# print the array\nmy_array\n\nR&gt; , , 1\nR&gt; \nR&gt;      [,1] [,2] [,3]\nR&gt; [1,]    1    4    7\nR&gt; [2,]    2    5    8\nR&gt; [3,]    3    6    9\nR&gt; \nR&gt; , , 2\nR&gt; \nR&gt;      [,1] [,2] [,3]\nR&gt; [1,]   10   13   16\nR&gt; [2,]   11   14   17\nR&gt; [3,]   12   15   18\nR&gt; \nR&gt; , , 3\nR&gt; \nR&gt;      [,1] [,2] [,3]\nR&gt; [1,]   19   22   25\nR&gt; [2,]   20   23   26\nR&gt; [3,]   21   24   27\n\nclass(my_array)\n\nR&gt; [1] \"array\"\n\n\nWe can figure something out about the size or dimensions of the array:\n\ndim(my_array)\n\nR&gt; [1] 3 3 3\n\nncol(my_array)\n\nR&gt; [1] 3\n\nnrow(my_array)\n\nR&gt; [1] 3\n\n\nCoercion of arrays to vectors The array can be coerced to a vector:\n\nas.vector(my_array)\n\nR&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\nR&gt; [26] 26 27\n\n\nThe key difference between vectors, arrays, and a matrices in R is their dimensions. A vector has one dimension, an array can have any number of dimensions, while a matrix is limited to two dimensions. Additionally, a matrix is often used to store data in a tabular format, while an array is used to store multi-dimensional data in general. A commonly encountered kind of matrix is seen in multivariate statistics is a distance, dissimilarity matrix.\nIn R, vectors, arrays, and matrices share a common characteristic: they do not have row, column names. Therefore or to refer to any element, row, or column, one must use their corresponding index. How?\nAccessing elements, rows, columns, and matrices In R, the square bracket notation is used to access elements, rows, columns, or matrices in arrays. The notation takes the form of [i, j, k, ...], where i, j, k, and so on, represent the indices of the rows, columns, or matrices to be accessed.\nSuppose we have the following array:\n\n\nmy_array &lt;- array(data = round(rnorm(n = 60, mean = 13, sd = 2), 1),\n                  dim = c(5, 4, 3))\nmy_array\n\nR&gt; , , 1\nR&gt; \nR&gt;      [,1] [,2] [,3] [,4]\nR&gt; [1,] 10.5 13.4 13.3 13.1\nR&gt; [2,] 13.3 12.9 13.3 12.9\nR&gt; [3,] 11.5 14.5 13.0 15.8\nR&gt; [4,] 12.2 14.7 14.3 15.4\nR&gt; [5,] 12.3 14.4  9.2 15.7\nR&gt; \nR&gt; , , 2\nR&gt; \nR&gt;      [,1] [,2] [,3] [,4]\nR&gt; [1,] 10.1 13.3 14.3 14.2\nR&gt; [2,] 11.5 14.5 15.3 11.4\nR&gt; [3,] 12.3 13.6 13.2 14.4\nR&gt; [4,] 12.5 14.0 12.3 12.6\nR&gt; [5,] 11.1 11.8 10.9 12.6\nR&gt; \nR&gt; , , 3\nR&gt; \nR&gt;      [,1] [,2] [,3] [,4]\nR&gt; [1,] 11.6 10.9  7.8 13.5\nR&gt; [2,] 15.2 14.6 13.7 13.8\nR&gt; [3,] 11.4  9.7 14.5 10.2\nR&gt; [4,] 12.5 10.5 14.9 14.3\nR&gt; [5,] 10.4 14.2 11.8 12.4\n\ndim(my_array)\n\nR&gt; [1] 5 4 3\n\n\nThis creates a \\(5\\times4\\times3\\) array with values from 1 to 60.\nWhen working with multidimensional arrays, it is possible to omit some of the indices in the square bracket notation. This results in a subset of the array, which can be thought of as a lower-dimensional array obtained by fixing the omitted dimensions. For example, consider a 3-dimensional array my_array above with dimensions dim(my_array) = c(5,4,3). If we use the notation my_array[1,,], we would obtain a 2-dimensional array with dimensions dim(my_array[1,,]) = c(4,3) obtained by fixing the first index at 1:\n\ndim(my_array[1,,])\n\nR&gt; [1] 4 3\n\nmy_array[1,,]\n\nR&gt;      [,1] [,2] [,3]\nR&gt; [1,] 10.5 10.1 11.6\nR&gt; [2,] 13.4 13.3 10.9\nR&gt; [3,] 13.3 14.3  7.8\nR&gt; [4,] 13.1 14.2 13.5\n\n\nHere are some more examples of how to use square brackets notation with arrays in R:\nTo access a single element in the array, use the notation [i, j, k], where i, j, k are the indices along each of the three dimensions, which in combination, uniquely identifies each element. Below we return the element in the second row, third column, and first matrix:\n\nmy_array[2, 3, 1]  \n\nR&gt; [1] 13.3\n\n\nTo access a single row in the array, use the notation [i, , ], where i is the index of the row. This will return the second rows and all of the columns of the first matrix:\n\nmy_array[2,,1]\n\nR&gt; [1] 13.3 12.9 13.3 12.9\n\n\nTo access a single column in the array, use the notation [ , j, ], where j is the index of the column. Here we will return all the elements in the row of column two and matrix three:\n\nmy_array[ , 2, 3]\n\nR&gt; [1] 10.9 14.6  9.7 10.5 14.2\n\n\nTo access a single matrix in the array, use the notation [ , , k], where k is the index of the matrix:\n\nmy_array[ , , 2]\n\nR&gt;      [,1] [,2] [,3] [,4]\nR&gt; [1,] 10.1 13.3 14.3 14.2\nR&gt; [2,] 11.5 14.5 15.3 11.4\nR&gt; [3,] 12.3 13.6 13.2 14.4\nR&gt; [4,] 12.5 14.0 12.3 12.6\nR&gt; [5,] 11.1 11.8 10.9 12.6\n\n\nTo obtain a subset of the array, use the notation [i, j, k] with i, j, k omitted to obtain a lower-dimensional array:\n\nmy_array[1, , ]\n\nR&gt;      [,1] [,2] [,3]\nR&gt; [1,] 10.5 10.1 11.6\nR&gt; [2,] 13.4 13.3 10.9\nR&gt; [3,] 13.3 14.3  7.8\nR&gt; [4,] 13.1 14.2 13.5\n\nmy_array[ , 2:3, ]\n\nR&gt; , , 1\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,] 13.4 13.3\nR&gt; [2,] 12.9 13.3\nR&gt; [3,] 14.5 13.0\nR&gt; [4,] 14.7 14.3\nR&gt; [5,] 14.4  9.2\nR&gt; \nR&gt; , , 2\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,] 13.3 14.3\nR&gt; [2,] 14.5 15.3\nR&gt; [3,] 13.6 13.2\nR&gt; [4,] 14.0 12.3\nR&gt; [5,] 11.8 10.9\nR&gt; \nR&gt; , , 3\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,] 10.9  7.8\nR&gt; [2,] 14.6 13.7\nR&gt; [3,]  9.7 14.5\nR&gt; [4,] 10.5 14.9\nR&gt; [5,] 14.2 11.8",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#data.frame",
    "href": "intro_r/04-data-in-R.html#data.frame",
    "title": "4. Data Classes and Structures",
    "section": "4.2 data.frame",
    "text": "4.2 data.frame\nA dataframe is perhaps the most commonly-used ‘container’ for data in R because they are so convenient and serve many purposes. A dataframe is not a data class — more correctly, it is a form of tabular data (like a table in MS Excel), with each vector (a variable, column) comprising the table sharing the same length. What makes a dataframe versatile is that its variables can be any combination of the atomic data types. It may even include list columns (we will not cover list columns in this module). Applying theclass() function to a dataframe shows that it belongs to class data.frame.\nHere is an example of an R data.frame with Date, numeric, categorical data classes:\n\n# Create a vector of dates\ndates &lt;- as.Date(c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\",\n                   \"2022-01-04\", \"2022-01-05\"))\n\n# Create a vector of numeric data\nnumeric_data &lt;- rnorm(n = 5, mean = 0, sd = 1)\n\n# Create a vector of categorical data\ncategorical_data &lt;- c(\"A\", \"B\", \"C\", \"A\", \"B\")\n\n# Combine the vectors into a data.frame\nmy_dataframe &lt;- data.frame(dates = dates,\n                           numeric_data = numeric_data,\n                           categorical_data = categorical_data)\n\n# Print the dataframe\nmy_dataframe\n\nR&gt;        dates numeric_data categorical_data\nR&gt; 1 2022-01-01   2.21936788                A\nR&gt; 2 2022-01-02  -0.07342001                B\nR&gt; 3 2022-01-03  -0.08531893                C\nR&gt; 4 2022-01-04  -0.30958730                A\nR&gt; 5 2022-01-05  -0.56099394                B\n\nclass(my_dataframe)\n\nR&gt; [1] \"data.frame\"\n\nstr(my_dataframe)\n\nR&gt; 'data.frame':    5 obs. of  3 variables:\nR&gt;  $ dates           : Date, format: \"2022-01-01\" \"2022-01-02\" ...\nR&gt;  $ numeric_data    : num  2.2194 -0.0734 -0.0853 -0.3096 -0.561\nR&gt;  $ categorical_data: chr  \"A\" \"B\" \"C\" \"A\" ...\n\nsummary(my_dataframe)\n\nR&gt;      dates             numeric_data      categorical_data  \nR&gt;  Min.   :2022-01-01   Min.   :-0.56099   Length:5          \nR&gt;  1st Qu.:2022-01-02   1st Qu.:-0.30959   Class :character  \nR&gt;  Median :2022-01-03   Median :-0.08532   Mode  :character  \nR&gt;  Mean   :2022-01-03   Mean   : 0.23801                     \nR&gt;  3rd Qu.:2022-01-04   3rd Qu.:-0.07342                     \nR&gt;  Max.   :2022-01-05   Max.   : 2.21937\n\n\nDataframes may also have row names:\n\nrownames(my_dataframe) &lt;- paste(rep(\"row\", 5), seq = 1:5)\nmy_dataframe\n\nR&gt;            dates numeric_data categorical_data\nR&gt; row 1 2022-01-01   2.21936788                A\nR&gt; row 2 2022-01-02  -0.07342001                B\nR&gt; row 3 2022-01-03  -0.08531893                C\nR&gt; row 4 2022-01-04  -0.30958730                A\nR&gt; row 5 2022-01-05  -0.56099394                B\n\n\nTypically we will create a dataframe by reading in data from a .csv file, but it is useful to be able to construct one from scratch.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#tibble",
    "href": "intro_r/04-data-in-R.html#tibble",
    "title": "4. Data Classes and Structures",
    "section": "4.3 tibble",
    "text": "4.3 tibble\nIn R, a dataframe, a tibble are both data structures used to store tabular data. Although tibbles are also dataframes but they differ subtly in several ways.\n\nA tibble is a relatively new addition to the R language and forms part of the tidyverse suite of packages. They are designed to be more user-friendly than traditional data frames and have several additional features, such as more informative error messages, stricter data input, output rules and better handling of NA.\nUnlike a dataframe, a tibble never automatically converts strings to factors, changes column names or which can help avoid unexpected behaviour when working with the data.\nA tibble does not have row names.\nA tibble has a slightly different and more compact printing method than a dataframe, which makes them easier to read, work with.\nFinally, a tibble has better performance than dataframes for many tasks, especially when working with large datasets.\n\nWhile a dataframe is a core data structure in R, a tibble provides additional functionality, are becoming increasingly popular among R users and particularly those working with tidyverse packages. Applying the class() function to a tibble revelas that it belongs to the classes tbl_df, tbl and data.frame.\nWe can convert our dataframe my_dataframe to a tibble, and present the output with the print() function that applies nicely to tibbles:\n\nlibrary(tidyverse) # we need to load the tidyverse package\nmy_tibble &lt;- as_tibble(my_dataframe)\nclass(my_tibble)\n\nR&gt; [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(my_tibble)\n\nR&gt; # A tibble: 5 × 3\nR&gt;   dates      numeric_data categorical_data\nR&gt;   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;           \nR&gt; 1 2022-01-01       2.22   A               \nR&gt; 2 2022-01-02      -0.0734 B               \nR&gt; 3 2022-01-03      -0.0853 C               \nR&gt; 4 2022-01-04      -0.310  A               \nR&gt; 5 2022-01-05      -0.561  B\n\n\nThis very simple tibble looks identical to a dataframe, but as we start using more complex sets of data you will learn to appreciate the small convenience that tibbles offer.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/04-data-in-R.html#list",
    "href": "intro_r/04-data-in-R.html#list",
    "title": "4. Data Classes and Structures",
    "section": "4.4 list",
    "text": "4.4 list\nI introduce lists here because you will see them often in R outputs, even if you do not construct them routinely yourself.\nThis is also not actually a data class, but rather another way of representing a collection of objects of different types, all the way from numerical vectors to dataframes. Lists are useful for storing complex data structures, can also be accessed using indexing.\nAs an example, we create another dataframe:\n\ndates &lt;- as.Date(c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\",\n                   \"2022-01-04\", \"2022-01-05\"))\n\n# Create a vector of numeric data\nnumeric_data &lt;- rnorm(n = 5, mean = 1, sd = 1)\n\n# Create a vector of categorical data\ncategorical_data &lt;- c(\"C\", \"D\", \"D\", \"F\", \"A\")\n\n# Combine the vectors into a data.frame\nmy_other_dataframe &lt;- data.frame(dates = dates,\n                                  numeric_data = numeric_data,\n                                  categorical_data = categorical_data)\n\nmy_list &lt;- list(A = my_dataframe,\n                B = my_other_dataframe)\nmy_list\n\nR&gt; $A\nR&gt;            dates numeric_data categorical_data\nR&gt; row 1 2022-01-01   2.21936788                A\nR&gt; row 2 2022-01-02  -0.07342001                B\nR&gt; row 3 2022-01-03  -0.08531893                C\nR&gt; row 4 2022-01-04  -0.30958730                A\nR&gt; row 5 2022-01-05  -0.56099394                B\nR&gt; \nR&gt; $B\nR&gt;        dates numeric_data categorical_data\nR&gt; 1 2022-01-01    2.3851335                C\nR&gt; 2 2022-01-02    0.6777682                D\nR&gt; 3 2022-01-03    0.1246084                D\nR&gt; 4 2022-01-04   -1.1505860                F\nR&gt; 5 2022-01-05   -0.3195184                A\n\nclass(my_list)\n\nR&gt; [1] \"list\"\n\nstr(my_list)\n\nR&gt; List of 2\nR&gt;  $ A:'data.frame':   5 obs. of  3 variables:\nR&gt;   ..$ dates           : Date[1:5], format: \"2022-01-01\" \"2022-01-02\" ...\nR&gt;   ..$ numeric_data    : num [1:5] 2.2194 -0.0734 -0.0853 -0.3096 -0.561\nR&gt;   ..$ categorical_data: chr [1:5] \"A\" \"B\" \"C\" \"A\" ...\nR&gt;  $ B:'data.frame':   5 obs. of  3 variables:\nR&gt;   ..$ dates           : Date[1:5], format: \"2022-01-01\" \"2022-01-02\" ...\nR&gt;   ..$ numeric_data    : num [1:5] 2.385 0.678 0.125 -1.151 -0.32\nR&gt;   ..$ categorical_data: chr [1:5] \"C\" \"D\" \"D\" \"F\" ...\n\n\nWe can access one of the dataframes is the list as follows:\n\nmy_list[[2]]\n\nR&gt;        dates numeric_data categorical_data\nR&gt; 1 2022-01-01    2.3851335                C\nR&gt; 2 2022-01-02    0.6777682                D\nR&gt; 3 2022-01-03    0.1246084                D\nR&gt; 4 2022-01-04   -1.1505860                F\nR&gt; 5 2022-01-05   -0.3195184                A\n\nmy_list[[\"A\"]]\n\nR&gt;            dates numeric_data categorical_data\nR&gt; row 1 2022-01-01   2.21936788                A\nR&gt; row 2 2022-01-02  -0.07342001                B\nR&gt; row 3 2022-01-03  -0.08531893                C\nR&gt; row 4 2022-01-04  -0.30958730                A\nR&gt; row 5 2022-01-05  -0.56099394                B\n\n\nTo access a variable within one of the elements of the list we can do something like:\n\nmy_list[[\"B\"]]$numeric_data\n\nR&gt; [1]  2.3851335  0.6777682  0.1246084 -1.1505860 -0.3195184",
    "crumbs": [
      "Home",
      "Introduction to R",
      "4. Data Classes and Structures"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html",
    "href": "intro_r/02-working-with-data.html",
    "title": "2. Working with Data and Code",
    "section": "",
    "text": "In this Chapter we will cover:",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#the-working-directory",
    "href": "intro_r/02-working-with-data.html#the-working-directory",
    "title": "2. Working with Data and Code",
    "section": "2.1 The Working Directory",
    "text": "2.1 The Working Directory\nThe working directory is the directory that R treats as its current location. All relative paths are interpreted with respect to this directory.\nYou can think of the working directory as the answer to the question:\n\n“If I refer to a file without giving its full address, where will R look?”*\n\nIn R, the working directory can be inspected with:\n\ngetwd()\n\nand (technically) changed with:\n\nsetwd(\"some/path\")\n\nBeginners sometimes try to fix file-not-found errors by repeatedly calling setwd(). This usually makes things worse. Changing the working directory inside scripts creates hidden assumptions about where the code is run and breaks reproducibility.\nThe preferred solution is not to move the working directory around, but to control it once, deliberately.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#rstudio-projects",
    "href": "intro_r/02-working-with-data.html#rstudio-projects",
    "title": "2. Working with Data and Code",
    "section": "2.2 Rstudio Projects",
    "text": "2.2 Rstudio Projects\nAn RStudio Project defines a directory as the root of an analysis and sets it as the working directory whenever the project is opened. This has three consequences that are foundational for this course:\n\ngetwd() always returns the project root.\nAll scripts, data, and outputs can be referenced using relative paths.\nCode runs the same way on different computers.\n\nA typical project structure might look like this:\n\nBCB744_project/\n\nWith sub-folders specific to the different purposes:\n\ndata/\nscripts/\nfigures/\nreports/\n\nIf this directory is an RStudio Project, then a script inside scripts/ can read data using:\n\ndata/kelp.csv\n\nwithout calling setwd(), and without caring where the project lives on the machine.\nThis is why projects are treated as non-negotiable infrastructure in this course. They make file paths explicit, stable, and inspectable.\nSoftware such as R interprets all file paths relative to its working directory — the directory it currently considers as “here”. Confusion about file paths is therefore almost always confusion about the working directory. When files cannot be found, the problem is rarely the file itself; it is the path used to describe it.\nFor this reason, analyses should be organised as projects, with a clear internal structure. A minimal example might look like this:\n\nBCB744/\n\ndata/ … (raw or cleaned data)\nscripts/ … (R scripts)\nfigures/ … (output plots)\nreports/ … (Quarto or R Markdown documents)\n\n\nWhen code is run from the project root, all file access can use relative paths, avoiding hard-coded references to specific machines.\n\n\n\n\n\n\nImportantDo This Now\n\n\n\n\nOn your computer, create a directory called filesystem_practice. Inside it, create three sub-directories: data, scripts, and output. Place a small text file inside data.\nPlace a small text or CSV file inside data. From the R Console, run getwd() and confirm that it points to the project root.\nFrom a script inside scripts, write the relative path needed to read the file in data. Then write the absolute path to the same file. Which one would still work on another computer?\nDeliberately break a path by misspelling a directory name. Observe the error message produced by the software. Correct the path and rerun the command.\n(Deliberately bad practice) Use setwd() to point R directly at the data directory. Now try running the script again from a different location. Explain why this approach fails as a general strategy.\n\nThese exercises should help you to develop a form of spatial reasoning that underlies all your analyses 9and working with your computer in general — good housekeeping stems from this! If you cannot reliably locate files you also cannot reason clearly about data provenance, reproducibility, or control over your analysis.\n\n\nNote: graphical file browsers hide structure behind icons and clicks. Code exposes structure explicitly. Learning to think in paths rather than windows is a necessary transition from consumer software use to scientific computing.\nNow that we know how and where to locate your data, let’s take a look at how those data are represented.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#delimited-text-files-csv-and-tsv",
    "href": "intro_r/02-working-with-data.html#delimited-text-files-csv-and-tsv",
    "title": "2. Working with Data and Code",
    "section": "3.1 Delimited Text Files (CSV and TSV)",
    "text": "3.1 Delimited Text Files (CSV and TSV)\nCSV and TSV files optimise human readability.\nDelimited text files represent the simplest and most transparent way of storing tabular data. Both comma-separated value (CSV) files and tab-separated value (TSV) files encode tables as plain text, with rows separated by line breaks and columns separated by a designated delimiter. The distinction between them arises from how reliably the chosen delimiter can be distinguished from the data themselves.\nCSV files use commas to separate fields (columns), which makes them widely supported and easy to exchange across software and platforms. However, this choice becomes ambiguous when data values themselves contain commas, such as free text, lists, or numbers formatted with thousands separators. TSV files address this ambiguity by using tab characters as delimiters, which are far less likely to occur naturally within data values. In this sense, TSV files trade a small loss in visual familiarity for greater robustness during parsing.\nAs such, CSV files are a simple and widely used format amongst biologists and ecologists, but they can become impractical for large datasets with complex structures or metadata.\nWe will most frequently use the functions read.csv() and readr::read_csv() (and related forms) for reading in CSV data. We can write CSV files to disk with the write.csv() and readr::write_csv() commands. For very large datasets that might take a long time to read in or save, data.table::fread() and data.table::fwrite() are faster alternatives to the aforementioned base R or tidyverse options. Even faster options are feather::read_feather() and feather::write_feather(); although feather saves tabular data, the format is not actually an ASCII CSV, however.\nThe same functions that read or write CSV files in R can be used for TSV, but one has to set the arguments sep = \"\\t\" or delim = \"\\t\" for the functions read.csv() and read_csv() respectively.\n\n\n\n\n\n\nNoteAscii Files\n\n\n\nASCII stands for “American Standard Code for Information Interchange”. An ASCII file is a plain text file that contains ASCII characters. ASCII is a character encoding standard that assigns a unique numeric code to each character, including letters, numbers, punctuation, and other symbols commonly used in the English language.\nASCII files are the most basic type of text file and are supported by virtually all operating systems and applications. We can create and edit ASCII files using any text editor, such as Notepad, TextEdit, or VS Code. ASCII files are typically used for storing, sharing simple text-based information such as program source code, configuration files, and other types of data that do not require special formatting or rich media content.\nASCII files are limited in their ability to represent non-English characters or symbols that are not included in the ASCII character set. To handle these types of characters, other character encoding standards such as UTF-8 and Unicode are used. However, ASCII files remain an important and widely used format for storing and sharing simple text-based data.\n\n\n\n\n\n\n\n\nNoteMissing Values and Csv and Tsv Files\n\n\n\nWhere we have missing data (blanks), the CSV format separates these by commas with empty field in-between. However, there can be problems with blanks if we read in a space-delimited format file. If we are having trouble reading in missing data as blanks, try replacing them in the spreadsheet with NA, the missing data code in R. In Excel, highlight the area of the spreadsheet that includes all the cells we need to fill with NA. Do an ‘Edit/Replace…’ and leave the ‘Find what:’ text box blank and in the ‘Replace with:’ text box enter NA. Once imported into R, the NA values will be recognised as missing data.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#microsoft-excel-files",
    "href": "intro_r/02-working-with-data.html#microsoft-excel-files",
    "title": "2. Working with Data and Code",
    "section": "3.2 Microsoft Excel Files",
    "text": "3.2 Microsoft Excel Files\nExcel files optimise human convenience, engagement, and presentation.\nHowever, Excel files also emphasise that software convenience can obscure underlying data transformations, which moves the analytical risk from computation to problems that might stem from user interaction.\nMicrosoft Excel files are a type of file format that is used to store data in a tabular form, much like CSV files. However, Excel files are proprietary and are specifically designed to work with Excel software. Excel files can contain more advanced formatting features such as colours, fonts, and formulas, which make them a popular choice for people who like embellishments.\nThese software behaviours impose constraints that are poorly aligned with statistical analysis, and it is from these constraints that the following limitations arise:\n\nCompatibility Excel files may not be compatible with all data science tools and programming languages. For example, R cannot read Excel files directly.\nData integrity Excel files can be prone to errors and inconsistencies in the data. For example, if a user changes a formula or formatting, it could affect the entire dataset. Also, it is possible for Excel to change the data types of certain columns, or to mix the class of data within a column, which can cause issues with data processing and analysis.\nFile size Excel files can quickly become very large when dealing with large datasets, which can lead to performance issues, storage problems.\nVersion control Excel files can make it difficult to keep track of changes and versions of the data, particularly when multiple people are working on the same file.\n\nIn contrast, CSV files are a simple, lightweight, and widely supported file format that can be easily used with most data science tools and programming languages. CSV files are also less prone to errors and inconsistencies than Excel files, making them a more reliable choice for data science tasks.\nFor these reasons, Excel is best used as a tool for data entry rather than data analysis. Exporting deliberately to plain-text formats such as CSV fixes the data representation, reveals assumptions to inspection, and sets a clear boundary between data generation and analysis. Naming exported files explicitly (often with dates or version identifiers) further tightens provenance and reduces ambiguity about which data underlie a given result.\n\n\n\n\n\n\nNoteWell-known Excel Errors\n\n\n\nExcel is a widely used spreadsheet application, but it has been responsible for several serious errors in data analysis, science, and data science. Some of these errors include:\n\nGene name errors (2016): Accurate. Ziemann, Eren, and El-Osta published their findings in Genome Biology, demonstrating that Excel’s automatic conversion transformed gene symbols like SEPT2 and MARCH1 into dates. Their survey examined supplementary files from 3,597 published papers across 18 journals between 2005 and 2015, finding that roughly one-fifth contained such errors. This remains one of the most damaging documented cases of Excel undermining scientific reproducibility, and it directly affects biology students working with genomic data.\nAnother compelling case involves automatic data type conversion destroying scientific measurements. Excel converts identifiers that look like scientific notation (e.g., “1E4” becomes 10,000) or treats leading zeros (e.g., “0001”) in sample IDs as insignificant (i.e., Excel displays “1”). Ecologists working with plot identifiers or anyone using structured identifiers encounters this pervasive, insidious problem.\nTruncation of large numbers Excel can handle only a limited number of digits for large numbers, truncating any value that exceeds this limit. This truncation has led to a loss of precision and inaccurate calculations in scientific and data analysis contexts where exact values were important.\nIssues with floating-point arithmetic Excel uses IEEE 754 double-precision floating-point representation, which produces rounding errors. The classic demonstration involves calculations like (0.1 + 0.2) ≠ 0.3 in binary representation. For iterative scientific calculations or cumulative errors in modelling, this matters.\nThe UK COVID-19 testing data loss (October 2020) deserves attention. Public Health England used the legacy .xls format (limited to 65,536 rows) rather than the newer .xlsx (limited to 1,048,576 rows), resulting in 15,841 positive test results being lost when the file exceeded row limits. This directly affected contact tracing during a pandemic.\nThe date format ambiguity problem creates reproducibility nightmares across international collaborations. Excel interprets “01/02/2020” differently depending on regional settings (January 2nd vs. February 1st), and automatically converting text strings to dates corrupts datasets when shared between researchers using different locale settings. This affects any collaborative science.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#rdata-files",
    "href": "intro_r/02-working-with-data.html#rdata-files",
    "title": "2. Working with Data and Code",
    "section": "3.3 Rdata Files",
    "text": "3.3 Rdata Files\nSpecialist files such as Rdata, binary files (Section 3.4), and NetCDF (Section 3.5) files were developed for optimasing scale (data size), metadata (data about data), and coupling (issues around speed and performance).\nRdata files are a file format used by the R programming language to store data objects. These files can contain any type of R object, such as vectors, matrices, dataframes, lists, and more. Rdata files are binary files, which means they are not human-readable like text files such as CSV files. Binary R data files have a .rda or .Rdata file extension and can be created or read using the save() and load(), respectively, functions in R.\nRdata files are convenient for a number of reasons:\n\nEfficient storage Rdata files can be more compact (they can be compressed) and efficient than other file formats, such as CSV files, because they are stored in a binary format. This means they take up less disk space and can be read and written faster.\nEasy access to R objects Rdata files make it easy to save and load R objects, which can be useful for preserving data objects for future analysis, sharing them with others. This is especially useful for complex datasets or objects that would be difficult to recreate.\nPreserve metadata Rdata files can preserve metadata such as variable names, row names, column names, and other attributes of R objects. This makes it easier to work with the data objects in the future without having to recreate this metadata.\nConvenient for reproducibility Rdata files can be used to save and load data objects as part of a reproducible research workflow. This can help ensure that data objects are preserved and can be easily accessed in the future, even if the data sources or code have changed.\n\nOn the downside, they can only be used within R, making them a less than ideal proposition when you intend to share your data with colleagues who sadly do not use R.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#sec-binary",
    "href": "intro_r/02-working-with-data.html#sec-binary",
    "title": "2. Working with Data and Code",
    "section": "3.4 Other Binary Files",
    "text": "3.4 Other Binary Files\nAs a biostatistician, you may encounter several other binary data files in your work. Such binary data files may be software-specific and can be used to store large datasets or data objects that are not easily represented in a text format. For example, a binary data file might contain a large matrix or array of numeric data that would be difficult to store in a text file. Binary data files can also be used to store images, audio files, and other types of data that are not represented as text.\nOne common type of binary data file that you may encounter as a statistician is a SAS data file. SAS is a statistical software package that is widely used in data analysis, and SAS data files are a binary format used to store datasets in SAS. These files typically have a .sas7bdat file extension and contain metadata such as variable names and formats in addition to the data itself. Another type of binary data file you may encounter is a binary .mat data file, which is a file format used to store Matlab data.\nWhen working with binary data files, it is important to be aware of the specific format of the file, the tools and software needed to read and manipulate the data. Some statistical software packages may have built-in functions for reading and writing certain types of binary data files, while others may require additional libraries or packages.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#sec-netcdf",
    "href": "intro_r/02-working-with-data.html#sec-netcdf",
    "title": "2. Working with Data and Code",
    "section": "3.5 Netcdf, Grib, and Hdf Files",
    "text": "3.5 Netcdf, Grib, and Hdf Files\nNetCDF, HDF, and GRIB are file formats commonly used in the scientific and research communities to store and share large and complex datasets. These datasets are optimised for storing gridded (regular intervals in space and time) data that are interoperable amongst computing systems. Here is a brief overview of each file format:\n\nNetCDF (Network Common Data Form) is a binary file format that is designed for storing and sharing scientific data. It can store multidimensional arrays and metadata, such as variable names and units, in a self-describing format. NetCDF files are commonly used in fields such as atmospheric science, oceanography, and climate modelling.\nLike NetCDF files, HDF (Hierarchical Data Format) is a file format that is designed to store and organise large and complex data structures. It can store a wide variety of data types, including multidimensional arrays, tables, and hierarchical data. HDF files are commonly used in fields such as remote sensing, astronomy, and engineering.\nAgain, GRIB (GRIdded Binary) files are similar to NetCDF files. They are a binary file format used to store meteorological and oceanographic data. It can store gridded data, such as atmospheric and oceanic model output, in a compact and efficient binary format. GRIB files are commonly used by weather forecasting agencies and research organisations.\n\nCompared to CSV files, these file formats offer several benefits for storing, sharing complex datasets:\n\nSupport for multidimensional arrays These file formats can store and handle multidimensional arrays, which cannot be represented in a CSV file. However, they can be exported as CSV files, often after subsetting the data, but the resultant CSV files consume significant amounts of disk space.\nEfficient storage Binary file formats can be more compact and efficient than text-based formats such as CSV files, which can save disk space and make it easier to share and transfer large datasets.\nMemory use efficiency NetCDF, GRIB, and HDF files are better for memory use efficiency compared to CSV files because they can store multidimensional arrays and metadata in a compact binary format, which can save disk space and memory when working with large and complex datasets. Also, they do not have to be read into memory all at once.\nSelf-describing metadata These file formats can include metadata, such as variable names and units, which are self-describing and can be easily accessed and understood by other researchers and software.\nSupport for compression Binary file formats can support compression, which can further reduce file size and make it easier to share and transfer large datasets.\n\nThe various efficiencies mentioned above may be offset by them being quite challenging to work with, and as such novices might experience steep learning curves.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#larger-than-memory-data",
    "href": "intro_r/02-working-with-data.html#larger-than-memory-data",
    "title": "2. Working with Data and Code",
    "section": "3.6 Larger Than Memory Data",
    "text": "3.6 Larger Than Memory Data\nAbove we dealt with data that fit into your computer’s memory (RAM). However, there are many datasets that are too large to fit into memory, and as such, we need to use alternative methods to work with them. These methods include:\n\nApache Arrow in the arrow package in R, which has support for the ‘feather’ file format, ‘parquet’ files\nDuckDB in the duckdb package in R, which create a database on disk, can be queried using SQL\n\nI will develop vignettes for these in the future. We will not use these in this course, but it is important to be aware of them.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#required-practice-in-bcb744",
    "href": "intro_r/02-working-with-data.html#required-practice-in-bcb744",
    "title": "2. Working with Data and Code",
    "section": "7.1 Required Practice in Bcb744",
    "text": "7.1 Required Practice in Bcb744\nAt this stage, reproducibility requires a small number of essential habits. These are the practices you are expected to adopt consistently and that will be evaluated in all assessments.\nAnalyses should be conducted using scripted code rather than manual interaction. Scripts provide an explicit record of analytical decisions and allow results to be regenerated without reliance on memory or interface state. Code should be organised within a coherent project structure, with separate locations for raw data, processed data, scripts, and outputs, so that analytical flow can be reconstructed with ease.\nVersion control forms part of this basic practice. Using Git (typically via platforms such as GitHub or GitLab) provides a transparent history of how analyses evolve and allows earlier states to be recovered if errors are introduced. This is not about public dissemination, but about maintaining an explicit record of change.\nLiterate programming tools such as R Markdown and Quarto are also part of required practice. These tools integrate code, output, and narrative text within a single document, making analytical reasoning visible alongside results. When data or code change, documents are regenerated rather than edited by hand, reducing the risk of divergence between analysis and interpretation.\nFinally, reproducibility depends on making dependencies explicit. Required packages should be declared deliberately, relative file paths should be used to avoid machine-specific assumptions, and data files should be treated as fixed inputs rather than mutable artefacts. Together, these practices establish a workflow that can be re-run by others — or by your future self — without guesswork.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "intro_r/02-working-with-data.html#a-look-to-the-future",
    "href": "intro_r/02-working-with-data.html#a-look-to-the-future",
    "title": "2. Working with Data and Code",
    "section": "7.2 A Look to the Future",
    "text": "7.2 A Look to the Future\nBeyond these core practices lies a wider R ecosystem of tools made to support reproducibility at larger scales or higher levels of complexity. You are not expected to use these tools in BCB744, but you should be aware of the problems they address and the contexts in which they become relevant.\nWorkflow management tools such as workflowr and targets formalise analytical pipelines by making dependencies between steps explicit and automating re-execution when inputs change. These tools are useful for large or long-running projects, but they introduce additional complexities that is unnecessary at this stage.\nContainerisation technologies such as Docker provide isolated computational environments with fixed software versions and dependencies. They are widely used in collaborative or production settings where analyses must run identically across different machines. They are powerful, but sit beyond the scope of this course and should be seen as an extension of the principles already discussed, not as a prerequisite.\nTesting frameworks such as testthat enable systematic verification of code behaviour, which becomes increasingly important as projects grow in size or are reused across contexts. At present, the emphasis is on writing code that is clear enough to inspect, rather than on formal test suites.\nThe purpose of introducing these tools here is not to encourage their immediate adoption, but to situate current practices within a broader trajectory. As projects scale, the same concerns (visibility, control, and recoverability) are addressed using increasingly formal mechanisms.\n\n\n\n\n\n\nNoteThe Mars Climate Orbiter Mission in 1998\n\n\n\nOne of the most famous examples of unclear communication is the Mars Climate Orbiter mission in 1998. NASA lost the spacecraft due to a navigation error caused by a unit conversion mistake. The error occurred because one team used metric units (newtons), while another team used imperial units (pound-force) for a crucial spacecraft operation parameter. The discrepancy in units led to incorrect trajectory calculations, causing the Mars Climate Orbiter to approach Mars at a dangerously low altitude, ultimately disintegrate in the Martian atmosphere.\nThis incident highlights the importance of clear communication, proper data handling, and rigorous verification processes in mission-critical systems, including space missions. The event emphasises the need for using appropriate tools, software, and methodologies to minimise the risk of errors in complex engineering projects.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "2. Working with Data and Code"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "13. Tidy Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA maximum of 10% may be deducted from your presentation marks should you be found to be dishonest in your self assessments.↩︎",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "basic_stats/13-confidence.html",
    "href": "basic_stats/13-confidence.html",
    "title": "13. Confidence Intervals",
    "section": "",
    "text": "1 Introduction\nA confidence interval (CI) tells us within what range we may be certain to find the true mean from which any sample has been taken. If we were to repeatedly sample the same population over and over and calculated a mean every time, the 95% CI indicates the range that 95% of those means would fall into.\n\n\n2 Calculating Confidence Intervals\n\nInput &lt;- (\"\nStudent  Grade   Teacher   Score  Rating\na        Gr_1    Vladimir  80     7\nb        Gr_1    Vladimir  90    10\nc        Gr_1    Vladimir 100     9\nd        Gr_1    Vladimir  70     5\ne        Gr_1    Vladimir  60     4\nf        Gr_1    Vladimir  80     8\ng        Gr_10   Vladimir  70     6\nh        Gr_10   Vladimir  50     5\ni        Gr_10   Vladimir  90    10\nj        Gr_10   Vladimir  70     8\nk        Gr_1    Sadam     80     7\nl        Gr_1    Sadam     90     8\nm        Gr_1    Sadam     90     8\nn        Gr_1    Sadam     80     9\no        Gr_10   Sadam     60     5\np        Gr_10   Sadam     80     9\nq        Gr_10   Sadam     70     6\nr        Gr_1    Donald   100    10\ns        Gr_1    Donald    90    10\nt        Gr_1    Donald    80     8\nu        Gr_1    Donald    80     7\nv        Gr_1    Donald    60     7\nw        Gr_10   Donald    60     8\nx        Gr_10   Donald    80    10\ny        Gr_10   Donald    70     7\nz        Gr_10   Donald    70     7\n\")\n\ndata &lt;- read.table(textConnection(Input), header = TRUE)\nsummary(data)\n\nR&gt;    Student             Grade             Teacher              Score       \nR&gt;  Length:26          Length:26          Length:26          Min.   : 50.00  \nR&gt;  Class :character   Class :character   Class :character   1st Qu.: 70.00  \nR&gt;  Mode  :character   Mode  :character   Mode  :character   Median : 80.00  \nR&gt;                                                           Mean   : 76.92  \nR&gt;                                                           3rd Qu.: 87.50  \nR&gt;                                                           Max.   :100.00  \nR&gt;      Rating      \nR&gt;  Min.   : 4.000  \nR&gt;  1st Qu.: 7.000  \nR&gt;  Median : 8.000  \nR&gt;  Mean   : 7.615  \nR&gt;  3rd Qu.: 9.000  \nR&gt;  Max.   :10.000\n\n\nThe package rcompanion has a convenient function for estimating the confidence intervals for our sample data. The function is called groupwiseMean() and it has a few options (methods) for estimating the confidence intervals, e.g. the ‘traditional’ way using the t-distribution, and a bootstrapping procedure.\nLet us produce the confidence intervals using the traditional method for the group means:\n\nlibrary(rcompanion)\n# Ungrouped data are indicated with a 1 on the right side of the formula,\n# or the group = NULL argument; so, this produces the overall mean\ngroupwiseMean(Score ~ 1, data = data, conf = 0.95, digits = 3)\n\nR&gt;    .id  n Mean Conf.level Trad.lower Trad.upper\nR&gt; 1 &lt;NA&gt; 26 76.9       0.95       71.7       82.1\n\n# One-way data\ngroupwiseMean(Score ~ Grade, data = data, conf = 0.95, digits = 3)\n\nR&gt;   Grade  n Mean Conf.level Trad.lower Trad.upper\nR&gt; 1  Gr_1 15   82       0.95       75.3       88.7\nR&gt; 2 Gr_10 11   70       0.95       62.6       77.4\n\n# Two-way data\ngroupwiseMean(Score ~ Teacher + Grade, data = data, conf = 0.95, digits = 3)\n\nR&gt;    Teacher Grade n Mean Conf.level Trad.lower Trad.upper\nR&gt; 1   Donald  Gr_1 5   82       0.95       63.6      100.0\nR&gt; 2   Donald Gr_10 4   70       0.95       57.0       83.0\nR&gt; 3    Sadam  Gr_1 4   85       0.95       75.8       94.2\nR&gt; 4    Sadam Gr_10 3   70       0.95       45.2       94.8\nR&gt; 5 Vladimir  Gr_1 6   80       0.95       65.2       94.8\nR&gt; 6 Vladimir Gr_10 4   70       0.95       44.0       96.0\n\n\nNow let us do it through bootstrapping:\n\ngroupwiseMean(Score ~ Grade,\n              data = data,\n              conf = 0.95,\n              digits = 3,\n              R = 10000,\n              boot = TRUE,\n              traditional = FALSE,\n              normal = FALSE,\n              basic = FALSE,\n              percentile = FALSE,\n              bca = TRUE)\n\nR&gt;   Grade  n Mean Boot.mean Conf.level Bca.lower Bca.upper\nR&gt; 1  Gr_1 15   82        82       0.95      75.3      86.7\nR&gt; 2 Gr_10 11   70        70       0.95      62.7      75.5\n\ngroupwiseMean(Score ~ Teacher + Grade,\n              data = data,\n              conf = 0.95,\n              digits = 3,\n              R = 10000,\n              boot = TRUE,\n              traditional = FALSE,\n              normal = FALSE,\n              basic = FALSE,\n              percentile = FALSE,\n              bca = TRUE)\n\nR&gt;    Teacher Grade n Mean Boot.mean Conf.level Bca.lower Bca.upper\nR&gt; 1   Donald  Gr_1 5   82      82.0       0.95      68.0      90.0\nR&gt; 2   Donald Gr_10 4   70      70.0       0.95      60.0      75.0\nR&gt; 3    Sadam  Gr_1 4   85      85.0       0.95      80.0      87.5\nR&gt; 4    Sadam Gr_10 3   70      70.0       0.95      60.0      76.7\nR&gt; 5 Vladimir  Gr_1 6   80      80.1       0.95      68.3      88.3\nR&gt; 6 Vladimir Gr_10 4   70      69.9       0.95      50.0      80.0\n\n\nThese upper and lower limits may then be used easily within a figure.\n\n# Load libraries\nlibrary(tidyverse)\n\n# Create dummy data\nr_dat &lt;- data.frame(value = rnorm(n = 20, mean = 10, sd = 2),\n                    sample = rep(\"A\", 20))\n\n# Create basic plot\nggplot(data = r_dat, aes(x = sample, y = value)) +\n  geom_errorbar(aes(ymin = mean(value) - sd(value), ymax = mean(value) + sd(value))) +\n  geom_jitter(colour = \"firebrick1\")\n\n\n\n\n\n\n\nFigure 1: A very basic figure showing confidence intervals (CI) for a random normal distribution.\n\n\n\n\n\n\n\n3 CI of Compared Means\nAS stated above, we may also use CI to investigate the difference in means between two, more sample sets of data. We have already seen this in the ANOVA Chapter but we shall look at it again here with our now expanded understanding of the concept.\n\n# First calculate ANOVA of seapl length of different iris species\niris_aov &lt;- aov(Sepal.Length ~ Species, data = iris)\n\n# Then run a Tukey test\niris_Tukey &lt;- TukeyHSD(iris_aov)\n\n# Lastly use base R to quickly plot the results\nplot(iris_Tukey)\n\n\n\n\n\n\n\nFigure 2: Results of a post-hoc Tukey test showing the confidence interval for the effect size between each group.\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {13. {Confidence} {Intervals}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/basic_stats/13-confidence.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 13. Confidence Intervals. http://samos-r.netlify.app/basic_stats/13-confidence.html.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "13. Confidence Intervals"
    ]
  },
  {
    "objectID": "basic_stats/11-decision_guide.html#the-importance-of-selecting-the-correct-test",
    "href": "basic_stats/11-decision_guide.html#the-importance-of-selecting-the-correct-test",
    "title": "11. Parametric Tests",
    "section": "1 The Importance of Selecting the Correct Test",
    "text": "1 The Importance of Selecting the Correct Test\nSelecting the appropriate inferential statistical method is important for correctly and accurately analysing the outcome of our sampling campaign or experimental treatment. The decision typically hinges on the type and distribution of our data, our research question, hypothesis and the assumptions each test requires.\nThe main decision-making process starts with the following considerations:\n\nResearch Question/Hypothesis: Start by clearly defining what we are trying to investigate or determine. Are we comparing group means? Investigating relationships between variables? Or assessing associations between categorical variables?\nType and Distribution of Data: Identify the types of variables we have (e.g., continuous, ordinal, nominal), check the distribution of our data (e.g. and normal vs. non-normal).\n\nThe foundation of the scientific process is hypotheses. These are the propositions or expectations that we set out to test. A hypothesis provides a direction to our research and guides us towards what we aim to prove.\nThe next step is to anticipate the nature of the data that our research will generate. This involves understanding not just the type of data (e.g., continuous, categorical), but also its potential distribution, variability. Such foresight stems from a clear understanding of the research design and the instruments we use, and the population we study. This might seem daunting to a novice, but experienced scientists should be able to do this with ease.\nOnce we have a firm grip on our hypotheses and a clear anticipation of the nature of our forthcoming data, we are in a position to choose the most suitable statistical inference test. Different tests are designed to handle different types of data, answer varied research questions. For instance and a t-test might be appropriate for comparing the means of two groups, while a linear model might shed insight into cause-effect relationships.\nWell-defined scientific enquiry should offer clarity. With this clarity, we can predict the statistical tests to use, even before the actual data are available. This is not just an academic exercise; it reflects thorough planning, a deep understanding of the research process. Knowing which tests to employ ahead of time also helps one to design the research methodology and ensure the data collected will indeed serve the purpose of the study.\nA robust scientific approach requires us to anticipate the nature of our data and understand our hypotheses thoroughly. This ensures that, even before our data are available, we are prepared with the appropriate statistical tools to analyse it, draw meaningful conclusions.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "11. Parametric Tests"
    ]
  },
  {
    "objectID": "basic_stats/11-decision_guide.html#a-detailed-breakdown-of-inferential-statistical-tests",
    "href": "basic_stats/11-decision_guide.html#a-detailed-breakdown-of-inferential-statistical-tests",
    "title": "11. Parametric Tests",
    "section": "2 A Detailed Breakdown of Inferential Statistical Tests",
    "text": "2 A Detailed Breakdown of Inferential Statistical Tests\nHere is a moderately detailed breakdown of the tests you will encounter in this module. Also included are tests that I have not (yet) covered, including Generalised Linear Models (GLMs), Generalised Additive Models (GAMs), and non-Linear Regressions.\n\nt-tests:\n\nUsed to compare means between two groups.\nAssumes independent samples, normally distributed data, and homogeneity of variance.\nIf the data are paired (e.g., before, after scores from the same group) and then a paired t-test is used.\nIf assumptions are not valid, use the Wilcoxon rank-sum (in lieu of a paired sample t-test) test, Mann-Whitney U test (in lieu of a Student or Welch’s t-test).\n\nANOVA (Analysis of Variance):\n\nUsed to compare means of three or more independent groups.\nAssumes independence, normal distribution, and homogeneity of variance across groups.\nIf assumptions are violated, consider a non-parametric equivalent (e.g., Kruskal-Wallis).\n\nANCOVA (Analysis of Covariance):\n\nExtends ANOVA by including one or more continuous covariates that might account for variability in the dependent variable.\nUsed to compare means of independent groups while statistically controlling for the effects of other continuous variables (covariates).\nIf assumptions are violated, consider a non-parametric equivalent (e.g., Kruskal-Wallis).\n\nChi-square Analysis:\n\nUsed for testing relationships between categorical variables.\nAssumes that observations are independent and that there are adequate expected frequencies in each cell of a contingency table.\n\nLinear Regression:\n\nExamines the linear relationship between a continuous dependent variable and one or more independent variables.\nCausality is typically implied (independent variable influences the outcome or measurement).\nAssumes linearity, independence of observations, homoscedasticity, and normally distributed residuals.\n\nGeneralised Linear Model (GLM):\n\nAn extension of linear regression that allows for response variables with error distribution models other than a normal distribution (e.g., Poisson, binomial).\nUseful when dealing with non-normally distributed dependent variables.\n\nnon-Linear Regression:\n\nUsed to model non-linear relationships which are described by cause-effect responses that are underpinned by well-defined mechanistic models or responses, often with parameter estimates that relate to components of the mechanistic model.\nAssumes independence of observations, homoscedasticity, and normally distributed residuals.\n\nGeneralised Additive Models (GAM):\n\nUsed to model non-linear relationships. It is an extension of GLM but does not restrict the relationship to be linear.\nAllows for flexible curves to be fit to data.\n\nCorrelations:\n\nUsed to examine the strength and direction of the linear relationship between two continuous variables.\n\nPearson’s: Assumes a linear relationship and that both variables are normally distributed.\nSpearman’s: Used when the relationship is monotonic but not necessarily linear, or when one/both of the variables are ordinal.\nKendall’s: Similar to Spearman’s but based on the concordant and discordant pairs. Useful for smaller sample sizes or when there are many tied ranks.\n\n\n\nRemember to always visualise your data and examine it thoroughly before selecting a test. If unsure, consider consulting with a statistician who can guide the decision-making process.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "11. Parametric Tests"
    ]
  },
  {
    "objectID": "basic_stats/11-decision_guide.html#a-tabulated-view",
    "href": "basic_stats/11-decision_guide.html#a-tabulated-view",
    "title": "11. Parametric Tests",
    "section": "3 A Tabulated View",
    "text": "3 A Tabulated View\nA tabulated summary of these tests is included below. Refer to 12. Non-parametric statistical tests at a glance for information about non-parametric tests to use when assumptions fail.\nStatistic | Application | Data Requirements | Assumptions |\n— — — — — — — — –| — — — — — — — — — — — — — — — — — — — — — — — — — — — — –| — — — — — — — — — — — — — — | — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — –|\nt-tests | Compare means between two groups. | Continuous dependent, categorical independent (2 groups). | Independent samples, normal distribution, homogeneity of variance. |\nANOVA | Compare means of three or more independent groups. | Continuous dependent, categorical independent (3+ groups). | Independence, normal distribution, homogeneity of variance across groups. |\nANCOVA | Compare means while controlling for other continuous variables. | Continuous dependent, categorical, continuous independents. | Same as ANOVA plus linearity and homogeneity of regression slopes. |\nChi-square Analysis | Test relationships between categorical variables. | Categorical variables. | Independent observations, adequate expected frequencies in each cell. |\nLinear Regression | Examine linear relationship between continuous variables. | Continuous dependent and independent(s). | Linearity, independence, homoscedasticity, normally distributed residuals. |\nNon-linear Regression| Model relationships that follow a specific non-linear equation. | Continuous dependent and independent(s). | Specific to the equation/form used, residuals should be random, normally distributed around zero. |\nGeneralised Linear Model (GLM) | Model relationships for non-normally distributed dependent variables. | Depending on link function (e.g., continuous, binary). | Depending on family (e.g., binomial: binary dependent; Poisson: count dependent). |\nGeneralised Additive Models (GAM) | Model non-linear relationships flexibly. | Continuous dependent, continuous/categorical independents. | Depending on response distribution but more flexible regarding the form of the predictors. |\nPearson’s Correlation | Measure linear association between two continuous variables. | Two continuous variables. | Both variables should be normally distributed, linear relationship. |\nSpearman’s Correlation| Measure monotonic relationship between two ordinal/continuous variables. | Two ordinal/continuous variables. | Monotonic relationship. Does not assume normality. |\nKendall’s Tau | Measure association between two ordinal variables. | Two ordinal variables. | No specific distributional assumptions. Measures strength of association based on concordant/discordant pairs. |",
    "crumbs": [
      "Home",
      "Biostatistics",
      "11. Parametric Tests"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#the-intercept",
    "href": "basic_stats/09-regressions.html#the-intercept",
    "title": "9. Simple Linear Regressions",
    "section": "5.1 The Intercept",
    "text": "5.1 The Intercept\nThe intercept (more precisely, the \\(y\\)-intercept, \\(\\alpha\\)) is the best estimate of the starting point of the fitted line on the left hand side of the graph where it crosses the \\(y\\)-axis. You will notice that there is also an estimate for the standard error of the estimate for the intercept.\nThere are several hypothesis tests associated with a simple linear regression. All of them assume that the residual error, \\(\\epsilon\\), in the linear regression model is independent of \\(X\\) (i.e. nothing about the structure of the error term can be inferred based on a knowledge of \\(X\\)), is normally distributed, with zero mean, constant variance. We say the residuals are i.i.d. (independent and identically distributed and which is a fancy way of saying they are random).\nOne of the tests looks at the significance of the intercept, i.e. it tests the H0 that \\(\\alpha=0\\). Is the value of the \\(y\\)-intercept zero? Rejecting this H0 causes the alternate hypothesis of \\(\\alpha \\neq 0\\) to be accepted. This test is automatically performed when fitting a linear model in R and asking for a summary of the regression object, but it is insightful, important to know that the test is simply a one-sample t-test. In the sparrows data and this statistic is in the Coefficients table in the row indicated by (Intercept) under the Pr(&gt;|t|) column.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#the-regression-coefficient",
    "href": "basic_stats/09-regressions.html#the-regression-coefficient",
    "title": "9. Simple Linear Regressions",
    "section": "5.2 The Regression Coefficient",
    "text": "5.2 The Regression Coefficient\nThe interpretation of the regression coefficient, \\(\\beta\\), is simple. For every one unit of change in the independent variable (here waiting time) there is a corresponding average change in the dependent variable (here the duration of the eruption). This is the slope or gradient, and it may be positive, negative. In the example the slope of the line is denoted by the value0.27 \\(cm.day^{-1}\\) in the column termed Estimate and in the row called age (the latter name will of course depend on the name of the response column in your dataset). The coefficient of determination (\\(r^2\\), see Section 7.2) multiplies the response variable to produce a prediction of the response based on the slope of the relationship between the response and the predictor. It tells us how much one unit in change of the independent variable determines the corresponding change in the response variable. There is also a standard error for the estimate.\nThe second hypothesis test performed when fitting a linear regression model concerns the regression coefficient. It looks for whether there is a significant relationship (slope) of \\(Y\\) on \\(X\\) by testing the H0 that \\(\\beta=0\\). As before, this is also simply a one-sample t-test. In the regression summary the probability associated with this test is given in the Coefficients table in the column called Pr(&gt;|t|) in the row age. In the sparrows data, the p-value associated with wing is less than 0.05 and we therefore reject the H0 that \\(\\beta=0\\). So, there is a significant linear relationship of eruption duration on the waiting time between eruptions.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#residual-standard-error-rse-and-root-mean-square-error-rmse",
    "href": "basic_stats/09-regressions.html#residual-standard-error-rse-and-root-mean-square-error-rmse",
    "title": "9. Simple Linear Regressions",
    "section": "7.1 Residual Standard Error (RSE) and Root Mean Square Error (RMSE)",
    "text": "7.1 Residual Standard Error (RSE) and Root Mean Square Error (RMSE)\nThe residual standard error (RSE) is a measure of the average amount that the response variable deviates from the regression line. It is calculated as the square root of the residual sum of squares divided by the degrees of freedom (Equation 3).\n\n\nThe RSE: \\[RSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2}{n-2}} \\tag{3}\\]\nwhere \\(y_i\\) represents the observed value of the dependent variable for the \\(i\\)-th observation, \\(\\hat{y}_i\\) represents the predicted value of the dependent variable for the \\(i\\)-th observation, and n is the number of observations in the sample.\nThe root mean square error (RMSE) is a similar measure, but it is calculated as the square root of the mean of the squared residuals. It is a measure of the standard deviation of the residuals (Equation 4).\n\n\nThe RMSE: \\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2} \\tag{4}\\] where the model components are as in Equation 3.\nRSE and RMSE are similar but different. There is a small difference in how they are calculated. The RSE takes into account the degrees of freedom which becomes important when models with different numbers of variables are compared. The RMSE is more commonly used in machine learning and data mining, where the focus is on prediction accuracy rather than statistical inference.\nBoth the RSE and RMSE provide information about the amount of error in the model predictions, with smaller values indicating a better fit. However, both may be influenced by outliers, other sources of variability in the data. Use a variety of means to assess the model fit diagnostics.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#sec-coef-det",
    "href": "basic_stats/09-regressions.html#sec-coef-det",
    "title": "9. Simple Linear Regressions",
    "section": "7.2 R-squared (R2)",
    "text": "7.2 R-squared (R2)\nThe coefficient of determination, the \\(R^{2}\\), of a linear model is the quotient of the variances of the fitted values, \\(\\hat{y_{i}}\\), and observed values, \\(y_{i}\\), of the dependent variable. If the mean of the dependent variable is \\(\\bar y\\), then the \\(R^{2}\\) is as shown in Equation 5.\n\n\n\n\n\n\nThe R2: \\[R^{2}=\\frac{\\sum(\\hat{Y_{i}} - \\bar{Y})^{2}}{\\sum(Y_{i} - \\bar{Y})^{2}} \\tag{5}\\]\n\n\n\n\n\n\n\n\nFigure 3: A linear regression through random normal data.\n\n\n\n\n\nSimply put, the \\(R^{2}\\) is a measure of the proportion of the variation in the dependent variable that is explained (can be predicted) by the independent variable(s) in the model. It ranges from 0 to 1, with a value of 1 indicating a perfect fit (i.e. a scatter of points to denote the \\(Y\\) vs. \\(X\\) relationship will all fall perfectly on a straight line). It gives us an indication of how well the observed outcome variable is predicted by the observed influential variable, and in the case of a simple linear regression, that the geometric relationship of \\(Y\\) on \\(X\\) is a straight line. For example, in Figure 3 there is absolutely no relationship of \\(y\\) on \\(x\\). Here, the slope is 0.001 and the \\(R^{2}\\) is 0.\nNote, however, that a high \\(R^{2}\\) does not necessarily mean that the model is a good fit; it may also suggest that the model is unduly influenced by outliers or the inclusion of irrelevant variables. Expert knowledge will help with the interpretation of the \\(R^{2}\\).\n\n\nRegressions may take on any relationship, not only a linear one. For example, there are parabolic, hyperbolic, logistic, exponential, etc. relationships of \\(Y\\) on \\(X\\), and here, too, does \\(R^{2}\\) tell us the same thing. If we assume that the samples were representatively drawn from a population (i.e. the sample fully captures the relationship of \\(Y\\) on \\(X\\) that is present in the entire population), the \\(R^{2}\\) will represent the relationship in the population too.\n\nIn the case of our sparrows data, the \\(R^{2}\\) is 0.973, meaning that the proportion of variance explained is 97.3%; the remaining 2.7% is not (yet) accounted for by the linear relationship. Adding more predictors into the regression (i.e. a multiple regression) might consume some of the unexplained variance and increase the overall \\(R^{2}\\).\nSometimes you will also see something called the adjusted \\(R^{2}\\). This is a modified version of \\(R^{2}\\) that takes into account the number of independent variables in the model. It penalises models that include too many variables that do not improve the fit. Generally this is not something to be too concerned with in linear models that have only one independent variable, such as the models seen in this Chapter.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#f-statistic",
    "href": "basic_stats/09-regressions.html#f-statistic",
    "title": "9. Simple Linear Regressions",
    "section": "7.3 F-statistic",
    "text": "7.3 F-statistic\nThe F-statistic (or F-value) is another measure of the overall significance of the model. It is used to test whether at least one of the independent variables in the model has a non-zero coefficient, indicating that it has a significant effect on the dependent variable.\nIt is calculated by taking the ratio of the mean square regression (MSR) to the mean square error (MSE) (Equation 6). The MSR measures the variation in the dependent variable that is explained by the independent variables in the model, while the MSE measures the variation in the dependent variable that is not explained by the independent variables.\n\n\nCalculating the F-statistic: \\[MSR = \\frac{\\sum_{i=1}^{n}(\\hat{Y}_i - \\bar{Y})^2}{1}\\] \\[MSE = \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{n-2}\n\\] \\[F = \\frac{MSR}{MSE} \\tag{6}\\]\nwhere the model components are as in Equation 3.\nIf the F-statistic is large and the associated p-value is small (typically less than 0.05), it indicates that at least one of the independent variables in the model has a significant effect on the dependent variable. In other words, the H0 that all the independent variables have zero coefficients can be rejected in favour of the Ha that at least one independent variable has a non-zero coefficient.\nNote that a significant F-statistic does not necessarily mean that all the independent variables in the model are significant. Additional diagnostic tools, such as individual t-tests, residual plots and should be used to determine which independent variables are significant and whether the model is a good fit for the data.\nFortunately, in this Chapter we will encounter linear regressions with only one independent variable. The situation where we deal with multiple independent variables is called multiple regression. We will encounter some multiple regression type models in Quantitative Ecology.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#plot-of-residuals-vs.-fitted-values",
    "href": "basic_stats/09-regressions.html#plot-of-residuals-vs.-fitted-values",
    "title": "9. Simple Linear Regressions",
    "section": "11.1 Plot of Residuals Vs. Fitted Values",
    "text": "11.1 Plot of Residuals Vs. Fitted Values\nA residual plot shows the residuals (values predicted by the linear model, \\(\\hat{Y}\\), minus the observed values, \\(Y\\), on the y-axis and the independent (\\(X\\)) variable on the x-axis. Points in a residual plot that are randomly dispersed around the horizontal axis indicates a linear regression model that is appropriate for the data. If this simple ‘test’ fails, a non-linear model might be more appropriate, or one might transform the data to normality (assuming that the non-normality of the data is responsible for the non-random dispersal above, below the horizontal line).",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#plot-of-standardised-residuals",
    "href": "basic_stats/09-regressions.html#plot-of-standardised-residuals",
    "title": "9. Simple Linear Regressions",
    "section": "11.2 Plot of Standardised Residuals",
    "text": "11.2 Plot of Standardised Residuals\nWe may use a plot of the residuals vs. the fitted values, which is helpful for detecting heteroscedasticity — e.g. a systematic change in the spread of residuals over a range of predicted values.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/09-regressions.html#normal-probability-plot-of-residuals-normal-q-q-plot",
    "href": "basic_stats/09-regressions.html#normal-probability-plot-of-residuals-normal-q-q-plot",
    "title": "9. Simple Linear Regressions",
    "section": "11.3 Normal Probability Plot of Residuals (Normal Q-q Plot)",
    "text": "11.3 Normal Probability Plot of Residuals (Normal Q-q Plot)\nLet see all these plots in action for the sparrows data. The package ggfortify has a convenient function to automagically make all of these graphs:\n\nlibrary(ggfortify)\nautoplot(lm(wing ~ age, data = sparrows), label.size = 3,\n         col = \"red3\", shape = 10, smooth.colour = 'blue3')\n\n\n\n\nFour diagnostic plots testing the assumptions to be met for linear regressions.\n\n\n\n\nOne might also use the package gg_diagnose to create all the various (above plus some!) diagnostic plots available for fitted linear models.\nDiagnostic plots will be further explored in the exercises (see below).",
    "crumbs": [
      "Home",
      "Biostatistics",
      "9. Simple Linear Regressions"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#two-sided-one-sample-t-test",
    "href": "basic_stats/07-t_tests.html#two-sided-one-sample-t-test",
    "title": "7. t-Tests",
    "section": "3.1 Two-sided One-sample t-test",
    "text": "3.1 Two-sided One-sample t-test\n\n\n\n\n\n\nNoteHypothesis for Two-sided One-sample t-test\n\n\n\n\\(H_{0}: \\bar{x} = \\mu_{0}\\)\\(H_{a}: \\bar{x} \\ne \\mu_{0}\\)\nThis is the same as:\n\\(H_{0}: \\bar{x} - \\mu_{0} = 0\\)\\(H_{a}: \\bar{x} - \\mu_{0} \\ne 0\\)\nHere \\(\\bar{x}\\) is the population mean\\(\\mu_{0}\\) the hypothesised mean to which \\(\\bar{x}\\) is being compared. In this example we have a two-sided one-sample t-test.\nAs stated above, \\(H_{0}\\) does not make a distinction between whether it expects \\(\\bar{x}\\) to be \\(\\lt\\)\\(\\gt\\) \\(\\mu_{0}\\).\n\n\nGenerally when we use a t-test it will be a two-sample t-test (see below). Occasionally, however, we may have only one set of observations (random samples taken to represent a population) whose mean, \\(\\bar{x}\\), we wish to compare against a known population mean, \\(\\mu_{0}\\), which had been established a priori (Equation 1). In R’s t.test() function, the default setting is for a two-sided one-sample t-test — that is, we do not care if our \\(H_{a}\\) is significantly less than \\(\\mu_{0}\\) or if it is significantly greater than \\(\\mu_{0}\\).\n\n\nThe one-sample t-test:\n\\[t = \\frac{\\overline{x} - \\mu}{s / \\sqrt{n}} \\tag{1}\\]\nwhere \\(t\\) is the calculated \\(t\\)-value, \\(\\overline{x}\\) is the sample mean, \\(\\mu\\) is the hypothesised population mean, \\(s\\) is the sample standard deviation,\\(n\\) the sample size.\n\n# create a single sample of random normal data\nset.seed(666)\nr_one &lt;- data.frame(dat = rnorm(n = 20, mean = 20, sd = 5),\n                    sample = \"A\")\n\n\n\n\n\n\n# compare random data against a population mean of 20\nt.test(r_one$dat, mu = 20)\n\nR&gt; \nR&gt;  One Sample t-test\nR&gt; \nR&gt; data:  r_one$dat\nR&gt; t = 0.0048653, df = 19, p-value = 0.9962\nR&gt; alternative hypothesis: true mean is not equal to 20\nR&gt; 95 percent confidence interval:\nR&gt;  16.91306 23.10133\nR&gt; sample estimates:\nR&gt; mean of x \nR&gt;  20.00719\n\n# compare random data against a population mean of 30\nt.test(r_one$dat, mu = 30)\n\nR&gt; \nR&gt;  One Sample t-test\nR&gt; \nR&gt; data:  r_one$dat\nR&gt; t = -6.7596, df = 19, p-value = 1.858e-06\nR&gt; alternative hypothesis: true mean is not equal to 30\nR&gt; 95 percent confidence interval:\nR&gt;  16.91306 23.10133\nR&gt; sample estimates:\nR&gt; mean of x \nR&gt;  20.00719\n\n\nWhat do the results of these two different tests show? Let us visualise these data to get a better understanding (Figure 1).\n\nggplot(data = r_one, aes(y = dat, x = sample)) +\n  geom_boxplot(fill = \"indianred\", notch = TRUE,\n               alpha = 0.3, colour = \"black\") +\n  # population  mean (mu) = 20\n  geom_hline(yintercept = 20, colour = \"dodgerblue2\", \n             size = 0.9) +\n  # population  mean (mu) = 30\n  geom_hline(yintercept = 30, colour = \"indianred2\", \n             size = 0.9) +\n  labs(y = \"Value\", x = NULL) +\n  coord_flip() +\n  theme_pubclean()\n\n\n\n\n\n\n\nFigure 1: Boxplot of random normal data with. A hypothetical population mean of 20 is shown as a blue line, with the red line showing a mean of 30.\n\n\n\n\n\nThe boxplot shows the distribution of our random data against two potential population means. Does this help now to illustrate the results of our one-sample t-tests?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#one-sided-one-sample-t-tests",
    "href": "basic_stats/07-t_tests.html#one-sided-one-sample-t-tests",
    "title": "7. t-Tests",
    "section": "3.2 One-sided One-sample t-tests",
    "text": "3.2 One-sided One-sample t-tests\n\n\n\n\n\n\nNoteHypothesis for One-sided One-sample t-test\n\n\n\nFor example, when we are concerned that our sample mean, \\(\\bar{x}\\), should be less than the a priori established value, \\(\\mu_{0}\\):\n\\(H_{0}: \\bar{x} \\ge \\mu_{0}\\)\\(H_{a}: \\bar{x} \\lt \\mu_{0}\\)\nOnly one of the two options is shown.\n\n\nRemember that a normal distribution has two tails. As indicated already, when we are testing for significance we are generally looking for a result that sits in the far end of either of these tails. Occasionally, however, we may want to know if the result is specifically in one of the two tails. Explicitly the leading, trailing tail. For example, is the mean value of our sample population, \\(\\bar{x}\\), significantly greater than the value \\(\\mu_{0}\\)? Or, is \\(\\bar{x}\\) less than the value \\(\\mu_{0}\\)? This t-test is called a one-sided one-sample t-tests. To specify this in R we must add an argument as seen below:\n\n# check against the trailing tail\nt.test(r_one$dat, mu = 30, alternative = \"less\")\n\nR&gt; \nR&gt;  One Sample t-test\nR&gt; \nR&gt; data:  r_one$dat\nR&gt; t = -6.7596, df = 19, p-value = 9.292e-07\nR&gt; alternative hypothesis: true mean is less than 30\nR&gt; 95 percent confidence interval:\nR&gt;      -Inf 22.56339\nR&gt; sample estimates:\nR&gt; mean of x \nR&gt;  20.00719\n\n# check against the leading tail\nt.test(r_one$dat, mu = 30, alternative = \"greater\")\n\nR&gt; \nR&gt;  One Sample t-test\nR&gt; \nR&gt; data:  r_one$dat\nR&gt; t = -6.7596, df = 19, p-value = 1\nR&gt; alternative hypothesis: true mean is greater than 30\nR&gt; 95 percent confidence interval:\nR&gt;  17.451    Inf\nR&gt; sample estimates:\nR&gt; mean of x \nR&gt;  20.00719\n\n\nAre these the results we would have expected? Why does the second test not return a significant result?\n\n\n\n\n\n\nImportantDo This Now!\n\n\n\nCreate a visualisation to graphically demonstrate the outcome of this t-test.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#two-sided-two-sample-t-test",
    "href": "basic_stats/07-t_tests.html#two-sided-two-sample-t-test",
    "title": "7. t-Tests",
    "section": "4.1 Two-sided Two-sample t-test",
    "text": "4.1 Two-sided Two-sample t-test\n\n\n\n\n\n\nNoteHypothesis for Two-sided Two-sample t-test\n\n\n\n\\(H_{0}: \\bar{A} = \\bar{B}\\)\\(H_{a}: \\bar{A} \\ne \\bar{B}\\)\nwhere \\(\\bar{A}\\) is the population mean of the first sample\\(\\bar{B}\\) the population mean of the second sample. In this example we have a two-sided two-sample t-test, which is the default in R’s t.test() function.\nAs stated above, \\(H_{0}\\) does not make a distinction between whether it expects the difference between \\(\\bar{A}\\)\\(\\bar{B}\\) to be greater than or less than 0.\n\n\nA two-sample t-test is used when we have samples from two different (independent) populations whose means, \\(\\bar{A}\\)\\(\\bar{B}\\), we would like to compare against one another. Sometimes it is called an independent sample t-test. Specifically, it tests whether the difference between the means of two samples is zero. Note that again we make no distinction between whether it is more interesting that the difference is greater than zero, less zero — as long as there is a difference between\\(\\bar{A}\\)\\(\\bar{B}\\). This test is called a two-sided two sample t-test and it is the most common use of a t-test.\nThere are two varieties of t-tests. In the case of samples whose variances do not differ, we perform a Student’s t-test. Equation 2 shows how to calculate the t-statistic for Student’s t-test. The other case is if we have unequal variances in \\(\\bar{A}\\)\\(\\bar{B}\\) (established with the Levene’s test for equality of variances; see Chapter 6); here, we perform Welch’s t-test as written in Equation 4. Welch’s t-test is the default in R’s t.test() function.\n\n\nStudent’s t-test: \\[t=\\frac{\\bar{A}-\\bar{B}}{\\sqrt{\\frac{S^{2}}{n}+\\frac{S^{2}}{m}}} \\tag{2}\\]\n\\(\\bar{A}\\)\\(\\bar{B}\\) are the means for groups \\(A\\)\\(B\\), respectively; \\(n\\)\\(m\\) are the sample sizes of the two sets of samples, respectively;\\(S^{2}\\) is the pooled variance, which is calculated as per Equation 3:\n\\[S^{2}=\\frac{(n-1)S_{A}^{2}+(m-1)S_{B}^{2} }{n+m-2} \\tag{3}\\]\nThe degrees of freedom, d.f., in the equation for the shared variance is \\(n_{A}+m_{B}-2\\).\n\nWelch’s t-test: \\[t=\\frac{\\bar{A}-\\bar{B}}{\\sqrt{\\frac{S^{2}_{A}}{n}+\\frac{S^{2}_{B}}{m}}} \\tag{4}\\]\nHere, \\(S_{A}\\)\\(S_{B}\\) are the variances of groups \\(A\\)\\(B\\), respectively (see Section X). The d.f. to use with Welch’s t-test is obtained using the Welch–Satterthwaite equation (Equation 5):\n\\[d.f. = \\frac{\\left( \\frac{S^{2}_{A}}{n}+\\frac{S^{2}_{B}}{m} \\right)^{2}}{\\left( \\frac{S^{4}_{A}}{n-1} + \\frac{S^{4}_{B}}{m-1} \\right)} \\tag{5}\\]\n\nWhat do we do with this t-statistic? In the olden days we had to calculate the t-statistics and the d.f. by hand. These two values, the d.f., t-value had to be read off a table of pre-calculated t-values and probabilities and degrees of freedom as in here. Luckily, the t-test function nowadays does this all automagically. But if you are feeling nostalgic over times that you have sadly never experienced, please calculate the t-statistic, the d.f. yourself and give the table a go. In fact and an excessive later in this chapter will give you an opportunity to do so.\nBack to the present day and the wonders of modern technology. Let us generate some new random normal data and test to see if the data belonging to the two groups differ significantly from one-another. First, we apply the t-test function as usual:\n\n# random normal data\nset.seed(666)\nr_two &lt;- data.frame(dat = c(rnorm(n = 20, mean = 4, sd = 1),\n                            rnorm(n = 20, mean = 5, sd = 1)),\n                    sample = c(rep(\"A\", 20), rep(\"B\", 20)))\n\n# perform t-test\n# note how we set the `var.equal` argument to TRUE because we know \n# our data has the same SD (they are simulated as such!)\nt.test(dat ~ sample, data = r_two, var.equal = TRUE)\n\nR&gt; \nR&gt;  Two Sample t-test\nR&gt; \nR&gt; data:  dat by sample\nR&gt; t = -1.9544, df = 38, p-value = 0.05805\nR&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0\nR&gt; 95 percent confidence interval:\nR&gt;  -1.51699175  0.02670136\nR&gt; sample estimates:\nR&gt; mean in group A mean in group B \nR&gt;        4.001438        4.746584\n\n# if the variances are not equal, simply set `var.equal` to false\n# and a Welch's t-test will be performed\n\nThe first argument we see in t.test() is dat ~ sample. Usually in R when we see a ~ (tilde) we are creating what is known as a formula. A formula tells R how it should look for interactions between data and factors. For example Y ~ X reads: \\(Y\\) as a function of \\(X\\). In our code above we see dat ~ sample. This means we are telling R that the t-test we want it to perform is when the dat column is a function of the sample column. In plain English we are dividing up the dat column into the two different samples we have, and then running a t-test on these samples. Another way of stating this is that the value of dat depends on the grouping it belong to (A and B). We will see this same formula notation cropping up later under ANOVAs, linear models, etc.\n\n\n\n\n\n\nImportantDo This Now!\n\n\n\nCreate a visualisation to graphically demonstrate the outcome of this t-test.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#one-sided-two-sample-t-test",
    "href": "basic_stats/07-t_tests.html#one-sided-two-sample-t-test",
    "title": "7. t-Tests",
    "section": "4.2 One-sided Two-sample t-test",
    "text": "4.2 One-sided Two-sample t-test\n\n\n\n\n\n\nNoteHypothesis for One-sided Two-sample t-test\n\n\n\nFor example, when we are concerned that the sample mean of the first population, \\(\\bar{A}\\), should be greater than that of the second, \\(\\bar{B}\\):\n\\(H_{0}: \\bar{A} \\le \\bar{B}\\)\\(H_{a}: \\bar{A} \\gt \\bar{B}\\)\nOnly one of the two options is shown.\n\n\nJust as with the one-sample t-tests above, we may also specify which tail of the distribution we are interested in when we compare the means of our two samples. This is a one-sided two-sample t-test, and here too we have the Student’s t-test, Welch’s t-test varieties. We do so by providing the same arguments as previously:\n\n# is the mean of sample B smaller than that of sample A?\ncompare_means(dat ~ sample, data = r_two,\n              method = \"t.test\", var.equal = TRUE,\n              alternative = \"less\")\n\nR&gt; # A tibble: 1 × 8\nR&gt;   .y.   group1 group2     p p.adj p.format p.signif method\nR&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \nR&gt; 1 dat   A      B      0.971  0.97 0.97     ns       T-test\n\n# is the mean of sample B greater than that of sample A?\ncompare_means(dat ~ sample, data = r_two,\n              method = \"t.test\", var.equal = TRUE,\n              alternative = \"greater\")\n\nR&gt; # A tibble: 1 × 8\nR&gt;   .y.   group1 group2      p p.adj p.format p.signif method\nR&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \nR&gt; 1 dat   A      B      0.0290 0.029 0.029    *        T-test\n\n\nWhat do these results show? Is this surprising?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#one-sample-and-two-sample-tests",
    "href": "basic_stats/07-t_tests.html#one-sample-and-two-sample-tests",
    "title": "7. t-Tests",
    "section": "6.1 One-sample and Two-sample Tests",
    "text": "6.1 One-sample and Two-sample Tests\nAs with t-tests, proportion tests may also be based on one sample, or two. If we have only one sample we must specify the total number of trials as well as what the expected population probability of success is. Because these are individual values, and not matrices, we will show what this would look like without using any objects but will rather give each argument within prop.test() a single exact value. In the arguments within prop.test(), x denotes the number of successes recorded, n shows the total number of individual trials performed, p is the expected probability. It is easiest to consider this as though it were a series of 100 coin tosses.\n\n# When the probability matches the population\nprop.test(x = 45, n = 100, p = 0.5)\n\nR&gt; \nR&gt;  1-sample proportions test with continuity correction\nR&gt; \nR&gt; data:  45 out of 100, null probability 0.5\nR&gt; X-squared = 0.81, df = 1, p-value = 0.3681\nR&gt; alternative hypothesis: true p is not equal to 0.5\nR&gt; 95 percent confidence interval:\nR&gt;  0.3514281 0.5524574\nR&gt; sample estimates:\nR&gt;    p \nR&gt; 0.45\n\n# When it does not\nprop.test(x = 33, n = 100, p = 0.5)\n\nR&gt; \nR&gt;  1-sample proportions test with continuity correction\nR&gt; \nR&gt; data:  33 out of 100, null probability 0.5\nR&gt; X-squared = 10.89, df = 1, p-value = 0.0009668\nR&gt; alternative hypothesis: true p is not equal to 0.5\nR&gt; 95 percent confidence interval:\nR&gt;  0.2411558 0.4320901\nR&gt; sample estimates:\nR&gt;    p \nR&gt; 0.33\n\n\nIf we have two samples that we would like to compare against one another we enter them into the function as follows:\n\n# NB: Note that the `mosquito` data are a matrix, NOT a data.frame\nprop.test(mosquito)\n\nR&gt; \nR&gt;  2-sample test for equality of proportions with continuity correction\nR&gt; \nR&gt; data:  mosquito\nR&gt; X-squared = 3.5704, df = 1, p-value = 0.05882\nR&gt; alternative hypothesis: two.sided\nR&gt; 95 percent confidence interval:\nR&gt;  -0.253309811  0.003309811\nR&gt; sample estimates:\nR&gt;    prop 1    prop 2 \nR&gt; 0.5833333 0.7083333\n\n\nDo mosquito’s bite Jack and Jill at different proportions?\n\n\n\n\n\n\nImportantDo This Now!\n\n\n\nTask F.4. Divide the class into two groups, Group A, Group B. In each group and collect data on 100 coin tosses. The intention is to compare the coin tosses across Groups A and B. State your hypothesis. Test it. Discuss. Report back in Task F.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#one-sided-and-two-sided-tests",
    "href": "basic_stats/07-t_tests.html#one-sided-and-two-sided-tests",
    "title": "7. t-Tests",
    "section": "6.2 One-sided and Two-sided Tests",
    "text": "6.2 One-sided and Two-sided Tests\nAs with all other tests that compare values, proportion tests may be specified as either one, two-sided. Just to be clear or the default setting for prop.test(), like everything else, is a two-sided test. See code below to confirm that the results are identical with, without the added argument:\n\n# Default\nprop.test(mosquito)\n\nR&gt; \nR&gt;  2-sample test for equality of proportions with continuity correction\nR&gt; \nR&gt; data:  mosquito\nR&gt; X-squared = 3.5704, df = 1, p-value = 0.05882\nR&gt; alternative hypothesis: two.sided\nR&gt; 95 percent confidence interval:\nR&gt;  -0.253309811  0.003309811\nR&gt; sample estimates:\nR&gt;    prop 1    prop 2 \nR&gt; 0.5833333 0.7083333\n\n# Explicitly state two-sided test\nprop.test(mosquito, alternative = \"two.sided\")\n\nR&gt; \nR&gt;  2-sample test for equality of proportions with continuity correction\nR&gt; \nR&gt; data:  mosquito\nR&gt; X-squared = 3.5704, df = 1, p-value = 0.05882\nR&gt; alternative hypothesis: two.sided\nR&gt; 95 percent confidence interval:\nR&gt;  -0.253309811  0.003309811\nR&gt; sample estimates:\nR&gt;    prop 1    prop 2 \nR&gt; 0.5833333 0.7083333\n\n\nShould we want to specify only one of the tails to be considered, we do so precisely the same as with t-tests. Below are examples of what this code would look like:\n\n# Jack is bit less than Jill\nprop.test(mosquito, alternative = \"less\")\n\nR&gt; \nR&gt;  2-sample test for equality of proportions with continuity correction\nR&gt; \nR&gt; data:  mosquito\nR&gt; X-squared = 3.5704, df = 1, p-value = 0.02941\nR&gt; alternative hypothesis: less\nR&gt; 95 percent confidence interval:\nR&gt;  -1.00000000 -0.01597923\nR&gt; sample estimates:\nR&gt;    prop 1    prop 2 \nR&gt; 0.5833333 0.7083333\n\n# Jack is bit more than Jill\nprop.test(mosquito, alternative = \"greater\")\n\nR&gt; \nR&gt;  2-sample test for equality of proportions with continuity correction\nR&gt; \nR&gt; data:  mosquito\nR&gt; X-squared = 3.5704, df = 1, p-value = 0.9706\nR&gt; alternative hypothesis: greater\nR&gt; 95 percent confidence interval:\nR&gt;  -0.2340208  1.0000000\nR&gt; sample estimates:\nR&gt;    prop 1    prop 2 \nR&gt; 0.5833333 0.7083333\n\n\nDo these results differ from the two-sided test? What is different?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#loading-data",
    "href": "basic_stats/07-t_tests.html#loading-data",
    "title": "7. t-Tests",
    "section": "7.1 Loading Data",
    "text": "7.1 Loading Data\nBefore we can run any analyses we will need to load our data. We are also going to convert these data from their wide format into a long format because this is more useful for the rest of our workflow.\n\necklonia &lt;- read_csv(here::here(\"data\", \"BCB744\", \"ecklonia.csv\")) %&gt;% \n  gather(key = \"variable\", value = \"value\", -species, -site, -ID)",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#visualising-data",
    "href": "basic_stats/07-t_tests.html#visualising-data",
    "title": "7. t-Tests",
    "section": "7.2 Visualising Data",
    "text": "7.2 Visualising Data\nWith our data loaded, let us visualise them in order to ensure that these are indeed the data we are after (Figure 2). Visualising the data will also help us to formulate a hypothesis.\n\nggplot(data = ecklonia, aes(x = variable, y = value, fill = site)) +\n  geom_boxplot(colour = \"black\", fill = \"dodgerblue4\", alpha = 0.4) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 2: Boxplots showing differences in morphometric properties of the kelp Ecklonia maxima at two sites in False Bay.\n\n\n\n\n\nThe first thing we should notice from the figure above is that our different measurements are on very different scales. This makes comparing all of our data visually rather challenging. Even given this complication, one should readily be able to make out that the measurement values at Batsata Rock appear to be greater than at Boulders Beach. Within the framework of the scientific process, that is what we would call an ‘observation’, and is the first step towards formulating a hypothesis. The next step is to refine our observation into a hypothesis. By what measurement are the kelps greater at one site than the other?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#formulating-a-hypothesis",
    "href": "basic_stats/07-t_tests.html#formulating-a-hypothesis",
    "title": "7. t-Tests",
    "section": "7.3 Formulating a Hypothesis",
    "text": "7.3 Formulating a Hypothesis\nLooking at the figure above it appears that for almost all measurements of length, Batsata Rock far exceeds that of Boulders Beach however, the stipe masses between the two sites appear to be more similar. Let us pull out just this variable, create a new boxplot (Figure 3).\n\n# filter the data\necklonia_sub &lt;- ecklonia %&gt;% \n  filter(variable == \"stipe_mass\")\n\n# then create a new figure\nggplot(data = ecklonia_sub, aes(x = variable, y = value, fill = site)) +\n  geom_boxplot(colour = \"black\", alpha = 0.4) +\n  coord_flip() +\n  labs(y = \"Stipe mass (kg)\", x = \"\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 3: Boxplots showing the difference in stipe mass (kg) of the kelp Ecklonia maxima at two sites in False Bay.\n\n\n\n\n\nNow we have a more interesting comparison at hand. The question I think of when I look at these data is “Are the stipe masses at Batsata Rock greater than at Boulders Beach?”. The hypothesis necessary to answer this question would look like this:\n\nH0: Stipe mass at Batsata Rock is not greater than at Boulders Beach.\nHa: Stipe mass at Batsata Rock is greater than at Boulders Beach.\n\nOr more formally:\n\n\\(H_{0}: \\bar{A} \\leq \\bar{B}\\)\n\\(H_{a}: \\bar{A} &gt; \\bar{B}\\).\n\nWhich test must we use for this hypothesis?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#choosing-a-test",
    "href": "basic_stats/07-t_tests.html#choosing-a-test",
    "title": "7. t-Tests",
    "section": "7.4 Choosing a Test",
    "text": "7.4 Choosing a Test\nBefore we can pick the correct statistical test for our hypothesis, we need to be clear on what it is we are asking. Starting with the data being used is usually a good first step. As we may see in the above figure, we have two sample sets that we are comparing. Therefore, unsurprisingly, we will likely be using a t-test. But we are not done yet. How is it that we are comparing these two sample sets? Remember from the examples above that there are multiple different ways to compare two sets of data. For our hypothesis we want to see if the stipe mass at Batsata Rock is greater than the stipe mass at Boulders Beach, not just that they are different. Because of this we will need a one-sided t-test. But wait, there is more! We have zeroed in on which sort of test would be appropriate for our hypothesis, but before we run it we need to check our assumptions.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#checking-assumptions",
    "href": "basic_stats/07-t_tests.html#checking-assumptions",
    "title": "7. t-Tests",
    "section": "7.5 Checking Assumptions",
    "text": "7.5 Checking Assumptions\nIn case we forgot, here are the assumptions for a t-test:\n\nthe dependent variable must be continuous,\nthe observations in the groups being compared are independent of each other,\nthe data are normally distributed,\nthat the data are homoscedastic, and in particular, that there are no outliers.\n\nWe know that the first two assumptions are met because our data are measurements of mass at two different sites. Before we can run our one-sided t-test we must meet the last two assumptions. Lucky us, we have a function tat will do that automagically.\nPlease refer to Chapter 6 to see what to do if the assumptions fail.\n\necklonia_sub %&gt;% \n  group_by(site) %&gt;% \n  summarise(stipe_mass_var = two_assum(value)[1],\n            stipe_mass_norm = two_assum(value)[2])\n\nR&gt; # A tibble: 2 × 3\nR&gt;   site           stipe_mass_var stipe_mass_norm\nR&gt;   &lt;chr&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\nR&gt; 1 Batsata Rock             2.00           0.813\nR&gt; 2 Boulders Beach           2.64           0.527\n\n\nLovely. The variances are equal and the data are normal. On to the next step.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#running-an-analysis",
    "href": "basic_stats/07-t_tests.html#running-an-analysis",
    "title": "7. t-Tests",
    "section": "7.6 Running an Analysis",
    "text": "7.6 Running an Analysis\nWith our assumptions checked, we may now analyse our data. We will see below how to do this with both of the functions we have learned in this chapter for comparing means of two sample sets.\n\nt.test(value ~ site, data = ecklonia_sub,\n       var.equal = TRUE, alternative = \"greater\")\n\nR&gt; \nR&gt;  Two Sample t-test\nR&gt; \nR&gt; data:  value by site\nR&gt; t = 1.8741, df = 24, p-value = 0.03657\nR&gt; alternative hypothesis: true difference in means between group Batsata Rock and group Boulders Beach is greater than 0\nR&gt; 95 percent confidence interval:\nR&gt;  0.09752735        Inf\nR&gt; sample estimates:\nR&gt;   mean in group Batsata Rock mean in group Boulders Beach \nR&gt;                     6.116154                     4.996154",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#interpreting-the-results",
    "href": "basic_stats/07-t_tests.html#interpreting-the-results",
    "title": "7. t-Tests",
    "section": "7.7 Interpreting the Results",
    "text": "7.7 Interpreting the Results\nWe may reject the null hypothesis that the stipe mass of kelps at Batsata Rock is not greater than at Boulders Beach if our t-test returns a p-value \\(\\leq\\) 0.05. We must also pay attention to some of the other results from our t-test, specifically the t-value (t) and the degrees of freedom (df) as these are also needed when we are writing up our results. From all of the information above, we may accept the alternative hypothesis. But how do we write that up?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#drawing-conclusions",
    "href": "basic_stats/07-t_tests.html#drawing-conclusions",
    "title": "7. t-Tests",
    "section": "7.8 Drawing Conclusions",
    "text": "7.8 Drawing Conclusions\nThere are many ways to present ones findings. Style, without too much flourish, is encouraged as long as certain necessary pieces of information are provided. The sentence below is a very minimalist example of how one may conclude this mini research project. A more thorough explanation would be desirable.\n\nThe stipe mass (kg) of the kelp Ecklonia maxima was found to be significantly greater at Batsata Rock than at Boulders Beach (p = 0.03, t = 1.87, df = 24).",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/07-t_tests.html#going-further",
    "href": "basic_stats/07-t_tests.html#going-further",
    "title": "7. t-Tests",
    "section": "7.9 Going Further",
    "text": "7.9 Going Further\nBut why though? As is often the case in life, and science is no exception, answers to our questions just create even more questions! Why would the mass of kelp stipes at one locations in the same body of water, only a kilometre or so apart be significantly different? It looks like we are going to need to design a new experiment… Masters thesis anyone?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "7. *t*-Tests"
    ]
  },
  {
    "objectID": "basic_stats/05-inference.html",
    "href": "basic_stats/05-inference.html",
    "title": "5. Statistical Inference",
    "section": "",
    "text": "NoteIn This Chapter\n\n\n\n\nThe concept of inferential statistics\nHypothesis testing\nProbabilities\nAssumptions and parametric statistics\n\nNormality and the Shapiro-Wilk test\nHomoscedasticity\n\n\n\n\n\n\n\n\n\n\nImportantTasks to Complete in This Chapter\n\n\n\n\nNone\n\n\n\n\n1 Introduction\nWe have seen in Chapter 2 and Chapter 3 how to summarise, describe, and visualise our data — these processes form part of descriptive statistics. The next step is the process of conducting inferential statistics.\nInferential statistics is a branch of statistics that focuses on drawing conclusions and making generalisations about a larger population based on the analysis of a smaller, representative sample. This is particularly valuable in research situations where it is impractical, impossible to collect data from every member of a population — i.e. all of biology and ecology. By employing probabilistic reasoning or inferential statistics enable us to estimate population parameters, make predictions, and test hypotheses with a certain level of confidence.\nOne of the key aspects of inferential statistics is the concept of sampling variability. Since samples are only a subset of the population, they imperfectly represent whole populations, leading to variations in the estimates of population parameters (repeatedly drawing samples at random from a population will result in slightly different values for key statistical parameters, such as the sample mean, variance). Inferential statistics accounts for this variability by providing measures of uncertainty such as confidence intervals and margins of error, which convey the range within which the true population parameter is likely to fall.\nParametric statistics form the foundation of inferential statistics, and they are used to make inferences about population parameters based on sample data. These statistics assume that the data are generated from a specific probability distribution — the normal distribution. An alternative to parametric tests is non-parametric statistics, and we shall hear more about it in Chapter 6.\nThe most common parametric statistics used in inferential statistics include:\n\nt-tests (Chapter 7) used to determine if there is a significant difference between the means of two groups of continuous dependent (response) variables.\nANOVA (Chapter 8) used to determine if there is a significant difference between the means of three or more groups of continuous variables.\nRegression analysis (Chapter 9) used to model the relationship between one or more continuous predictor variables and a continuous response variable.\nPearson correlation (Chapter 10) used to measure the linear association or relationships between two continuous variables.\nChi-squared tests used to determine if there is a significant association between two categorical variables.\n\nThese tests typically involve the calculation of a test statistic and the comparison of this value with a critical value and then establishing a p-value to determine whether the results are statistically significant or likely due to chance. These methods are included within a subset of inferential statistics called probabilistic statistics.\n\n\n\n\n\n\nNoteProbabilistic and Bayesian Statistics\n\n\n\nProbabilistic and Bayesian statistics are two related but distinct branches of statistics that offer tools for modelling, analysing, and drawing inferences from complex data sets. At their core, both approaches rely on the use of probability theory to quantify uncertainty, variability in data but they differ in their assumptions about the nature of this uncertainty and how it should be modelled.\nProbabilistic statistics is a classical approach that assumes that all sources of variability in a data set can be described by a fixed set of probability distributions, such as the normal distribution, the Poisson distribution. These distributions are characterised by a set of parameters such as the mean and standard deviation, that can be estimated from the data. Probabilistic statistics is widely used in fields such as biology, physics, and economics, where the data are often assumed to be generated by a deterministic process with some random noise present. In contrast, Bayesian statistics takes a more flexible approach to modelling uncertainty, allowing for uncertainty in both the parameters of the model, the underlying distribution itself. Bayesian methods are useful when dealing with complex and high-dimensional data sets and with lots of unknowns and assumptions, and have become increasingly popular in fields such as ecology, machine learningin recent years.\n\n\n\n\n2 Hypothesis Testing\nHypothesis testing is a fundamental aspect of the scientific method and is used to evaluate the validity of scientific hypotheses. A hypothesis is a proposed explanation for a phenomenon or observation that can be tested through experimentation or observation. To test a hypothesis, we design experiments, collect data or which we analyse using inferential statistical methods to determine whether the data support or refute the hypothesis.\nTwo competing hypotheses about the data are set up at the onset of hypothesis testing: a null hypothesis (H0) and an alternative hypothesis (Ha). The null hypothesis typically represents the status quo or a default assumption (a statement of no difference), while the alternative hypothesis represents a new, alternative explanation for the data.\nThe goal is to make objective and evidence-based conclusions about the validity of the hypothesis, and to determine whether it can be accepted, rejected based on the available evidence. Hypothesis testing is a critical tool for advancing scientific knowledge and understanding or as it allows us to identify the most promising hypotheses and develop more accurate models of the natural world. Effectively, scientific progress can only be made if the null hypothesis is rejected, the alternative hypothesis accepted.\n\n\n\n\n\n\nNoteHypotheses and Theories\n\n\n\nHypotheses and theories are both important components of the scientific process, but they serve different functions, represent distinct levels of understanding.\nA hypothesis is a tentative explanation or proposition for a specific phenomenon, often based on observations, grounded in existing knowledge. It is a testable statement that can be either supported or refuted through further observation and experimentation, and hypothesis testing through the application of inferential statistics. Hypotheses are typically formulated at the beginning of a research study. They guide the design of experiments, the collection of data. Hypotheses help us make predictions and answer specific questions about the phenomena under investigation. If a hypothesis is repeatedly tested and confirmed through various experiments and it may gain credibility and contribute to the development of a theory.\nA theory is a well-substantiated explanation for a broad range of observed phenomena that has been consistently supported by a large body of evidence. Theories are more comprehensive and mature than hypotheses, as they integrate, generalise multiple related hypotheses and empirical findings to explain complex phenomena. They are built upon a solid foundation of tested hypotheses and provide a coherent framework that enables us to make accurate predictions and generate new hypotheses, and further advance our understanding of the natural world.\n\n\nAt the heart of many basic scientific inquiries, and hence hypotheses, is the simple question “Is A different from B?” The scientific notation for this question is:\n\nH0: Group A is not different from Group B\nHa: Group A is different from Group B\n\nMore formally, one would say:\n\n\\(H_{0}: \\bar{A} = \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} \\neq \\bar{B}\\)\n\\(H_{0}: \\bar{A} \\leq \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} &gt; \\bar{B}\\)\n\\(H_{0}: \\bar{A} \\geq \\bar{B}\\) vs. the alternative hypothesis that \\(H_{a}: \\bar{A} &lt; \\bar{B}\\)\n\n\n\n\n\n\n\nNoteNote\n\n\n\nHypothesis 1 is a two-sided t-test and hypotheses 2 and 3 are one-sided tests. This will make sense once you have studied the material in Chapter 7 about t-tests.\n\n\n\n\n3 Probabilities\nThe p-value (the significance level, \\(\\alpha\\)) is the probability of finding the observed (or measured) outcome to be more extreme (i.e., very different) than that suggested by the null hypothesis (\\(H_{0}\\)). Typically, biologists set the p-value at \\(\\alpha \\leq 0.05\\) — in other words, the measured outcome of our experiment only has a 1 in 20 chance of being the same as that of the reference (or control) group. So, when the p-value is \\(\\leq\\) 0.05, for example, we say that there is a very good probability that our experimental treatment resulted in an outcome that is very different (we say statistically significantly different) from the measurement obtained from the group to which the treatment had not been applied — in this case we do not accept \\(H_{0}\\) and by necessity \\(H_{a}\\) becomes true.\n\n\n\n\n\n\nNoteThe \\(H_{0}\\)\n\n\n\nIn inferential statistics, when conducting hypothesis testing, we do not “accept”, “prove” the null hypothesis. Instead or we either “reject” or “fail to reject” the null hypothesis based on the evidence provided by our sample data. So, it does not mean the null hypothesis is true, just that there is not enough evidence in your sample to reject it.\n\n\nThe choice of p-value at which we reject \\(H_{0}\\) is arbitrary and exists by convention only. Traditionally, the 5% cut-off (i.e., less than 1 in 20 chance of being wrong\\(p \\leq 0.05\\)) is used in biology, but sometimes the threshold is set at 1%, 0.1% (0.01 or 0.001 or respectively), particularly in the medical sciences where avoiding false positives, negatives could be a public health concern. However, more and more biologists shy away from the p-value as they argue that it can give a false sense of security.\n\n\nStatistical tests indicate a statistically significant outcome (the \\(p \\leq 0.05\\)) and we accept the \\(H_{a}\\), or it does not (\\(p \\gt 0.05\\)) and we do not reject the \\(H_{0}\\). There is no “almost significant”. It is, or it is not. \nWe generally refer to \\(p \\leq 0.05\\) as being statistically significant. Statistically highly significant is seen at as \\(p \\leq 0.001\\). In the first instance there is a less than 1 in 20 chance that our experimental sample is not different from the reference group, and in the second instance there is a less than 1 in a 1000 chance tat they are the same. This says something about the acceptable error rates: there is a better chance the \\(H_{0}\\) may in fact be falsely accepted or rejected when the p-value is set at 0.05 than at 0.001.\n\n\n\n\n\n\nNoteType I and Type Ii Errors\n\n\n\nA Type I error is the false rejection of the \\(H_{0}\\) hypothesis (i.e., in reality we should not be rejecting it, but the p-value suggests that we must). A Type II error, on the other hand, is the false acceptance of the \\(H_{0}\\) hypothesis (i.e., the p-value suggests we should not reject the \\(H_{0}\\), but in fact we must). When a statistical test results in a p-value of, say, \\(p \\leq 0.05\\) we would conclude that our experimental sample is statistically different from the reference group, but probabilistically there is a 1 in 20 chance that this outcome is incorrect (i.e., the difference was arrived at by random chance only).\nThe choice of p-value threshold depends on several factors, including the nature of the data, the research question, and the desired level of statistical significance. In medical sciences, where the consequences of false positive, false negative results can have significant implications for patient health or a more stringent threshold is often used. A p-value of 0.001 is commonly used in medical research to minimise the risk of Type I errors (rejecting the null hypothesis when it is actually true) and to ensure a high level of statistical confidence in the results.\nIn biological sciences, the consequences of false positive, false negative results may be less severe and a p-value of 0.05 is often considered an appropriate threshold for statistical significance. However, it is important to note that the choice of p-value threshold is ultimately subjective, should be based on a careful consideration of the research question and the nature of the data, and the potential consequences of false positive, false negative results.\n\n\nTo conclude, when \\(p \\gt 0.05\\) there is a lack of compelling evidence to suggest that our experiment has had an influential effect of the hypothesised outcome — even if a graphs hints at differences between groups. When \\(p \\leq 0.05\\), however, there is a good probability that the experiment (etc.) has had an effect, and that the effect is likely not due to random chance. In this case we have a statistically significant finding.\n\n\n4 Assumptions\nIrrespective of the kind of statistical test we wish to perform, we have to make a couple of important assumptions that are not guaranteed to be true. In fact, these assumptions are often violated because real data, especially biological data, are messy.\nThe issue of assumption is an important one, and one that we need to understand well. This is will be the purpose of Chapter 6, where we will learn about how to test the assumptions, and discover what to do when it does.\n\n\n5 Conclusion\nWe use inferential statistics to draw conclusions about a population based on a sample of data. By using probability theory and statistical inference, we can make inferences about the characteristics of a larger population with a certain level of confidence. We must always keep the assumptions behind inferential statistics in mind so that we can apply the right statistical test, answer our research question within the limits of what our data can tell us.\nIn practice, the process works like this:\n\nSetting the significance level (\\(\\alpha\\)):\n\nBefore conducting the test, you decide on a significance level, \\(\\alpha\\), which is the probability of rejecting the null hypothesis when it is actually true (Type I error). Common choices for \\(\\alpha\\) are 0.05, 0.01, and 0.10, though the choice is context-dependent.\n\nConducting the test:\n\nYou then compute the test statistic (like a t-statistic, F-statistic, etc.) based on your sample data.\nThis test statistic is then compared to a distribution (like the t-distribution for the t-test) to find the p-value.\n\nInterpreting the p-value:\n\nThe p-value is the probability of observing a test statistic as extreme as, or more extreme than, the statistic computed from the sample, assuming that the null hypothesis is true.\nIf the p-value is less than \\(\\alpha\\) (i.e., below the critical value), then the evidence suggests that the null hypothesis can be rejected in favour of the alternative hypothesis.\nIf the p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis. This does not mean the null hypothesis is true, just that there is not enough evidence in your sample to reject it.\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {5. {Statistical} {Inference}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/basic_stats/05-inference.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 5. Statistical Inference. http://samos-r.netlify.app/basic_stats/05-inference.html.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "5. Statistical Inference"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-histograms",
    "href": "basic_stats/03-visualise.html#sec-histograms",
    "title": "3. Statistical Figures",
    "section": "2.1 Frequency Distributions",
    "text": "2.1 Frequency Distributions\nFrequency distributions are typically displayed as histograms. Histograms are a type of graph that displays the frequency of occurrences of observations forming a continuous variable. To construct a histogram, the data are divided into intervals, or bins, and the number of occurrences of observations within each bin is tallied. The height of each bar (y-axis) in the histogram represents the number of observations falling within that bin. The x-axis displays the bins, arranged such that the intervals they represent go from small to large on an ordinal scale. The intervals should be chosen such that they best represent the distribution of the data without being too narrow, too wide. Histograms can be used to quickly assess the distribution of the data or identify any skewness or outliers, and provide a visual representation of the central tendency, variation of the data.\nWe have a choice of absolute (Figure 1 A) and relative (Figure 1 B-C) frequency histograms. In absolute frequency distributions, the sum of all the counts per bin will add up to the total number of observations. In relative frequency distributions the frequency of each category is expressed as a proportion, percentage of the total number of observations, and hence the sum of the relative counts per bin is 1. This is useful if two populations being compared have different numbers of observations. There is also the empirical cumulative distribution function (ECDF) (Figure 1 D) that shows the cumulative proportion of observations that fall below or equal to a certain value. See the Old Faithful data, for example. The eruptions last between 1.6 and 5.1 minutes. So, we create intervals of time spanning these times, and within each count the number of times an event lasts as long as denoted by the intervals. Here we might choose intervals of 1-2 minutes, 2-3 minutes, 3-4 minutes, 4-5 minutes, and 5-6 minutes. The ggplot2 geom_histogram() function automatically creates the bins, but we may specify our own. It is best to explain these principles by example (Figure 1 A-D).\n\n# a normal frequency histogram, with count along y\nhist1 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"'Vanilla' histogram\",\n       x = \"Eruption duration (min)\",\n       y = \"Count\") + theme_pubclean()\n\n# when the binwidth is 1, the density histogram *is* the relative\n# frequency histogram\nhist2 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..density..),\n                 position = 'identity', binwidth = 1,\n                 colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Relative frequency\",\n       x = \"Eruption duration (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\n\n# if binwidth is something other than 1, the relative frequency in\n# a histogram is ..density.. * binwidth\nhist3 &lt;- ggplot(data = faithful, aes(x = waiting)) +\n  geom_histogram(aes(y = 0.5 * ..density..),\n                 position = 'identity', binwidth = 0.5,\n                 colour = \"salmon\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Relative frequency\",\n       x = \"Waiting time (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\n# ECDF\nhist4 &lt;- ggplot(data = faithful, aes(x = eruptions)) + \n  stat_ecdf() +\n  labs(title = \"ECDF\",\n       x = \"Eruption duration (min)\",\n       y = \"Relative\\ncontribution\") + theme_pubclean()\n\nggarrange(hist1, hist2, hist3, hist4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\n\n\n\nFigure 1: Example histograms for the Old Faithful data. A) A default frequency histogram with the count of eruption times falling within the specified bins. B) A relative frequency histogram with bins adjusted to a width of 1 minute intervals; here, the sum of counts within each of the four bins is 1. C) Another relative frequency histogram, but with the bins adjusted to each be 0.5 minute increments; again the sum of counts represented by each bin is equal to 1.\n\n\n\n\n\nAs we see above, ggplot2 can automatically construct a frequency histogram with the geom_histogram() function. We can also manually create a frequency distribution with the cut() function.\n\n\n\n\n\n\nImportantDo It Now!\n\n\n\nStarting with the cut() function, recreate Figure 1 A-C manually.\n\n\nWhat if we have continuous data belonging with multiple categories? The iris dataset provides a nice collection of measurements that we may use to demonstrate a grouped frequency histogram. These data are size measurements (cm) of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of three species of Iris. The species are Iris setosa, I. versicolor, and I. virginica. The figures are shown in Figure 2.\n\n# first we make long data\niris.long &lt;- iris %&gt;% \n  gather(key = \"variable\", value = \"size\", -Species)\n\nggplot(data = iris.long, aes(x = size)) +\n  geom_histogram(position = \"dodge\", # omitting this creates a stacked histogram\n                 colour = NA, bins = 20,\n                 aes(fill = Species)) +\n  facet_wrap(~variable) +\n  labs(title = \"Iris data\",\n       subtitle = \"Grouped frequency histogram\",\n       x = \"Size (cm)\",\n       y = \"Count\") +\n  theme_pubclean()\n\n\n\n\n\n\n\nFigure 2: Grouped histograms for the four Iris variables.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-bargraphs",
    "href": "basic_stats/03-visualise.html#sec-bargraphs",
    "title": "3. Statistical Figures",
    "section": "2.2 Bar Graphs",
    "text": "2.2 Bar Graphs\nBar graphs are popular among biologists and ecologists. Often used to represent discrete categories or groups, bar graphs provide a visual representation of mean values for each category, thus allowing us to identify trends, patterns, and differences across data sets, experimental treatments. In complex biological systems such as population dynamics, species abundance, or ecological niches, bar graphs offer a clear, concise way to depict the interactions and variations among different elements. Importantly and bar graphs may also include some indication of variation, such as error bars (a term that also applies when the variation statistic used is the standard deviation), other visual cues to denote the range of variation within the data such as confidence intervals. This additional layer of information not only highlights the variability inherent in biological and ecological data but also aids in the interpretation of results and the overall understanding of the phenomena under investigation. Note that it is not incorrect to plot the median in bar graphs, but bat graphs is typically reserved for displaying the mean. For plotting the median, see Section 2.3, below.\nA naïve application of bar graphs is to indicate the number of observations within several groups. Although this can be presented numerically in tabular form, sometimes one might want to create a bar, pie graph of the number of occurrences in a collection of non-overlapping classes or categories. Both the data and graphical displays will be demonstrated here.\nThe first case is of a variation of frequency distribution histograms, but here showing the raw counts per each of the categories that are represented in the data — unlike ‘true’ frequency histograms in Section 2.1 that divide data into bins, this one takes a cruder approach. The count within each of the categories sums to the sample size, \\(n\\). In the second case, we may want to report those data as proportions. Here we show the frequency proportion in a collection of non-overlapping categories. For example, we have a sample size of 12 (\\(n=12\\)). In this sample, two are coloured blue, six red, and five purple. The relative proportions are \\(2/12=0.1666667\\) blue, \\(6/12=0.5\\) red,\\(5/12=0.4166667\\) purple. The important thing to note here is that the relative proportions sum to 1, i.e. \\(0.1666667+0.5+0.4166667=1\\). These data may be presented as a table or as a graph.\nIn Figure 3 I demonstrate the numerical and graphical summaries using the built-in iris data (I would not do this in real life, it is silly; just write it out in the text of the Methods section):\n\n# the numerical summary produced by a piped series of functions;\n# create a summary of the data (i.e. number of replicates per species)\n# used for (A), (B) and (C), below\niris.cnt &lt;- iris %&gt;%\n  count(Species) %&gt;% # automagically creates a column, n, with the counts\n  mutate(prop = n / sum(n)) # creates the relative proportion of each species\n\n\n# a stacked bar graph with the cumulative sum of observations\nplt1 &lt;- ggplot(data = iris.cnt, aes(x = \"\", y = n, fill = Species)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  labs(title = \"Stacked bar graph\", subtitle = \"cumulative sum\",\n       x = NULL, y = \"Count\") +\n  theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\n# a stacked bar graph with the relative proportions of observations\nplt2 &lt;- ggplot(data = iris.cnt, aes(x = \"\", y = prop, fill = Species)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  scale_y_continuous(breaks = c(0.00, 0.33, 0.66, 1.00)) +\n  labs(title = \"Stacked bar graph\", subtitle = \"relative proportions\",\n       x = NULL, y = \"Proportion\") +\n  theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\n# a basic pie chart\nplt3 &lt;- plt1 + coord_polar(\"y\", start = 0) +\n  labs(title = \"Friends do not let...\", subtitle = \"...friends make pie charts\",\n       x = NULL, y = NULL) +\n  scale_fill_brewer(palette = \"Blues\") +\n  theme_minimal()\n# if you seriously want a pie chart, rather use the base R function, `pie()`\n\n# here now a bar graph...\n# the default mapping of `geom_bar` is `stat = count`, which is a\n# bar for each of the categories (`Species`), with `count` along y\nplt4 &lt;- ggplot(data = iris, aes(x = Species, fill = Species)) +\n  geom_bar(show.legend = FALSE) +\n  labs(title = \"Side-by-side bars\", subtitle = \"n per species\", y = \"Count\") +\n theme_pubclean() + scale_color_few() +\n  scale_fill_few()\n\nggarrange(plt1, plt2, plt3, plt4, nrow = 2, ncol = 2, labels = \"AUTO\")\n\n\n\n\n\n\n\nFigure 3: Examples of histograms for the built-in Iris data. A) A default frequency histogram showing the count of samples for each of the three species. B) A relative frequency histogram of the same data; here, the sum of counts of samples available for each of the three species is 1. C) A boring pie chart. D) A frequency histogram of raw data counts shown as a series of side-by-side bars.\n\n\n\n\n\nNow I will demonstrate more realistic bar graphs. We stay with the iris data (Figure 4):\n\niris |&gt;\n  pivot_longer(cols = Sepal.Length:Petal.Width,\n               names_to = \"variable\",\n               values_to = \"size\") |&gt; \n  group_by(Species, variable) |&gt; \n  summarise(mean = round(mean(size), 1),\n            sd = round(sd(size), 1), .groups = \"drop\") |&gt; \n  ggplot(aes(x = Species, y = mean)) +\n  geom_bar(position = position_dodge(), stat = \"identity\", \n           col = \"black\", fill = \"salmon\", alpha = 0.4) +\n  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),\n                width = .2) +\n  facet_wrap(~variable,\n             scales = \"free\") +\n  ylab(\"Size (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 4: Bar graphs indicating the mean size (± SD) for various flower features of three species of Iris.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-boxplots",
    "href": "basic_stats/03-visualise.html#sec-boxplots",
    "title": "3. Statistical Figures",
    "section": "2.3 Box Plots",
    "text": "2.3 Box Plots\nA box plot provides a graphical summary of the distribution of data. They allow us to compare the medians, quartiles, and ranges of the data for multiple groups, and identify any differences, similarities in the distributions. For example, box plots can be used to compare the body size distributions of different species, or to compare the reproductive output of different populations. Additionally, box plots can be used to identify outliers, other anomalies in the data and which may be indicative of underlying ecological processes or environmental factors.\nBox plots plots are traditionally used to display data that are not normally distributed, but I like to use them for any old data, even normal data. I prefer these over the old-fashioned bar graphs (seen in Section 2.2). As a variation of the basic box-and-whisker plot, I also quite like to superimpose a jittered scatter plot of the raw data on each bar.\nI create a simple example using the msleep dataset (Figure 5). Additional examples are provided in Chapter 2.\n\nmsleep |&gt; \n  ggplot(aes(x = vore, y = sleep_total)) + \n  geom_boxplot(colour = \"black\", fill = \"salmon\", alpha = 0.4,\n               outlier.colour = \"red3\", outlier.fill = \"red\",\n               outlier.alpha = 1.0, outlier.size = 2.2) +\n  geom_jitter(width = 0.10, fill = \"blue\", alpha = 0.5,\n              col = \"navy\", shape = 21, size = 2.2) +\n  labs(x = \"'-vore'\",\n       y = \"Sleep duration (hr)\") +\n  theme_pubclean()\n\n\n\n\n\n\n\nFigure 5: Box-plot summarising the amount of sleep required by different ‘vores’.\n\n\n\n\n\nBox plots are sometimes called box-and-whisker plots. The keen eye can glance the ‘shape’ of the data distribution; they provide an alternative view to that given by the frequency distribution. There is a lot of information in these graphs, so let us see what is there. From the geom_boxplot documentation, which says it best (type ?geom_boxplot):\n\n“The lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles).”\n“The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first, third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called ‘outlying’ points and are plotted individually.”\n“In a notched box plot, the notches extend 1.58 * IQR / sqrt(n). This gives a roughly 95% confidence interval for comparing medians.”\n\nHere be more examples (Figure 6), this time of notched box plots:\n\nlibrary(ggsci) # for nice colours\n\nggplot(data = iris.long, aes(x = Species, y = size)) +\n  geom_boxplot(alpha = 0.4, notch = TRUE) +\n  geom_jitter(width = 0.1, shape = 21, fill = NA,\n              alpha = 0.4, aes(colour = as.factor(Species))) +\n  facet_wrap(~variable, nrow = 2) +\n  scale_color_npg() +\n  labs(y = \"Size (cm)\") +\n  guides(colour = FALSE) +\n  theme(axis.text.x = element_text(face = \"italic\"))\n\n\n\n\n\n\n\nFigure 6: A panelled collection of box plots, one for each of the four variables, with a scatterplot to indicate the spread of the raw data points.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-densityplots",
    "href": "basic_stats/03-visualise.html#sec-densityplots",
    "title": "3. Statistical Figures",
    "section": "2.4 Density Plots",
    "text": "2.4 Density Plots\nOften when we are displaying a distribution of data we are interested in the ‘shape’ of the data more than the actual count of values in a specific category, as shown by a standard histogram. When one wishes to more organically visualise the frequency of values in a sample set a density graphs is used. These may also be thought of as smooth histograms. These work well with histograms, rug plots and as we may see in the figure below. It is important to note with density plots that they show the relative density of the distribution along the \\(y\\)-axis, and not the counts of the data. This can of course be changed, as seen below, but is not the default setting. Sometimes it can be informative to see how different the count, density distributions appear.\nFigure 7 shows examples af density plots:\n\n# a normal density plot\ndens1 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"A vanilla density plot\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a density and rug plot combo\ndens2 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  geom_rug(colour = \"red\") +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"A density and rug plot\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a relative frequency histogram overlaid with a density plot\ndens3 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..density..),\n                 position = 'identity', binwidth = 1,\n                 colour = \"black\", fill = \"turquoise\", alpha = 0.6) +\n  geom_density(colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"Relative frequency with density\",\n       x = \"Eruption duration (min)\",\n       y = \"Density\") + theme_pubr()\n\n# a normal frequency histogram with density overlaid\n# note that the density curve must be adjusted by\n# the number of data points times the bin width\ndens4 &lt;- ggplot(data = faithful, aes(x = eruptions)) +\n  geom_histogram(aes(y = ..count..),\n                 binwidth = 0.2, colour = \"black\", fill = \"turquoise\", alpha = 0.6) +\n  geom_density(aes(y = ..density.. * nrow(datasets::faithful) * 0.2), position = \"identity\",\n               colour = \"black\", fill = \"salmon\", alpha = 0.6) +\n  labs(title = \"Old Faithful data\",\n       subtitle = \"Frequency with density\",\n       x = \"Eruption duration (min)\",\n       y = \"Count\") + theme_pubr()\n\nggarrange(dens1, dens2, dens3, dens4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\n\n\n\nFigure 7: A bevy of density graphs option based on the iris data. A) A lone density graph. B) A density graph accompanied by a rug plot. C) A histogram with a density graph overlay. D) A ridge plot.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-violinplots",
    "href": "basic_stats/03-visualise.html#sec-violinplots",
    "title": "3. Statistical Figures",
    "section": "2.5 Violin Plots",
    "text": "2.5 Violin Plots\nWe may combine the box plot and density graph concepts into a new figure type. They can become quite snooty and display more information in more informative ways than vanilla box plots. These are known as violin plots and are very useful when we want to show the distribution of multiple categories of a continuous variate alongside one another.\nViolin plots show the same information as box plots but take things one step further by allowing the shape of the box plot to also show the distribution of the continuous data within the sample sets. They show not only central tendencies (like median) but also the full distribution, including possible multimodal, skewed characteristics.\nOne needs to install additional packages to make then, such as the package ggstatplot. This package offers many non-traditional options for graphical statistical summaries. Here, the violin plot includes the following features:\n\nViolins The vertical, symmetrical, and mirrored shapes represent the estimated probability density of the data at different values. The wider the violin at a given point, the higher the density of data at that value.\nBox plot A box plot can be embedded within the violin plot to show the median, quartiles, and the possible outliers.\nStatistical annotations The violin plots offered by ggstatplot accommodate various statistical annotations such as mean, median, confidence intervals, or p-values, depending on the your needs.\n\nWe will use the iris data below to highlight the different types of violin plots one may use (Figure 8):\n\nlibrary(ggstatsplot)\nset.seed(123) # for reproducibility\n\n# plot\nggstatsplot::ggbetweenstats(\n  data = iris,\n  x = Species,\n  y = Sepal.Length,\n  ylab = \"Sepal length (cm)\",\n  title = \"Distribution of sepal length across the three *Iris* species\"\n)\n\n\n\n\n\n\n\nFigure 8: Examples of violin plots made for the Iris data.\n\n\n\n\n\nHere is another version of the iris data analysed with violin plots (Figure 9):\n\nvio1 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_violin() + \n  labs(title = \"Iris data\",\n       subtitle = \"Basic violin plot\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# A violin plot showing the quartiles as lines\nvio2 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_violin(show.legend = FALSE, draw_quantiles = c(0.25, 0.5, 0.75)) + \n  labs(title = \"Iris data\",\n       subtitle = \"Violin plot with quartiles\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# Box plots nested within violin plots\nvio3 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, colour = Species)) +\n  geom_violin(fill = \"grey70\") + \n  geom_boxplot(width = 0.1, colour = \"grey30\", fill = \"white\") +\n  labs(title = \"Iris data\",\n       subtitle = \"Box plots nested within violin plots\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\n# Boxes in violins with the raw data jittered about\nvio4 &lt;- ggplot(data = iris, aes(x = Species, y = Sepal.Length, colour = Species)) +\n  geom_violin(fill = \"grey70\") + \n  geom_boxplot(width = 0.1, colour = \"black\", fill = \"white\") +\n  geom_jitter(shape = 1, width = 0.1, colour = \"red\", alpha = 0.7, fill = NA) +\n  labs(title = \"Iris data\",\n       subtitle = \"Violins, boxes, and jittered data\", y = \"Sepal length (cm)\") +\n  theme_pubr() +\n  theme(axis.text.x = element_text(face = \"italic\"),\n        legend.position = \"none\")\n\nggarrange(vio1, vio2, vio3, vio4, ncol = 2, nrow = 2, labels = \"AUTO\")\n\n\n\n\n\n\n\nFigure 9: More variations of violin plots applied to the Iris data.\n\n\n\n\n\nThe ggpubr package also provides many convenience functions for the drawing of publication quality graphs, including violin plots.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-scatterplots",
    "href": "basic_stats/03-visualise.html#sec-scatterplots",
    "title": "3. Statistical Figures",
    "section": "3.1 Scatter Plots",
    "text": "3.1 Scatter Plots\nThe relationship between two continuous variables is typically displayed with scatter plots. In a scatter plot, each data point is represented by a dot, other symbol plotted on a Cartesian coordinate system or with one variable mapped to the \\(x\\)-axis and the other to the \\(y\\)-axis. One may choose to fit a best fit line through these points, but displaying the scatter of points is typically enough. In scatter plots, the points are not connected by lines, and the use of discrete points causes us to not assume a specific order, continuity in the data between ‘consecutive’ points on the graph. Also or a scatter plot typically does not require that the \\(x\\)-axis is independent.\nThe most basic use of scatter plots is the following:\n\nExploratory data analysis Scatter plots are useful in the initial exploration of data sets. They help us identify patterns and relationships that might warrant further investigation using more advanced statistical techniques.\nIdentifying trends One can identify whether there is a positive, negative, or no apparent trend between the two variables by observing the overall pattern (slope) of an imaginary, real line fitted to the data points. The detection of trends is something we will encounter in Chapter 9 on Simple linear regressions.\nIdentifying correlations Scatter plots can be used to visually assess the correlation between two variables. A strong positive correlation will result in data points forming a line or curve sloping upward, while a strong negative correlation will result in data points forming a line, curve sloping downward. A weak or no correlation will result in a more scattered and less structured pattern. We will discover more about this in Chapter 10 on Correlation.\nAssessing clustering Scatter plots can reveal natural groupings or clusters of data points, which can be helpful in understanding the structure of the data, identifying potential subgroups for further analysis.\n\nAll of these applications of scatter plots are shown in Figure 10. In Figure 10 I show the relationship between two (matched) continuous variables. The statistical strength of the relationship can be indicated by a correlation (no causal relationship implied as is the case here) or a regression (when a causal link of \\(x\\) on \\(y\\) is demonstrated), and the grouping structure is clearly indicated with colour.\n\nplt1 &lt;- ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +\n  geom_point() +\n  labs(x = \"Petal length (cm)\", y = \"Petal width (cm)\") +\n  theme(legend.position = c(0.22, 0.75)) +\n  scale_color_fivethirtyeight() +\n  scale_fill_fivethirtyeight() +\n  theme_minimal()\n\nplt2 &lt;- ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, show.legend = FALSE) +\n  scale_color_fivethirtyeight() +\n  scale_fill_fivethirtyeight() +\n  labs(x = \"Petal length (cm)\", y = \"Petal width (cm)\") +\n  theme_minimal()\n\nggarrange(plt1, plt2, ncol = 2, nrow = 1, labels = \"AUTO\",\n          common.legend = TRUE)\n\n\n\n\n\n\n\nFigure 10: Examples of scatterplots made for the Iris data. A) A default scatter plot showing the relationship between petal length and width. B) The same as (A) but with a correlation line added.\n\n\n\n\n\nScatter plots may also indicate some of the following properties of our datasets, which make them useful as a diagnostic tool in inferential data analysis, specifically when it comes to assessing assumptions about our data:\n\nDetecting outliers Outliers are data points that deviate significantly from the overall pattern of the data. Scatter plots can help identify such points that might warrant further investigation or indicate problems in data collection.\nAssessing linearity Scatter plots can reveal whether the relationship between two variables is linear or nonlinear. A linear relationship will result in data points forming a straight line, while a nonlinear relationship will result in data points forming a curve, more complex pattern.\n\nWe will encounter these uses in later Chapters dealing with inferential statistics.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-linegraph",
    "href": "basic_stats/03-visualise.html#sec-linegraph",
    "title": "3. Statistical Figures",
    "section": "3.2 Line Graphs",
    "text": "3.2 Line Graphs\nA line graph connects data points with lines, typically emphasising a continuous relationship, a sequence over time or some other continuous scale. The\\(x\\)-axis often represents time (or another independent variable), while the \\(y\\)-axis represents the other variable (usually the dependent variable). Line graphs are particularly useful for tracking changes, trends, or patterns over time, for comparing multiple data series. They suggest a more explicit connection between data points and making it easier to observe fluctuations and the overall direction of the data.\nWe typically encounter line graphs in visual displays of time-series. One might include a point for each observation in time, but it may be omitted. The important thing to note is that a line connects each consecutive observation to the next, indicating the continuity of time. It is a useful tool for exploring trends, patterns, and seasonality in data. For example, a time-series plot can be used to visualise the seasonal trends in temperature over an annual cycle (Figure 11). In this example, points are not used at all, and I opt instead for a stepped line that suggests continuity, yet maintain a ‘discrete’ measure per month (i.e. ignoring the higher frequency daily and finer scale variations within a month).\n\nlibrary(lubridate)\nread_csv(here::here(\"data\", \"BCB744\", \"SACTN_SAWS.csv\")) |&gt; \n  mutate(month = month(date)) |&gt; \n  group_by(site, month) |&gt; \n  dplyr::summarise(temp = mean(temp, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = month, y = temp)) +\n  geom_step(colour = \"red4\") +\n  scale_x_continuous(breaks = c(1, 3, 5, 7, 9, 11)) +\n  xlab(\"Month\") + ylab(\"Temperature (°C)\") +\n  facet_wrap(~site, ncol = 5) +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 11: A time series plot showing the monthly climatology for several sites around South Africa. The specific kind of line drawn here forms a stepped graph.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/03-visualise.html#sec-heatmaps",
    "href": "basic_stats/03-visualise.html#sec-heatmaps",
    "title": "3. Statistical Figures",
    "section": "3.3 Heatmaps and Hovmöller Diagrams",
    "text": "3.3 Heatmaps and Hovmöller Diagrams\nWe can extend the time series line graph to two dimensions. A heatmap is a raster representation of data where the values in a matrix are represented as colours. We will see some heatmaps in Chapter 10 on Correlations. A special kind of heatmap is a calendar heatmap, which is a visualisation technique that uses a calendar layout to show patterns in data over time. For example, a calendar heatmap can be used to show the daily time series, climatologies of temperature or some other environmental variable that varies seasonally (Figure 12).\n\n# Load the function to the local through Paul Bleicher's GitHub page\nsource(\"https://raw.githubusercontent.com/iascchen/VisHealth/master/R/calendarHeat.R\")\n\ntemps &lt;- heatwaveR::sst_WA |&gt; \n  filter(t &gt;= \"2010-01-01\" & t &lt;= \"2019-12-31\") |&gt; \n  mutate(weekday = wday(t),\n         weekday_f = wday(t, label = TRUE),\n         week = week(t),\n         month = month(t, label = TRUE),\n         year = year(t)) |&gt; \n  group_by(year, month) |&gt; \n  mutate(monthweek = 1 + week - min(week))\n\nggplot(temps, aes(monthweek, weekday_f, fill = temp)) + \n  geom_tile(colour = \"white\") +\n  facet_grid(year(t) ~ month) +\n  scale_x_continuous(breaks = c(1, 3, 5)) +\n  scale_y_discrete(breaks = c(\"Sun\", \"Wed\", \"Sat\")) +\n  scale_fill_viridis_c() +\n  xlab(\"Week of Month\") +\n  ylab(\"\") +\n  ggtitle(\"Time-Series Calendar Heatmap: Western Australia SST\") +\n  labs(fill = \"[°C]\")\n\n\n\n\n\n\n\nFigure 12: A calendar heatmap showing a timeseries of SST for Western Australia. The infamous marine heatwave that resulted in a new field of study on extreme temperatures can be seen in the summer of 2011.\n\n\n\n\n\nA special kind of heatmap is used in Ocean and Atmospheric Science is the Hovmöller Diagram (see Figure 13), where we have one continuous spatial covariate along one axis (e.g. latitude, longitude) and time along the other axis on a two-dimensional graph. These diagrams were originally developed by Swedish meteorologist Ernest Hovmöller. By mapping oceanographic variables such as sea surface temperature or salinity, or ocean currents, Hovmöller Diagrams allow us to track the progression of phenomena like El Niño, La Niña events or to examine the migration of ocean eddies and gyres.\nA variation of Hovmöller Diagrams is the horizon plot (Figure 14), which shows the same kind of information (and more) but in a more visually impactful format, in my opinion. I provide more information on horizon plots in my vignette, where I also demonstrate their application to the visualisation of extreme temperature events.\n\nlibrary(data.table)\nlibrary(colorspace)\n\nNWA &lt;- fread(here::here(\"data\", \"BCB744\", \"NWA_Hovmoller.csv\"))\n\n# calculate anomalies\nNWA |&gt; \n  mutate(anom = zonal_sst - mean(zonal_sst)) |&gt; \n  ggplot(aes(x = t, y = lat, fill = anom)) +\n  geom_tile(colour = \"transparent\") +\n  scale_fill_binned_diverging(palette = \"Blue-Red 3\", n_interp = 21) +\n  # scale_fill_viridis_c() +\n  xlab(\"\") + ylab(\"Latitude [°N]\") + labs(fill = \"[°C]\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 13: A Hovmöller Diagram of zonally averaged SST for a region off Northwest Africa in the Canary upwelling system. A variation of this figure appears in the vignette and shows the timeline of marine heatwaves and cold spells in the region.\n\n\n\n\n\n\nlibrary(ggHoriPlot)\n\ncutpoints &lt;- NWA  %&gt;% \n  mutate(\n    outlier = between(\n      zonal_sst, \n      quantile(zonal_sst, 0.25, na.rm = TRUE)-\n        1.5*IQR(zonal_sst, na.rm = TRUE),\n      quantile(zonal_sst, 0.75, na.rm = TRUE)+\n        1.5*IQR(zonal_sst, na.rm=TRUE))) %&gt;% \n  filter(outlier)\n\n# The origin\nori &lt;- round(sum(range(cutpoints$zonal_sst))/2, 2)\n\n# The horizon scale cutpoints\nsca &lt;- round(seq(range(cutpoints$zonal_sst)[1], \n                 range(cutpoints$zonal_sst)[2], \n                 length.out = 7)[-4], 2)\n\nNWA %&gt;% ggplot() +\n  geom_horizon(aes(t,\n                   zonal_sst,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = TRUE) +\n  facet_grid(lat~.) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand = c(0,0),\n               date_breaks = \"1 year\",\n               date_labels = \"%Y\") +\n  xlab('Year') +\n  ggtitle('Canary current system zonal SST')\n\n\n\n\n\n\n\nFigure 14: Zonally average time series of SST in the Canary current system displayed as a horizon plot.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "3. Statistical Figures"
    ]
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html",
    "href": "basic_stats/01-scientific-philosophy.html",
    "title": "1. Philosophy of Science",
    "section": "",
    "text": "Today, we are going to delve into the concept of hypothesis testing. We will look at its foundations, its uses in various disciplines, and the ongoing debates about its role in the greater scheme of scientific knowledge creation. To start, let us go back to a key figure in the philosophy of science…"
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#parametric-statistics",
    "href": "basic_stats/01-scientific-philosophy.html#parametric-statistics",
    "title": "1. Philosophy of Science",
    "section": "3.1 Parametric Statistics",
    "text": "3.1 Parametric Statistics\nParametric statistics rely on certain distributional assumptions — most commonly, the normal distribution — and serve as a foundation in quantitative analysis across many scientific disciplines. They facilitate the extraction of precise statistical properties, parameter estimates and making them particularly powerful for testing hypotheses and deriving inferences in controlled experimental setups or data derived from systematic, structured sampling campaigns. In cases where data do not conform to the assumed distributions, we must employ data transformations, leverage non-parametric methods that do not require specific distributional assumptions."
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#non-parametric-statistics",
    "href": "basic_stats/01-scientific-philosophy.html#non-parametric-statistics",
    "title": "1. Philosophy of Science",
    "section": "3.2 Non-parametric Statistics",
    "text": "3.2 Non-parametric Statistics\nWhile parametric statistics offer significant advantages in terms of precision and power, their applicability across various scientific disciplines necessitates a thorough understanding of their assumptions, limitations. Non-parametric statistics are inherently more flexible and do not depend on restrictive assumptions about the nature of our data — as such and they can accommodate non-normal data, skewed data, or data measured on an ordinal scale. They focus instead on ranks, medians rather than mean values and provide a means to conduct robust set of statistical inference tests on a far wider range of data types.\nThere are a few trade-offs we need to know about when opting for non-rarametric approaches. This includes the potential loss in statistical power and the nuances of interpreting rank-based or median-based results as opposed to mean values. Nevertheless, non-parametric statistics are a critical component of our toolbox across disciplines such as taxonomy, systematics, organismal biology, ecology, socio-ecological studies, and Earth sciences."
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#multivariate-analyses",
    "href": "basic_stats/01-scientific-philosophy.html#multivariate-analyses",
    "title": "1. Philosophy of Science",
    "section": "3.3 Multivariate Analyses",
    "text": "3.3 Multivariate Analyses\nEcologists consider questions about the complex interactions between the biotic and abiotic world. Often they work across multiple spatial and temporal scales. Multivariate analyses such as cluster analysis and ordination are powerful exploratory tools. They untangle ecological datasets to discern patterns and relationships among multiple variables — be it species abundance across different habitats, environmental gradients, or the dynamical properties of ecosystems. Cluster analysis groups similar entities based on their characteristics, reveals natural groupings within the data. Ordination and on the other hand, reduces multidimensional space, making it easier to visualise, interpret complex ecological relationships. In contrast to the more traditional parametric and non-parametric statistics and which often focus on testing hypotheses about the relationships between variables, multivariate analyses provide a more overarching view. They allow us to uncover hidden structures, gradients in the data without a priori hypotheses. While parametric methods hinge on assumptions about data distribution and non-parametric methods offer flexibility in handling data that do not meet these assumptions and multivariate analyses surpass these by focusing on the ecosystem’s interconnectedness and the patterns emerging from these connections."
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#bayesian-methods",
    "href": "basic_stats/01-scientific-philosophy.html#bayesian-methods",
    "title": "1. Philosophy of Science",
    "section": "3.4 Bayesian Methods",
    "text": "3.4 Bayesian Methods\nBayesian methods offer a distinct perspective within the statistical toolbox, allowing us to formally incorporate prior knowledge into our data analysis. Unlike traditional frequentist statistics, which focus solely on the observed data, Bayesian approaches let us blend in existing beliefs, information and then update those beliefs as new evidence comes in. This emphasis on continuously refining our understanding or rather than just finding a single best-fit hypothesis given the data, makes Bayesian methods powerful in scientific fields where we have substantial background knowledge but still need to carefully quantify uncertainty. Bayesian methods are particularly useful in fields like ecology, phyologenetics or where prior knowledge about species interactions or relatiosnhips, environmental conditions, or ecosystem dynamics can be leveraged to make more informed inferences. They also provide a natural framework for decision-making under uncertainty, allowing us to quantify the risks, benefits of different courses of action.\nThe downside of Bayesian analyses is that they can be computationally intensive, especially when dealing with complex models, large datasets. They also require careful consideration of the prior distributions or which can introduce subjectivity into the analysis. But as computational resources continue to expand and methodologies evolve, Bayesian approaches are likely to play an increasingly prominent role in advancing our understanding of the natural world."
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#taxonomy-systematics-and-phylogenetics",
    "href": "basic_stats/01-scientific-philosophy.html#taxonomy-systematics-and-phylogenetics",
    "title": "1. Philosophy of Science",
    "section": "3.5 Taxonomy, Systematics, and Phylogenetics",
    "text": "3.5 Taxonomy, Systematics, and Phylogenetics\nPhylogenetic analysis, while grounded in data, statistical principles and operates within a distinct framework from traditional parametric, non-parametric, or multivariate methods. Its primary goal is to infer evolutionary relationships, patterns of change and rather than classical hypothesis testing. Phylogenetics explicitly models the interconnectedness of evolutionary lineages, a stark contrast to the assumption of independent data points often found in other statistical approaches. While multivariate analyses help examine complex interactions among multiple variables, phylogenetics focuses on how those variables (traits, genes) have evolved across a branching or tree-like structure. Bayesian statistics offer a powerful tool within phylogenetics, aiding in the estimation of probabilities for different evolutionary histories. Yet, the core of phylogenetics lies in specialised algorithms, models designed to reconstruct these evolutionary narratives.\nPhylogenetic analysis is deeply intertwined with systematics and taxonomy, disciplines that seek to understand, classify the diversity of life. Systematics broadly encompasses the study of organismal relationships and while taxonomy focuses on the practice of naming and classification. Phylogenetics serves as a powerful tool within the systematics toolbox, using data to infer evolutionary patterns, inform classification decisions. While statistical methods like parametric and non-parametric tests are used in systematics and taxonomy (e.g. and for comparing morphological traits), much of the analytical toolkit centres on techniques specifically designed for evolutionary data. These techniques include methods for building phylogenetic trees, assessing congruence between different data sources (like genes, morphology) and interpreting patterns of diversification over time."
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#artificial-intelligence-and-machine-learning",
    "href": "basic_stats/01-scientific-philosophy.html#artificial-intelligence-and-machine-learning",
    "title": "1. Philosophy of Science",
    "section": "3.6 Artificial Intelligence and Machine Learning",
    "text": "3.6 Artificial Intelligence and Machine Learning"
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#models-and-simulations",
    "href": "basic_stats/01-scientific-philosophy.html#models-and-simulations",
    "title": "1. Philosophy of Science",
    "section": "3.7 Models and Simulations",
    "text": "3.7 Models and Simulations"
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#qualitative-analysis",
    "href": "basic_stats/01-scientific-philosophy.html#qualitative-analysis",
    "title": "1. Philosophy of Science",
    "section": "3.8 Qualitative Analysis",
    "text": "3.8 Qualitative Analysis"
  },
  {
    "objectID": "basic_stats/01-scientific-philosophy.html#phylogenetic-analysis",
    "href": "basic_stats/01-scientific-philosophy.html#phylogenetic-analysis",
    "title": "1. Philosophy of Science",
    "section": "3.9 Phylogenetic Analysis",
    "text": "3.9 Phylogenetic Analysis\n\nNon-Parametric Statistics:Free us from strict distributional assumptions, great in areas like taxonomy where data might violate parametric rules.\nMultivariate Analyses: Let us tackle ecological complexity where multiple factors interweave with messy, non-linear outcomes.\nBayesian Statistics: Update our beliefs based on evidence, valuable where prior knowledge exists, data is uncertain.\nAI and Machine Learning: Data-driven patterns and prediction,a powerful addition to the hypothesis-testing arsenal.\nModels and Simulations: Allow us to explore complex systems and make predictions, vital in fields like oceanography.\nQualitative Analysis: Socio-ecological studies benefit from in-depth exploration of human attitudes and actions, where quantification may not tell the full story.\nPhylogenetic Analysis: Data-driven exploration of evolutionary relationships, less about statistical tests, more about algorithms and inference."
  },
  {
    "objectID": "BCB743/BCB743_index.html",
    "href": "BCB743/BCB743_index.html",
    "title": "BCB743 (External Module)",
    "section": "",
    "text": "BCB743 lives in the main Tangled Bank site. This is a stub to keep internal links valid.\n\n\n\nCitationBibTeX citation:@online{a._j.,\n  author = {A. J. , Smit},\n  title = {BCB743 {(External} {Module)}},\n  url = {http://samos-r.netlify.app/BCB743/BCB743_index.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S BCB743 (External Module). http://samos-r.netlify.app/BCB743/BCB743_index.html."
  },
  {
    "objectID": "basic_stats/02-summarise-and-describe.html#measures-of-central-tendency",
    "href": "basic_stats/02-summarise-and-describe.html#measures-of-central-tendency",
    "title": "2. Data Summaries & Descriptions",
    "section": "4.1 Measures of Central Tendency",
    "text": "4.1 Measures of Central Tendency\nStatistic | Function | Package |\n|: — — — -|: — — — — -|: — — — -| | Mean | mean() | base | | Median | median() | base | | Mode | Do it! | | | Skewness | skewness() | e1071 | | Kurtosis | kurtosis() | e1071 |\nCentral tendency is a fundamental concept in statistics, referring to the central, typical value of a dataset that best represents its overall distribution. The measures of central tendency are also sometimes called ‘location’ statistics. As a key component of descriptive statistics or central tendency is essential for summarising and simplifying complex data. It provides a single representative value that captures the data’s general behaviour and which might tell us something about the bigger population from which the random samples were drawn.\nA thorough assessment of the central tendency in EDA serves several purposes:\n\nSummary of data Measures of central tendency, such as the mean, median, and mode, provide a single value that represents the centre, typical value of a dataset. They help summarise the data and allow us to gain an early insight into the dataset’s general properties and behaviour.\nComparing groups or distributions Central tendency measures allow us to compare different datasets or groups within a dataset. They can help identify differences or similarities in the data. This may be useful for hypothesis testing and inferential statistics.\nData transformation decisions Understanding the central tendency of our data can inform decisions on whether to apply transformations to the data to better meet the assumptions of certain statistical tests or improve the interpretability of the results.\nIdentifying potential issues Examining the central tendency can help reveal issues with the data, such as outliers, data entry errors or that could influence the results of inferential statistics. Outliers, for example, can greatly impact the mean, making the median a more robust measure of central tendency in such cases.\n\nUnderstanding the central tendency informs the choice of inferential statistics in the following ways:\n\nAssumptions of statistical tests Many inferential statistical tests have assumptions about the distribution of the data, such as normality, linearity, or homoscedasticity. Analysing the central tendency helps assess whether these assumptions are met, informs the choice of an appropriate test.\nChoice of statistical models The central tendency can influence the choice of statistical models or the selection of dependent and independent variables in regression analyses — certain models or relationships may be more appropriate depending on the data’s distribution and central tendencies.\nChoice of estimators Central tendency measures can influence our choice of estimators for further inferential statistics, depending on the data’s distribution, presence of outliers (e.g. and mean vs. median).\n\nBefore I discuss each central tendency statistic, I will generate some random data to represent normal, skewed distributions. I will use these data in my discussions and below.\n\n# Generate random data from a normal distribution\nset.seed(666)\nn &lt;- 5000 # Number of data points\nmean &lt;- 0\nsd &lt;- 1\nnormal_data &lt;- rnorm(n, mean, sd)\n\n# Generate random data from a slightly\n# right-skewed beta distribution\nalpha &lt;- 2\nbeta &lt;- 5\nright_skewed_data &lt;- rbeta(n, alpha, beta)\n\n# Generate random data from a slightly\n# left-skewed beta distribution\nalpha &lt;- 5\nbeta &lt;- 2\nleft_skewed_data &lt;- rbeta(n, alpha, beta)\n\n# Generate random data with a bimodal distribution\nmean1 &lt;- 0\nmean2 &lt;- 10\nsd1 &lt;- 3\nsd2 &lt;- 4\n\n# Generate data from two normal distributions\ndata1 &lt;- rnorm(n, mean1, sd1)\ndata2 &lt;- rnorm(n, mean2, sd2)\n\n# Combine the data from both distributions to\n# create a bimodal distribution\nbimodal_data &lt;- c(data1, data2)\n\n\n# Set up a three-panel plot layout\nold_par &lt;- par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))\n\n# Plot the histogram of the normal distribution\nhist(normal_data, main = \"Normal Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightblue\", border = \"black\")\n\n# Plot the histogram of the right-skewed distribution\nhist(right_skewed_data, main = \"Right-Skewed Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightgreen\", border = \"black\")\n\n# Plot the histogram of the left-skewed distribution\nhist(left_skewed_data, main = \"Left-Skewed Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"lightcoral\", border = \"black\")\n\n# Plot the histogram of the left-skewed distribution\nhist(bimodal_data, main = \"Bimodal Distribution\",\n     xlab = \"Value\", ylab = \"Frequency\",\n     col = \"khaki2\", border = \"black\")\n\n# Reset the plot layout to default\npar(old_par)\n\n\n\n\n\n\n\nFigure 2: A series of plots with histograms for the previously generated normal, right-skewed, and left-skewed distributions.\n\n\n\n\n\n\n4.1.1 The sample mean\nThe mean is the arithmetic average of the data, and it is calculated by summing all the data, dividing it by the sample size and n (Equation 1).\n\n\nThe mean, \\(\\bar{x}\\), is calculated thus: \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} \\tag{1}\\] where \\(x_{1} + x_{2} + \\cdots + x_{n}\\) are the observations\\(n\\) is the number of observations in the sample.\nWe can calculate the mean of a sample using the … wait for it … mean() function:\n\nround(mean(normal_data), 3)\n\nR&gt; [1] 0.009\n\n\n\n\n\n\n\n\nImportantDo It Now!\n\n\n\nHow would you manually calculate the mean value for the normal_data?\n\n\nThe mean is quite sensitive to the presence of outliers or extreme values in the data, and it is advised that its use be reserved for normally distributed data from which the extremes/outliers have been removed. When extreme values are indeed part of our data, not simply ‘noise’, then we have to resort to a different measure of central tendency: the median.\n\n\n4.1.2 The median\nThe median indicates the centre value in our dataset. The simplest way to explain what it is is to describe how it is determined. It can be calculated by ‘hand’ (if you have a small enough amount of data) by arranging all the numbers in sequence from low to high, and then finding the middle value. If there are five numbers, say 5, 2, 6, 13, 1, then you would arrange them from low to high, i.e. 1, 2, 5, 6, 13. The middle number is 5. This is the median. But there is no middle if we have an even number of values. What now? Take this example sequence of six integers (they may also be floating point numbers), which has already been ordered for your pleasure: 1, 2, 5, 6, 9, 13. Find the middle two numbers (i.e. 5, 6) and take the mean. It is 5.5. That is the median.\nThe median is therefore the value that separates the lower half of the sample data from the upper half. In normally distributed continuous data the median is equal to the mean. Comparable concepts to the median are the 1st and 3rd quartiles, which, respectively, separate the first quarter of the data from the last quarter — see the later in the section on ‘Measures of variance, dispersal’ in this Chapter. The advantage of the median over the mean is that it is unaffected by extreme values or outliers. The median is also used to provide a robust description of non-parametric data (see Chapter 4 for a discussion on normal data and other data distributions).\nWhat is the median of the normal_data dataset? We use the median() function for this:\n\nround(median(normal_data), 3)\n\nR&gt; [1] 0.017\n\n\nIt is easier to see what the median is by looking at a much smaller dataset. Let us take 11 random data points:\n\nsmall_normal_data &lt;- round(rnorm(11, 13, 3), 1)\nsort(small_normal_data)\n\nR&gt;  [1]  8.9  9.1  9.2 10.3 10.7 11.8 11.9 12.8 13.8 14.5 19.7\n\nmedian(small_normal_data)\n\nR&gt; [1] 11.8\n\n\nThe mean and median together provide a comprehensive understanding of the data’s central tendency and underlying distribution.\n\n\n\n\n\n\nNoteWhat Is the Relationship Between the Median and Quantiles?\n\n\n\nThe relation between the median and quantiles lies in their roles as measures that describe the relative position of data points within a dataset. Quantiles are values that partition a dataset into equal intervals, with each interval containing the same proportion of the data. The most common types of quantiles are quartiles, quintiles, deciles, and percentiles.\nThe median is a special case of a quantile, specifically the 50th percentile, the second quartile (Q2). It divides the dataset into two equal halves or with 50% of the data points falling below the median and 50% of the data points falling above the median. In this sense, the median is a central quantile that represents the middle value of the dataset.\nBoth the median and quantiles help describe the distribution and spread of a dataset, with the median providing information about the centre, other quantiles (such as quartiles) offering insights into the overall shape and skewness, and dispersion of the data.\n\n\n\n\n4.1.3 The mode\nThe mode is a measure that represents the value or values that occur most frequently in a dataset. Unlike the mean and median, the mode can be used with both numerical, categorical data and making it quite versatile. For a dataset with a single value that appears most often, the distribution is considered unimodal. However, datasets can also be bimodal (having two modes), multimodal (having multiple modes) when there are multiple values that occur with the same highest frequency.\nWhile the mode may not always be a good representative of the dataset’s centre, especially in the presence of extreme values, skewed distributions or it can still offer valuable information about the data’s characteristics when used alongside the other measures of central tendency.\nThere is no built-in function to calculate the mode of a numeric vector, but you can make one if you need it. There are some examples on the internet that you will be able to adapt to your needs, but my cursory evaluation of them does not suggest they are particularly useful. The easiest way to see the data’s mode(s) is to examine a histogram of your data. All the data we have explored above are examples of unimodal distributions, but a bimodal distribution can also be seen in Figure 2.\n\n\n4.1.4 Skewness\nSkewness is a measure of symmetry (or asymmetry) of the data distribution, and it is best understood by understanding the location of the median relative to the mean. A distribution with a skewness of zero is considered symmetric, with both tails extending equally on either side of the mean. Here, the mean will be the same as the median. A negative skewness indicates that the mean of the data is less than their median — the data distribution is left-skewed; that is, there is a longer, heavier tail to the left of the mean. A positive skewness results from data that have a mean that is larger than their median; these data have a right-skewed distribution; so there will be a longer or heavier tail to the right of the mean. Base R does not have a built-in skewness function but we can use the one included with the e1071 package:\n\nlibrary(e1071)\n# Positive skewness\nskewness(right_skewed_data)\n\nR&gt; [1] 0.5453162\n\n# Is the mean larger than the median?\nmean(right_skewed_data) &gt; median(right_skewed_data)\n\nR&gt; [1] TRUE\n\n# Negative skewness\nskewness(left_skewed_data)\n\nR&gt; [1] -0.5790834\n\n# Is the mean less than the median?\nmean(left_skewed_data) &lt; median(left_skewed_data)\n\nR&gt; [1] TRUE\n\n\n\n\n4.1.5 Kurtosis\nKurtosis describes the tail shape of the data’s distribution. Kurtosis is effectively a measure of the ‘tailedness’ or the concentration of data in the tails of a distribution, relative to a normal distribution. A normal distribution has zero kurtosis (or close to), thus the standard tail shape (mesokurtic). Negative kurtosis indicates data with a thin-tailed (platykurtic) distribution. Positive kurtosis indicates a fat-tailed distribution (leptokurtic).\nSimilarly as to skewness, we use the e1071 package for a kurtosis function. All the output shown below suggests a tendency towards thin-tailedness, but it is subtle.\n\nkurtosis(normal_data)\n\nR&gt; [1] -0.01646261\n\nkurtosis(right_skewed_data)\n\nR&gt; [1] -0.1898941\n\nkurtosis(left_skewed_data)\n\nR&gt; [1] -0.1805365\n\n\nI have seldom used the concepts of the skewness or kurtosis in any EDA, but it is worth being aware of them. The overall purpose of examining data using the range of central tendency statistics is to get an idea of whether our data are normally distributed — a normal distribution is a key requirement for all parametric inferential statistics. See Chapter 4 for a discourse of data distributions. These central tendency statistics will serve you well as a first glance, but formal tests for normality do exist, I encourage their use before embarking on the rest of the journey. We will explore these formal tests in Chapter 7.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "basic_stats/02-summarise-and-describe.html#measures-of-variance-or-dispersion-around-the-centre",
    "href": "basic_stats/02-summarise-and-describe.html#measures-of-variance-or-dispersion-around-the-centre",
    "title": "2. Data Summaries & Descriptions",
    "section": "4.2 Measures of Variance or Dispersion Around the Centre",
    "text": "4.2 Measures of Variance or Dispersion Around the Centre\nStatistic | Function |\n|: — — — — — — — |: — — — — -| | Variance | var() | | Standard deviation | sd() | | Minimum | min() | | Maximum | max() | | Range | range() | | Quantile | quantile() | | Inter Quartile Range | IQR() |  \nA good understanding of variability, or variation around the central point, is crucial in EDA for several reasons:\n\nSignal vs. noise Variability helps distinguish between the signal (true underlying pattern) and noise (random fluctuations that might arise from stochastic processes, measurement, experimental error or other unaccounted for influences) in the data. High variability can make it difficult to detect meaningful patterns or relationships in the data, while low variability may indicate a strong underlying pattern.\nPrecision and reliability Variability is related to the precision and reliability of measurements. Smaller variability indicates more consistent and precise measurements, whereas larger variability suggests inconsistency, potential issues with the data collection process.\nComparing groups Understanding variability is essential when comparing different groups or datasets. Even if two groups have similar central tendencies, their variability may differ significantly, leading to different interpretations of the data.\nAssumptions of statistical tests Many inferential statistical tests have assumptions about the variability of the data, such as homoscedasticity (equal variances across groups), independence of observations. Assessing variability helps determine whether these assumptions are met and informs the choice of appropriate tests.\nEffect sizes and statistical power Variability plays a role in determining the effect size (magnitude of the difference between groups or strength of relationships) and the statistical power (ability to detect a true effect) of a study. High variability can make it harder to detect significant effects, requiring larger sample sizes to achieve adequate statistical power.\n\nUnderstanding variability informs the choice of inferential statistics:\n\nParametric vs non-parametric tests If the data exhibit normality and homoscedasticity, parametric tests may be appropriate (see Chapter 7). However, if the data have high variability, violates the assumptions of parametric tests or non-parametric alternatives may be more suitable.\nChoice of estimators Variability can influence the choice of estimators (e.g., mean vs. median) for central tendency, depending on the data’s distribution, presence of outliers.\nSample size calculations Variability informs sample size calculations for inferential statistics. Higher variability typically requires larger sample sizes to achieve sufficient statistical power.\nModel selection Variability can influence the choice of statistical models, as certain models may better accommodate the variability in the data than others (e.g., linear vs. non-linear models, fixed vs. random effects).\n\nLet us now look at the estimators of variance.\n\n4.2.1 Variance and standard deviation\nVariance and standard deviation (SD) are examples of interval estimates. The sample variance, \\(S^{2}\\), may be calculated according to the following formula (Equation 2). If we cannot be bothered to calculate the variance and SD by hand, we may use the built-in functions var() and sd():\n\n\nThe sample variance: \\[S^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\tag{2}\\]\nThis reads, “the sum of the squared differences from the mean, divided by the sample size minus 1.” To get the standard deviation, \\(S\\), we take the square root of the variance, i.e. \\(S = \\sqrt{S^{2}}\\).\n\nvar(normal_data)\n\nR&gt; [1] 1.002459\n\nsd(normal_data)\n\nR&gt; [1] 1.001229\n\n\n\n\n\n\n\n\nImportantDo It Now!\n\n\n\nManually calculate the variance and SD for the normal_data. Make sure your answer is the same as those reported above.\n\n\nThe interpretation of the concepts of mean and median are fairly straight forward and intuitive. Not so for the measures of variance. What does \\(S\\) represent? Firstly, the unit of measurement of \\(S\\) is the same as that of \\(\\bar{x}\\) (but the variance does not share this characteristic). If temperature is measured in °C, then \\(S\\) also takes a unit of °C. Since \\(S\\) measures the dispersion around the mean, we write it as \\(\\bar{x} \\pm S\\) (note that often the mean and standard deviation are written with the letters mu and sigma, respectively; i.e. \\(\\mu \\pm \\sigma\\)). The smaller \\(S\\) the closer the sample data are to \\(\\bar{x}\\), and the larger the value is the further away they will spread out from \\(\\bar{x}\\). So, it tells us about the proportion of observations above, below\\(\\bar{x}\\). But what proportion? We invoke the 68-95-99.7 rule: ~68% of the population (as represented by a random sample of \\(n\\) observations taken from the population) falls within 1\\(S\\) of \\(\\bar{x}\\) (i.e. ~34% below \\(\\bar{x}\\) and ~34% above \\(\\bar{x}\\)); ~95% of the population falls within 2\\(S\\); and ~99.7% falls within 3\\(S\\) (Figure 3).\n\n\n\n\n\n\n\n\nFigure 3: The idealised Normal distribution showing the proportion of data within 1, 2, and 3SD from the mean.\n\n\n\nLike the mean, \\(S\\) is affected by extreme values and outliers, so before we attach \\(S\\) as a summary statistic to describe some data, we need to ensure that the data are in fact normally distributed. We will talk about how to do this in Chapter 7, where we will go over the numerous ways to check the assumption of normality. When the data are found to be non-normal, we need to find appropriate ways to express the spread of the data. Enter the quartiles.\n\n\n4.2.2 The minimum, maximum, and range\nA description of the extremes (edges of the distribution) of the data can also be provided by the functions min(), max() and range(). These concepts are straight forward and do not require elaboration. They apply to data of any distribution, and not only to normal data. These statistics are often the first places you want to start when looking at the data for the first time. Note that range() does something different from min() and max():\n\nmin(normal_data)\n\nR&gt; [1] -3.400137\n\nmax(normal_data)\n\nR&gt; [1] 3.235566\n\nrange(normal_data)\n\nR&gt; [1] -3.400137  3.235566\n\n\nrange() actually gives us the minimum and maximum values, and not the difference between them. To find the range value properly we must be a bit more clever:\n\nrange(normal_data)[2] - range(normal_data)[1]\n\nR&gt; [1] 6.635703\n\n\n\n\n4.2.3 Quartiles and the interquartile range\nA more forgiving approach (forgiving of the extremes, often called ‘robust’) is to divide the distribution of ordered data into quarters, finding the points below which 25% (0.25 and the first quartile; Q1), 50% (0.50, the median; Q2), 75% (0.75 and the third quartile; Q3) of the data are distributed. These are called quartiles (for ‘quarter;’ not to be confused with quantile, which is a more general concept that divides the distribution into any arbitrary proportion from 0 to 1).\nThe interquartile range (IQR) is a measure of statistical dispersion that provides information about the spread of the middle 50% of a dataset. It is calculated by subtracting the first quartile (25th percentile) from the third quartile (75th percentile).\nThe quartiles and IQR have several important uses:\n\nIdentifying central tendency As I have shown earlier, the second quartile, or median, is a measure of central tendency that is less sensitive to outliers than the mean. It offers a more robust estimate of the typical value in skewed distributions, those with extreme values.\nMeasure of variability The IQR is a robust measure of variability that is less sensitive to outliers and extreme values compared to other measures like the range or standard deviation. It gives a better understanding of the data spread in the middle part of the distribution.\nIdentifying outliers The IQR can be used to identify potential outliers in the data. A common method is to define outliers as data points falling below the first quartile minus 1.5 times the IQR or above the third quartile plus 1.5 times the IQR.\nDescribing skewed data For skewed distributions, the quartiles, IQR provide a better description of the data spread than the standard deviation and as it is not influenced by the skewness of the data. It can help reveal the degree of asymmetry in the distribution and the concentration of values in the middle portion.\nComparing distributions The IQR can be used to compare the variability or spread of two or more distributions. It provides a more robust comparison than the standard deviation or range when the distributions have outliers or are not symmetric, and the median reveals departures from normality.\nBox plots The quartiles and IQR are key components of box plots, which are graphical representations of the distribution of a dataset. Box plots display the median, first quartile, third quartile, and potential outliers, providing a visual representation of the data’s central tendency, spread, and potential outliers.\n\nIn R we use the quantile() function to provide the quartiles. Here is a demonstration:\n\n# Look at the normal data\nquantile(normal_data, p = 0.25)\n\nR&gt;        25% \nR&gt; -0.6597937\n\nquantile(normal_data, p = 0.75)\n\nR&gt;       75% \nR&gt; 0.6840946\n\n# Look at skewed data\nquantile(left_skewed_data, p = 0.25)\n\nR&gt;       25% \nR&gt; 0.6133139\n\nquantile(left_skewed_data, p = 0.75)\n\nR&gt;       75% \nR&gt; 0.8390202\n\n\nWe calculate the interquartile range using the IQR() function:\n\nIQR(normal_data)\n\nR&gt; [1] 1.343888",
    "crumbs": [
      "Home",
      "Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "basic_stats/02-summarise-and-describe.html#summary",
    "href": "basic_stats/02-summarise-and-describe.html#summary",
    "title": "2. Data Summaries & Descriptions",
    "section": "5.1 summary()",
    "text": "5.1 summary()\nThe first method is a generic function that can be applied to a range of R data structures, and whose output depends on the class of the structure. It is called summary(). This function can be applied to the dataset itself (here a tibble) and also to the output of some models fitted to the data (later we will see, for instance, how it is applied to t-tests, ANOVAs, correlations, and regressions). When applied to a dataframe, tibble or we will be presented with something quite useful. Let us return to the Palmer penguin dataset, and you will see many familiar descriptive statistics:\n\nsummary(penguins)\n\nR&gt;       species          island    bill_length_mm  bill_depth_mm  \nR&gt;  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \nR&gt;  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \nR&gt;  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \nR&gt;                                  Mean   :43.92   Mean   :17.15  \nR&gt;                                  3rd Qu.:48.50   3rd Qu.:18.70  \nR&gt;                                  Max.   :59.60   Max.   :21.50  \nR&gt;                                  NA's   :2       NA's   :2      \nR&gt;  flipper_length_mm  body_mass_g       sex           year     \nR&gt;  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \nR&gt;  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \nR&gt;  Median :197.0     Median :4050   NA's  : 11   Median :2008  \nR&gt;  Mean   :200.9     Mean   :4202                Mean   :2008  \nR&gt;  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \nR&gt;  Max.   :231.0     Max.   :6300                Max.   :2009  \nR&gt;  NA's   :2         NA's   :2",
    "crumbs": [
      "Home",
      "Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "basic_stats/02-summarise-and-describe.html#psychdescribe",
    "href": "basic_stats/02-summarise-and-describe.html#psychdescribe",
    "title": "2. Data Summaries & Descriptions",
    "section": "5.2 psych::describe()",
    "text": "5.2 psych::describe()\nThe psych package has the describe() function, which provides a somewhat more verbose output containing many of the descriptive statistics I introduced earlier in this Chapter:\n\npsych::describe(penguins)\n\nR&gt;                   vars   n    mean     sd  median trimmed    mad    min    max\nR&gt; species*             1 344    1.92   0.89    2.00    1.90   1.48    1.0    3.0\nR&gt; island*              2 344    1.66   0.73    2.00    1.58   1.48    1.0    3.0\nR&gt; bill_length_mm       3 342   43.92   5.46   44.45   43.91   7.04   32.1   59.6\nR&gt; bill_depth_mm        4 342   17.15   1.97   17.30   17.17   2.22   13.1   21.5\nR&gt; flipper_length_mm    5 342  200.92  14.06  197.00  200.34  16.31  172.0  231.0\nR&gt; body_mass_g          6 342 4201.75 801.95 4050.00 4154.01 889.56 2700.0 6300.0\nR&gt; sex*                 7 333    1.50   0.50    2.00    1.51   0.00    1.0    2.0\nR&gt; year                 8 344 2008.03   0.82 2008.00 2008.04   1.48 2007.0 2009.0\nR&gt;                    range  skew kurtosis    se\nR&gt; species*             2.0  0.16    -1.73  0.05\nR&gt; island*              2.0  0.61    -0.91  0.04\nR&gt; bill_length_mm      27.5  0.05    -0.89  0.30\nR&gt; bill_depth_mm        8.4 -0.14    -0.92  0.11\nR&gt; flipper_length_mm   59.0  0.34    -1.00  0.76\nR&gt; body_mass_g       3600.0  0.47    -0.74 43.36\nR&gt; sex*                 1.0 -0.02    -2.01  0.03\nR&gt; year                 2.0 -0.05    -1.51  0.04",
    "crumbs": [
      "Home",
      "Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "basic_stats/02-summarise-and-describe.html#skimrskim",
    "href": "basic_stats/02-summarise-and-describe.html#skimrskim",
    "title": "2. Data Summaries & Descriptions",
    "section": "5.3 skimr::skim()",
    "text": "5.3 skimr::skim()\nThe skimr package offers something similar, but different. The skim() function returns:\n\nlibrary(skimr)\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇",
    "crumbs": [
      "Home",
      "Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "basic_stats/02-summarise-and-describe.html#jmvdescriptives",
    "href": "basic_stats/02-summarise-and-describe.html#jmvdescriptives",
    "title": "2. Data Summaries & Descriptions",
    "section": "5.4 jmv::descriptives()",
    "text": "5.4 jmv::descriptives()\nHere is yet another view into our data, this time courtesy of the jmv package:\n\nlibrary(jmv)\ndescriptives(penguins, freq = TRUE)\n\nR&gt; \nR&gt;  DESCRIPTIVES\nR&gt; \nR&gt;  Descriptives                                                                                                                           \nR&gt;  ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nR&gt;                          species    island    bill_length_mm    bill_depth_mm    flipper_length_mm    body_mass_g    sex    year        \nR&gt;  ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nR&gt;    N                         344       344               342              342                  342            342    333          344   \nR&gt;    Missing                     0         0                 2                2                    2              2     11            0   \nR&gt;    Mean                                             43.92193         17.15117             200.9152       4201.754            2008.029   \nR&gt;    Median                                           44.45000         17.30000             197.0000       4050.000            2008.000   \nR&gt;    Standard deviation                               5.459584         1.974793             14.06171       801.9545           0.8183559   \nR&gt;    Minimum                                          32.10000         13.10000                  172           2700                2007   \nR&gt;    Maximum                                          59.60000         21.50000                  231           6300                2009   \nR&gt;  ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nR&gt; \nR&gt; \nR&gt;  FREQUENCIES\nR&gt; \nR&gt;  Frequencies of species                                \nR&gt;  ───────────────────────────────────────────────────── \nR&gt;    species      Counts    % of Total    Cumulative %   \nR&gt;  ───────────────────────────────────────────────────── \nR&gt;    Adelie          152      44.18605        44.18605   \nR&gt;    Chinstrap        68      19.76744        63.95349   \nR&gt;    Gentoo          124      36.04651       100.00000   \nR&gt;  ───────────────────────────────────────────────────── \nR&gt; \nR&gt; \nR&gt;  Frequencies of island                                 \nR&gt;  ───────────────────────────────────────────────────── \nR&gt;    island       Counts    % of Total    Cumulative %   \nR&gt;  ───────────────────────────────────────────────────── \nR&gt;    Biscoe          168      48.83721        48.83721   \nR&gt;    Dream           124      36.04651        84.88372   \nR&gt;    Torgersen        52      15.11628       100.00000   \nR&gt;  ───────────────────────────────────────────────────── \nR&gt; \nR&gt; \nR&gt;  Frequencies of sex                                 \nR&gt;  ────────────────────────────────────────────────── \nR&gt;    sex       Counts    % of Total    Cumulative %   \nR&gt;  ────────────────────────────────────────────────── \nR&gt;    female       165      49.54955        49.54955   \nR&gt;    male         168      50.45045       100.00000   \nR&gt;  ──────────────────────────────────────────────────",
    "crumbs": [
      "Home",
      "Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "basic_stats/02-summarise-and-describe.html#summarytoolsdfsummary",
    "href": "basic_stats/02-summarise-and-describe.html#summarytoolsdfsummary",
    "title": "2. Data Summaries & Descriptions",
    "section": "5.5 summarytools::dfSummary()",
    "text": "5.5 summarytools::dfSummary()\nAnd lastly, there is the summarytools package, thedfSummary() function within:\n\nlibrary(summarytools)\nprint(dfSummary(penguins, \n                varnumbers   = FALSE, \n                valid.col    = FALSE, \n                graph.magnif = 0.76),\n      method = 'render')\n\n\nData Frame Summary\npenguins\nDimensions: 344 x 8\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nMissing\n\n\n\n\nspecies [factor]\n\n\n\n1. Adelie\n\n\n2. Chinstrap\n\n\n3. Gentoo\n\n\n\n\n\n\n152\n(\n44.2%\n)\n\n\n68\n(\n19.8%\n)\n\n\n124\n(\n36.0%\n)\n\n\n\n\n0 (0.0%)\n\n\nisland [factor]\n\n\n\n1. Biscoe\n\n\n2. Dream\n\n\n3. Torgersen\n\n\n\n\n\n\n168\n(\n48.8%\n)\n\n\n124\n(\n36.0%\n)\n\n\n52\n(\n15.1%\n)\n\n\n\n\n0 (0.0%)\n\n\nbill_length_mm [numeric]\n\n\n\nMean (sd) : 43.9 (5.5)\n\n\nmin ≤ med ≤ max:\n\n\n32.1 ≤ 44.5 ≤ 59.6\n\n\nIQR (CV) : 9.3 (0.1)\n\n\n\n164 distinct values\n\n2 (0.6%)\n\n\nbill_depth_mm [numeric]\n\n\n\nMean (sd) : 17.2 (2)\n\n\nmin ≤ med ≤ max:\n\n\n13.1 ≤ 17.3 ≤ 21.5\n\n\nIQR (CV) : 3.1 (0.1)\n\n\n\n80 distinct values\n\n2 (0.6%)\n\n\nflipper_length_mm [integer]\n\n\n\nMean (sd) : 200.9 (14.1)\n\n\nmin ≤ med ≤ max:\n\n\n172 ≤ 197 ≤ 231\n\n\nIQR (CV) : 23 (0.1)\n\n\n\n55 distinct values\n\n2 (0.6%)\n\n\nbody_mass_g [integer]\n\n\n\nMean (sd) : 4201.8 (802)\n\n\nmin ≤ med ≤ max:\n\n\n2700 ≤ 4050 ≤ 6300\n\n\nIQR (CV) : 1200 (0.2)\n\n\n\n94 distinct values\n\n2 (0.6%)\n\n\nsex [factor]\n\n\n\n1. female\n\n\n2. male\n\n\n\n\n\n\n165\n(\n49.5%\n)\n\n\n168\n(\n50.5%\n)\n\n\n\n\n11 (3.2%)\n\n\nyear [integer]\n\n\n\nMean (sd) : 2008 (0.8)\n\n\nmin ≤ med ≤ max:\n\n\n2007 ≤ 2008 ≤ 2009\n\n\nIQR (CV) : 2 (0)\n\n\n\n\n\n\n2007\n:\n110\n(\n32.0%\n)\n\n\n2008\n:\n114\n(\n33.1%\n)\n\n\n2009\n:\n120\n(\n34.9%\n)\n\n\n\n\n0 (0.0%)\n\n\n\n\nGenerated by summarytools 1.1.5 (R version 4.5.2)2026-02-07\n\n\n\nAs you can see, there are many options; you may use the one you least dislike. I will not be prescriptive or openly opinionated about it.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "2. Data Summaries & Descriptions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#bernoulli-and-binomial-distributions",
    "href": "basic_stats/04-distributions.html#bernoulli-and-binomial-distributions",
    "title": "4. Data Distributions",
    "section": "3.1 Bernoulli and Binomial Distributions",
    "text": "3.1 Bernoulli and Binomial Distributions\nThe Bernoulli and Binomial distributions belong to what might be termed the “trial-based” subfamily of discrete distributions: they directly model outcomes of repeated experiments. Trial-based distributions require specifying both the number of trials and success probability.\nBernoulli and Binomial distributions are both discrete probability distributions that describe the outcomes of binary events. They are similar but there are also some key differences between the two. In real life examples encountered in ecology and biology we will probably mostly encounter the Binomial distributions. Let us consider each is more detail.\nBernoulli distribution The Bernoulli distribution represents a single binary trial or experiment with only two possible outcomes: ‘success’ (usually represented as 1) and ‘failure’ (usually represented as 0). The probability of success is denoted by \\(p\\), while the probability of failure is \\(1 - p\\). A Bernoulli distribution is characterised by only one parameter, \\(p\\), which represents the probability of success for the single trial (Equation 1) (Figure 1, Figure 2).\n\n\nThe Bernoulli distribution: \\[\nP(X=k) = \\begin{cases}\n  p, & \\text{if } k=1 \\\\\n  1-p, & \\text{if } k=0\n\\end{cases}\n\\tag{1}\\]\nwhere \\(X\\) is a random variable, \\(k\\) is the outcome (1 for success and 0 for failure),\\(p\\) is the probability of success.\n\n\n\n\n\n\n\n\nFigure 1: Bernoulli distribution with 10 trials and 20 simulations set at a probability of p = 0.2. This could be a heavily loaded coin that has a 20% chance of landing heads.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bernoulli distribution with 10 trials and 20 simulations set at a probability of p = 0.5. This represents an unbiased coin that has an equal chance of landing on head or tail.\n\n\n\n\n\nBinomial distribution The Binomial distribution represents the sum of outcomes in a fixed number of independent Bernoulli trials with the same probability of success, \\(p\\). It is characterised by two parameters, \\(n\\) (the number of trials)\\(p\\) (the probability of success in each trial). The Binomial distribution describes the probability of obtaining a specific number of successes (\\(k\\)) in \\(n\\) trials (Equation 2) (Figure 3).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Binomial distribution: \\[P(X=k) = C(n,k) \\cdot p^k \\cdot (1-p)^{(n-k)} \\tag{2}\\]\nwhere \\(X\\) is a random variable, \\(k\\) is the number of successes, \\(n\\) is the total number of trials, \\(p\\) is the probability of success in each trial,\\(C(n,k)\\) represents combinations.\nIn practice, determine \\(n\\) (number of trials), \\(p\\) (success probability),\\(k\\) (desired number of successes). Calculate the binomial coefficient \\(C(n,k) = \\frac{n!}{k!(n-k)!}\\), then apply Equation 2. If flipping a coin 10 times (\\(n=10\\), \\(p=0.5\\)) and seeking exactly 3 heads (\\(k=3\\)): \\(C(10,3) = 120\\), so \\(P(X=3) = 120 \\cdot (0.5)^3 \\cdot (0.5)^7 = 120 \\cdot (0.5)^{10} = 0.117\\).\n\n\n\n\n\n\n\n\nFigure 3: Ten simulations of a binomial distribution with 40 randomly generated points for each of 100 trials at an a priori set probability of p = 0.75.\n\n\n\n\n\nThere are several examples of Binomial distributions in ecological and biological contexts. The Binomial distribution is relevant when studying the number of successes in a fixed number of independent trials, each with the same probability of success. A few examples of the Bernoulli distribution:\n\nSeed germination Suppose we plant 100 seeds of a particular plant species and wants to know the probability of a certain number of seeds germinating. If the probability of germination for each seed is constant then we can model the number of germinated seeds by a Binomial distribution.\nDisease prevalence An epidemiologist studies the prevalence of a disease within a population. For a random sample of 500 individuals, and with a fixed probability of an individual having the disease, the number of infected individuals in the sample can be modelled using a Binomial distribution.\nSpecies occupancy We do an ecological assessment to determine the occupancy of bird species across 50 habitat patches. If the probability of the species occupying a patch is the same across all patches, the number of patches occupied by the species will follow a Binomial distribution.\nAllele inheritance We want to examine the inheritance of a specific trait following Mendelian inheritance patterns. If the probability of inheriting the dominant allele for a given gene is constant, the number of offspring with the dominant trait in a fixed number of offspring follows the Binomial distribution.\n\nNote that in these examples we assume a fixed probability and independence between trials and this is not always be true in real-world situations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#negative-binomial-and-geometric-distributions",
    "href": "basic_stats/04-distributions.html#negative-binomial-and-geometric-distributions",
    "title": "4. Data Distributions",
    "section": "3.2 Negative Binomial and Geometric Distributions",
    "text": "3.2 Negative Binomial and Geometric Distributions\nThe Geometric and Negative Binomial distributions form a “waiting time” subfamily, focusing on the number of trials preceding specified success patterns. These waiting-time distributions focus on success probability, target achievement levels.\nNegative Binomial distribution A Negative Binomial random variable, \\(X\\), counts the number of successes in a sequence of independent Bernoulli trials with probability \\(p\\) before \\(r\\) failures occur. This distribution could for example be used to predict the number of heads that result from a series of coin tosses before three tails are observed (Equation 3) (Figure 4).\n\n\nThe Negative Binomial distribution: \\[P(X=k) = \\binom{k+r-1}{k} p^r (1-p)^k \\tag{3}\\]\nThe equation describes the probability mass function (PMF) of a Negative Binomial distribution, where \\(X\\) is a random variable, \\(k\\) is the number of failures, \\(r\\) is the number of successes,\\(p\\) is the probability of success in each trial. The binomial coefficient is denoted by \\(\\binom{k+r-1}{k}\\), which calculates the number of ways to arrange \\(k\\) failures\\(r\\) successes such that the last trial is a success.\n\n\n\n\n\n\n\n\nFigure 4: A negative binomial distribution with 50 trials and 10 simulations at an a priori expectation of p = 0.75.\n\n\n\n\n\nGeometric distribution A geometric random variable, \\(X\\), represents the number of trials that are required to observe a single success. Each trial is independent and has success probability \\(p\\). As an example, the geometric distribution is useful to model the number of times a die must be tossed in order for a six to be observed (Equation 4) (Figure 5).\n\n\nThe Geometric distribution: \\[P(X=k) = p (1-p)^k \\tag{4}\\]\nThe equation represents the PMF of a Geometric distribution, where \\(X\\) is a random variable, \\(k\\) is the number of failures before the first success,\\(p\\) is the probability of success in each trial. The Geometric distribution can be thought of as a special case of the Negative Binomial distribution with \\(r = 1\\), which models the number of failures before achieving a single success.\n\n\n\n\n\n\n\n\nFigure 5: A geometric distribution with 50 trials and 10 simulations at an a priori expectation of p = 0.75.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#poisson-distribution",
    "href": "basic_stats/04-distributions.html#poisson-distribution",
    "title": "4. Data Distributions",
    "section": "3.3 Poisson Distribution",
    "text": "3.3 Poisson Distribution\n\n\nThe Poisson distribution: \\[P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\tag{5}\\]\nThe function represents the PMF of a Poisson distribution, where \\(X\\) is a random variable, \\(k\\) is the number of events or occurrences,\\(\\lambda\\) (lambda) is the average rate of occurrences (events per unit of time or space). The constant \\(e\\) is the base of the natural logarithm,\\(k!\\) is the factorial of \\(k\\). The Poisson distribution is commonly used to model the number of events occurring within a fixed interval of time or space when events occur independently and at a constant average rate.\nFor practical application, identify \\(\\lambda\\) (average rate)\\(k\\) (observed count). If a gerbil enters a nest 4 times per hour (\\(\\lambda=4\\)) and you want the probability of exactly 2 entries in one hour (\\(k=2\\)): \\(P(X=2) = \\frac{4^2 \\cdot e^{-4}}{2!} = \\frac{16 \\cdot 0.0183}{2} = 0.147\\).\nThe Poisson distribution represents a “rate-based” subfamily, which is used to model count phenomena without explicit trial structure. They involve the average occurrence rates over specified intervals.\nA Poisson random variable, \\(X\\), tallies the number of events occurring in a fixed interval of time or space, given that these events occur with an average rate \\(\\lambda\\). Poisson distributions can be used to model events such as meteor showers or number of people entering a shopping mall (Equation 5) (Figure 6).\n\n\n\n\n\n\n\n\nFigure 6: A Poisson distribution with 50 trials and 10 simulations at an a priori expectation of p = 0.75.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#hypergeometric-distribution",
    "href": "basic_stats/04-distributions.html#hypergeometric-distribution",
    "title": "4. Data Distributions",
    "section": "3.4 Hypergeometric Distribution",
    "text": "3.4 Hypergeometric Distribution\n\n\nThe Hypergeometricd distribution: \\[P(X=k) = \\frac{C(K,k) \\cdot C(N-K,n-k)}{C(N,n)} \\tag{6}\\] In the probability mass function, above, \\(N\\) is population size, \\(K\\) is number of success items in population, \\(n\\) is sample size,\\(k\\) is observed successes in sample.\nConsider drawing 5 cards from a deck without replacement, seeking exactly 2 hearts. Here \\(N=52\\), \\(K=13\\) (hearts), \\(n=5\\), \\(k=2\\): \\(P(X=2) = \\frac{C(13,2) \\cdot C(39,3)}{C(52,5)} = \\frac{78 \\cdot 9139}{2,598,960} = 0.274\\).\nThe hypergeometric distribution occupies a distinct position within the discrete distribution taxonomy: we might term the “finite population sampling” subfamily.\nThe hypergeometric distribution models sampling without replacement from finite populations containing two types of items. Unlike binomial sampling, each draw changes the composition of remaining items.\nTo do: insert figures.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#sec-normal",
    "href": "basic_stats/04-distributions.html#sec-normal",
    "title": "4. Data Distributions",
    "section": "4.1 Normal Distribution",
    "text": "4.1 Normal Distribution\n\n\nThe Normal distribution: \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2 }\n\\tag{7}\\]\nwhere \\(x\\) is a continuous random variable, \\(\\mu\\) (mu) is the mean,\\(\\sigma\\) (sigma) is the standard deviation. The constant factor \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\) ensures that the Probability Density Function (PDF) integrates to 1, and the exponential term is responsible for the characteristic bell-shaped curve of the Normal distribution.\n\n\n\n\n\n\n\nFigure 7: The idealised Normal distribution showing the proportion of data within 1, 2, and 3SD from the mean.\n\n\n\n\nAnother name for this kind of distribution is a Gaussian distribution. A random sample with a Gaussian distribution is normally distributed. These values are independent and identically distributed random variables (i.i.d.), and they have an expected mean given by \\(\\mu\\) (or \\(\\hat{x}\\) in Chapter 3.2.1) and a finite variance given by \\(\\sigma^{2}\\) (or \\(S^{2}\\) in Chapter 3.3.1); if the number of samples drawn from a population is sufficiently large, the estimated mean, SD will be indistinguishable from the population (as per the central limit theorem). It is represented by Equation 7 (Figure 8).\n\n\n\n\n\n\n\n\nFigure 8: Normal distribution with 40 trials and 5 simulations.\n\n\n\n\n\n\n\n\n\n\n\nNoteCentral Limit Theorem\n\n\n\nThe Central Limit Theorem (CLT) is a fundamental result in probability theory and statistics, which states that the distribution of the sum (or average) of a large number of independent, identically distributed (IID) random variables approaches a Normal distribution regardless of the shape of the original distribution. So, the CLT asserts that the Normal distribution is the limiting distribution for the sum, average of many random variables or as long as certain conditions are met.\nThe CLT provides a basis for making inferences about population parameters using sample statistics. For example, when dealing with large sample sizes, the sampling distribution of the sample mean is approximately normally distributed, even if the underlying population distribution is not normal. This allows us to apply inferential techniques based on the Normal distribution, such as hypothesis testing, constructing confidence intervals and to estimate population parameters using sample data.\nSome conditions must be met for the CLT to be true:\n\nThe random variables must be independent The observations should not be influenced by one another.\nThe random variables must be identically distributed They must come from the same population with the same mean and variance.\nThe number of random variables (sample size) must be sufficiently large Although there is no strict rule for the sample size, a common rule of thumb is that the sample size should be at least 30 for the CLT to be a reasonable approximation.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#uniform-distribution",
    "href": "basic_stats/04-distributions.html#uniform-distribution",
    "title": "4. Data Distributions",
    "section": "4.2 Uniform Distribution",
    "text": "4.2 Uniform Distribution\nThe continuous uniform distribution is sometime called a rectangular distribution. Simply, it states that all measurements of the same magnitude included with this distribution are equally probable. This is basically random numbers (Figure 9).\n\n\n\n\n\n\n\n\nFigure 9: Uniform distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#student-t-distribution",
    "href": "basic_stats/04-distributions.html#student-t-distribution",
    "title": "4. Data Distributions",
    "section": "4.3 Student T Distribution",
    "text": "4.3 Student T Distribution\nThis is a continuous probability distribution that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. It is used in the statistical significance testing between the means of different sets of samples, and not much so in the modelling of many kinds of experiments, observations (Figure 10).\n\n\n\n\n\n\n\n\nFigure 10: Uniform distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#chi-squared-distribution",
    "href": "basic_stats/04-distributions.html#chi-squared-distribution",
    "title": "4. Data Distributions",
    "section": "4.4 Chi-squared Distribution",
    "text": "4.4 Chi-squared Distribution\nMostly used in hypothesis testing, but not to encapsulate the distribution of data drawn to represent natural phenomena (Figure 11).\n\n\n\n\n\n\n\n\nFigure 11: Chi distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#exponential-distribution",
    "href": "basic_stats/04-distributions.html#exponential-distribution",
    "title": "4. Data Distributions",
    "section": "4.5 Exponential Distribution",
    "text": "4.5 Exponential Distribution\nThis is a probability distribution that describes the time between events in a Poisson point process, i.e., a process in which events occur continuously, independently at a constant average rate (Figure 12).\n\n\n\n\n\n\n\n\nFigure 12: An exponential distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#f-distribution",
    "href": "basic_stats/04-distributions.html#f-distribution",
    "title": "4. Data Distributions",
    "section": "4.6 F Distribution",
    "text": "4.6 F Distribution\nThis is a probability distribution that arises in the context of the analysis of variance (ANOVA) and regression analysis. It is used to compare the variances of two populations (Figure 13).\n\n\n\n\n\n\n\n\nFigure 13: An F distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#gamma-distribution",
    "href": "basic_stats/04-distributions.html#gamma-distribution",
    "title": "4. Data Distributions",
    "section": "4.7 Gamma Distribution",
    "text": "4.7 Gamma Distribution\nThis is a two-parameter family of continuous probability distributions. It is used to model the time until an event occurs. It is a generalisation of the exponential distribution (Figure 14).\n\n\n\n\n\n\n\n\nFigure 14: A Gamma distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/04-distributions.html#beta-distribution",
    "href": "basic_stats/04-distributions.html#beta-distribution",
    "title": "4. Data Distributions",
    "section": "4.8 Beta Distribution",
    "text": "4.8 Beta Distribution\nThis is a family of continuous probability distributions defined on the interval [0, 1] parameterised by two positive shape parameters, typically denoted by α, β. It is used to model the behaviour of random variables limited to intervals of finite length in a wide variety of disciplines (Figure 15).\n\n\n\n\n\n\n\n\nFigure 15: A Beta distribution with 100 trials and 5 simulations.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "4. Data Distributions"
    ]
  },
  {
    "objectID": "basic_stats/06-assumptions.html#tests-for-normality",
    "href": "basic_stats/06-assumptions.html#tests-for-normality",
    "title": "6. Assumptions",
    "section": "2.1 Tests for Normality",
    "text": "2.1 Tests for Normality\n\n\n\n\n\n\n\n\nFigure 1: Histograms showing two randomly generated normal distributions.\n\n\n\n\n\nRemember from Chapter 4 what a normal distribution is/looks like? Let us have a peek below to remind ourselves (Figure 1).\nWhereas histograms may be a pretty way to check the normality of our data, there are actual statistical tests for this, which is preferable to a visual inspection alone. But remember that you should always visualise your data before performing any statistics on them.\n\n\n\n\n\n\nNoteHypothesis for Normailty\n\n\n\n\\(H_{0}\\): The distribution of our data is not different from normal (or the variable is normally distributed).\n\n\nThe Shapiro-Wilk test is frequently used to assess the normality of a dataset. It is known to have good power and accuracy for detecting departures from normality, even for small sample sizes, and it is also robust to outliers, making it useful for analysing data that may contain extreme values.\nIt tests the H0 that the population from which the sample, \\(x_{1},..., x_{n}\\), was drawn is not significantly different from normal. The test does so by sorting the data from lowest to highest, and a test statistic, \\(W\\), is calculated based on the deviations of the observed values from the expected values under a normal distribution (Equation 1). \\(W\\) is compared to a critical value, based on the sample size, significance level and to determine whether to reject or fail to reject the H0.\n\n\nThe Shapiro-Wilk test: \\[W = \\frac{(\\sum_{i=1}^n a_i x_{(i)})^2}{\\sum_{i=1}^n (x_i - \\overline{x})^2} \\tag{1}\\]\nHere, \\(W\\) represents the Shapiro-Wilk test statistic, \\(a_{i}\\) are coefficients that depend on the sample size and distribution of the data, \\(x_{(i)}\\) represents the \\(i\\)-th order statistic, or the \\(i\\)-th smallest value in the sample,\\(\\overline{x}\\) represents the sample mean.\nThe Shapiro-Wilk test is available within base R as the function shapiro.test(). If the p-value is above 0.05 we may assume the data to be normally distributed. In order to demonstrate what the output of shapiro.test() looks like we will run it on all of the random data we generated.\n\nshapiro.test(r_dat$dat)\n\nR&gt; \nR&gt;  Shapiro-Wilk normality test\nR&gt; \nR&gt; data:  r_dat$dat\nR&gt; W = 0.70346, p-value &lt; 2.2e-16\n\n\nNote that this shows that the data are not normally distributed. This is because we have incorrectly run this function simultaneously on two different samples of data. To perform this test correctly, and in the tidy way, we need to recognise the grouping structure (chickens, giraffes) and select only the second piece of information from theshapiro.test() output and ensure that it is presented as a numeric value:\n\n# we use the square bracket notation to select only the *-value;\n# had we used `[1]` we would have gotten W\nr_dat %&gt;% \n  group_by(sample) %&gt;% \n  summarise(norm_dat = as.numeric(shapiro.test(dat)[2]))\n\nR&gt; # A tibble: 2 × 2\nR&gt;   sample   norm_dat\nR&gt;   &lt;chr&gt;       &lt;dbl&gt;\nR&gt; 1 Chickens    0.461\nR&gt; 2 Giraffes    0.375\n\n\nNow we see that our two sample sets are indeed normally distributed.\nSeveral other tests are available to test whether our data are consistent with a normal distribution:\n\nKolmogorov-Smirnov test This test is a non-parametric test that compares the empirical distribution of a sample with a hypothesised normal distribution. It is based on the maximum absolute difference between the cumulative distribution function of the sample and the theoretical normal distribution function. This test can also be used to see if one’s own data are consistent with other kinds of data distributions. In R the Kolmogorov-Smirnov test is available as ks.test().\nAnderson-Darling test Similar to the Shapiro-Wilk test, the Anderson-Darling test is used to test the hypothesis that a sample comes from a normal (or any other) distribution. It is based on the squared differences between the empirical distribution function of the sample, the theoretical normal distribution function. This function is not natively available in base R but the functionad.test() is made available in two packages (that I know of), namely, nortest, kSamples. Read the help files — even though the name of the function is the same in the two packages and they are implemented differently.\nLilliefors test This test is a modification of the Kolmogorov-Smirnov test that is specifically designed for small sample sizes. It is based on the maximum difference between the empirical distribution function of the sample and the normal distribution function. Some R packages such as nortest and descTools seem to use Lilliefors synonymously with Kolmogorov-Smirnov. These functions are called lillie.test() and LillieTest(), respectively.\nJarque-Bera test This test is based on the skewness and kurtosis of a sample and tests whether the sample has the skewness and kurtosis expected from a normal distribution. Find it in R as jarque.bera.test() in the DescTools and tseries packages. Again, read the help files as a function with the same name appears in two independent packages, I cannot give assurance that it implemented consistently.\nCramer-Von Mises test The Cramer-Von Mises test is used to assess the goodness of fit of a distribution to a sample of data. The test is based on the cumulative distribution function (CDF) of the sample and the theoretical distribution being tested. See the cvm.test() function in the goftest package.\n\nTake your pick. The Shapiro-Wilk and Kolmogorov-Smirnov tests are the most frequently used normality tests in my experience but be adventurous and use the Cramer-Von Mises test and surprise your supervisor in an interesting way — more than likely, they will not have heard of it before. When you decide, however, do your homework, read about these pros and cons of the tests as they are not all equally robust to all the surprises data can throw at them.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "6. Assumptions"
    ]
  },
  {
    "objectID": "basic_stats/06-assumptions.html#sec-homogeneity",
    "href": "basic_stats/06-assumptions.html#sec-homogeneity",
    "title": "6. Assumptions",
    "section": "2.2 Tests for Homogeneity of Variances",
    "text": "2.2 Tests for Homogeneity of Variances\nBesides requiring that our data are normally distributed, we must also ensure that they are homoscedastic. This word means that the scedasticity (variance) of our samples is homogeneous (similar). In practical terms this means that the variance of the samples we are comparing should not be more than two to four times greater than one another. In R, we use the function var() to check the variance in a sample:\n\nr_dat %&gt;% \n  group_by(sample) %&gt;% \n  summarise(sample_var = var(dat))\n\nR&gt; # A tibble: 2 × 2\nR&gt;   sample   sample_var\nR&gt;   &lt;chr&gt;         &lt;dbl&gt;\nR&gt; 1 Chickens    0.00994\nR&gt; 2 Giraffes    0.0872\n\n\nAbove we see that the variance of our two samples is heteroscedastic because the variance of one more than two to four times greater than the other. However, there are formal tests to establish the equality of variances, as we can see in the following hypothesis tests:\n\n\n\n\n\n\nNoteHypotheses for Equality of Variances\n\n\n\nThe two-sided and one-sided formulations are:\n\\(H_{0}: \\sigma^{2}_{A} = \\sigma^{2}_{B}\\)\\(H_{a}: \\sigma^{2}_{A} \\ne \\sigma^{2}_{B}\\)\n\\(H_{0}: \\sigma^{2}_{A} \\le \\sigma^{2}_{B}\\)\\(H_{a}: \\sigma^{2}_{A} \\gt \\sigma^{2}_{B}\\)\n\\(H_{0}: \\sigma^{2}_{A} \\ge \\sigma^{2}_{B}\\)\\(H_{a}: \\sigma^{2}_{A} \\lt \\sigma^{2}_{B}\\)\nwhere \\(\\sigma^{2}_{A}\\)\\(\\sigma^{2}_{B}\\) are the variances for samples \\(A\\)\\(B\\), respectively.\n\n\nThe most commonly used test for equality of variances is Levene’s test, car::leveneTest(). Levene’s test assess the equality of variances between two or more groups in a dataset. The H0 is that the variances of the groups are equal. It is a non-parametric test that does not assume anything about the data’s normality and as such it is more robust than the F-test.\nThe test is commonly used in t-tests and ANOVA to check that the variances of the dependent variable are the same across all levels of the independent variable. Violating this assumption can lead to incorrect conclusions made from the test outcome, such as those resulting from Type I, Type II errors.\nIn Levene’s test, the absolute deviations of the observations from their group medians are calculated, and the test statistic is computed as the ratio of the sum of the deviations to the degrees of freedom (Equation 2). The test statistic follows an F distribution under the H0, and a significant result indicates that the variances of the groups are significantly different.\n\n\nLevene’s test:\n\\[W = \\frac{(N-k)}{(k-1)} \\cdot \\frac{\\sum_{i=1}^k n_i (\\bar{z}_i - \\bar{z})^2}{\\sum_{i=1}^k \\sum_{j=1}^{n_i} (z_{ij} - \\bar{z}_i)^2} \\tag{2}\\]\nwhere \\(W\\) represents the Levene’s test statistic, \\(N\\) is the total sample size, \\(k\\) is the number of groups being compared, \\(n_i\\) is the sample size of the \\(i\\)-th group, \\(z_{ij}\\) is the \\(j\\)-th observation in the \\(i\\)-th group, \\(z_{i}\\) is the mean of the ith group,\\(\\bar{z}\\) is the overall mean of all groups combined.\nThe test statistic is calculated by comparing the deviations of the observations within each group from their group mean (\\(\\bar{z}_i\\)) to the deviations of the group means from the overall mean (\\(\\bar{z}\\)).\nLevene’s test is considered robust to non-normality and outliers, making it a useful tool for analysing data that do not meet the assumptions of normality, but it can be sensitive to unequal sample sizes, may not be appropriate for very small sample sizes.\n\ncar::leveneTest(dat ~ sample, data = r_dat)\n\nR&gt; Levene's Test for Homogeneity of Variance (center = median)\nR&gt;         Df F value    Pr(&gt;F)    \nR&gt; group    1  702.15 &lt; 2.2e-16 ***\nR&gt;       1998                      \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAbove, we see that p &lt; 0.05, causing us to accept the alternative hypothesis that the variances are unequal between the sample of giraffes, the sample of chickens.\nSeveral other statistical tests are available to assess the homogeneity of variances in a dataset:\n\nF-test This test is also known as the variance ratio test. Use the var.test() function in R. It assumes that the underlying data follows a normal distribution and is designed to test the H0 that the variances of two populations are equal. The test statistic is the ratio of the variances of the two populations. You will often see this test used in the context of an ANOVA to test for homogeneity of variance across groups.\nBartlett’s test This test is similar to Levene’s test and is used to assess the equality of variances across multiple groups. The test is based on the \\(\\chi\\)-squared distribution and assumes that the data are normally distributed. Base R has the bartlett.test() function.\nBrown-Forsythe test This test is a modification of the Levene’s test that uses the absolute deviations of the observations from their respective group medians instead of means. This makes the test more robust to outliers and non-normality. It is available in onewaytests as the function bf.test().\nFligner-Killeen test This is another non-parametric test that uses the medians of the groups instead of the means. It is based on the \\(\\chi\\)-squared distribution and is also robust to non-normality and outliers. The Fligner test is available in Base R as fligner.test().\n\nAs always, supplement your analysis with these checks: i) perform any of the diagnostic plots we covered in the earlier Chapters, or ii) compare the variances, see if they differ by more than a factor of four.\nSee this discussion if you would like to know about some more advanced options when faced with heteroscedastic data.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "6. Assumptions"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html",
    "href": "basic_stats/08-anova.html",
    "title": "8. ANOVA",
    "section": "",
    "text": "Table 1",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#remember-the-t-test",
    "href": "basic_stats/08-anova.html#remember-the-t-test",
    "title": "8. ANOVA",
    "section": "2.1 Remember the t-test",
    "text": "2.1 Remember the t-test\nAs you already know, a t-test is used when we want to compare two different sample sets against one another. This is also known as a two-factor, two level test. When one wants to compare multiple (more than two) sample sets against one another an ANOVA is required (I will get there shortly). Remember how to perform a t-test in R: we will revisit this test using thechicks data, but only for Diets 1, 2 from day 21.\n\n# First grab the data\nchicks &lt;- as_tibble(ChickWeight)\n\n# Then subset out only the sample sets to be compared\nchicks_sub &lt;- chicks %&gt;% \n  filter(Diet %in% c(1, 2), Time == 21)\n\nOnce we have filtered our data we may now perform the t-test.\n\nt.test(weight ~ Diet, data = chicks_sub)\n\nR&gt; \nR&gt;  Welch Two Sample t-test\nR&gt; \nR&gt; data:  weight by Diet\nR&gt; t = -1.2857, df = 15.325, p-value = 0.2176\nR&gt; alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\nR&gt; 95 percent confidence interval:\nR&gt;  -98.09263  24.19263\nR&gt; sample estimates:\nR&gt; mean in group 1 mean in group 2 \nR&gt;          177.75          214.70\n\n\nAs one may recall from Chapter 7, whenever we want to give a formula to a function in R, we use the ~. The formula used above, weight ~ Diet, reads in plain English as “weight as a function of diet”. This is perhaps easier to understand as “Y as a function of X.” This means that we are assuming whatever is to the left of the ~ is the dependent variable, and whatever is to the right is the independent variable. Did the Diet 1, 2 produce significantly fatter birds?\nOne could also supplement the output by producing a graph (Figure 1).\n\nlibrary(ggstatsplot)\n\n## Since the Confidence Intervals for the Effect Sizes Are Computed Using\n## Bootstrapping, Important to Set a Seed for Reproducibility\nset.seed(13)\n\n## Parametric T-test and Box Plot\nggbetweenstats(\n  data = chicks_sub,\n  x = Diet,\n  y = weight,\n  xlab = \"Diet\",\n  ylab = \"Chick mass (g)\",\n  plot.type = \"box\",\n  p.adjust.method = \"bonferroni\",\n  pairwise.display = \"ns\",\n  type = \"p\",\n  results.subtitle = FALSE,\n  conf.level = 0.95,\n  title = \"t-test\",\n  ggtheme = ggthemes::theme_fivethirtyeight(),\n  package = \"basetheme\",\n  palette = \"ink\"\n)\n\n\n\n\n\n\n\nFigure 1: Box-and-whisker plot showing the differences in means between chicks reared to 21 days old and fed Diets 1 and 2\n\n\n\n\n\nNotice above that we did not need to specify to use a t-test. The ggbetweenstats() function automatically determines if an independent samples t-test or a 1-way ANOVA is required based on whether there are two groups or three or more groups within the grouping (factor) variable.\nThat was a nice revision. But applied to the chicks data it seemed a bit silly, because you may ask, “What if I wanted to know if there are differences among the means computed at Day 1, Day 6, Day 10, and Day 21?” We should not use t-tests to do this (although we can). So now we can move on to the ANOVA.\n\n\n\n\n\n\nImportantTask G.1: Do It Now!\n\n\n\n\nWhy should we not just apply t-tests once per each of the pairs of comparisons we want to make?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#why-not-do-multiple-t-tests",
    "href": "basic_stats/08-anova.html#why-not-do-multiple-t-tests",
    "title": "8. ANOVA",
    "section": "2.2 Why Not Do Multiple t-tests?",
    "text": "2.2 Why Not Do Multiple t-tests?\nIn the chicks data we have four diets, not only two as in the t-test example just performed. Why not then simply do a t-test multiple times, once for each pair of diets given to the chickens? Multiple t-tests would be written as:\n\n\\(H_{0}: \\mu_1 = \\mu_2\\)\n\\(H_{0}: \\mu_1 = \\mu_3\\)\n\\(H_{0}: \\mu_1 = \\mu_4\\)\n\\(H_{0}: \\mu_2 = \\mu_3\\)\n\\(H_{0}: \\mu_2 = \\mu_4\\)\n\\(H_{0}: \\mu_3 = \\mu_4\\)\n\nThis would be invalid. The problem is that the chance of committing a Type I error increases as more multiple comparisons are done. So, the overall chance of rejecting the H0 increases. Why? If one sets \\(\\alpha=0.05\\) (the significance level below which the H0 is no longer accepted), one will still reject the H0 5% of the time when it is in fact true (i.e. when there is no difference between the groups). When many pairwise comparisons are made, the probability of rejecting the H0 at least once is higher because we take this 5% risk each time we repeat a t-test. In the case of the chicken diets, we would have to perform six t-tests, and the error rate would increase to slightly less than \\(6\\times5\\%\\). See Table 1.\n\n\n\n\n\nTable 2: Table 1. Probability of committing a Type I error due to applying multiple t-tests to test for differences between K means. α from 0.2 to 0.0001 are shown.\n\n\n\n\n\n\nK\n0.2\n0.1\n0.05\n0.02\n0.01\n0.001\n\n\n\n\n2\n0.20\n0.10\n0.05\n0.02\n0.01\n0.00\n\n\n3\n0.49\n0.27\n0.14\n0.06\n0.03\n0.00\n\n\n4\n0.74\n0.47\n0.26\n0.11\n0.06\n0.01\n\n\n5\n0.89\n0.65\n0.40\n0.18\n0.10\n0.01\n\n\n10\n1.00\n0.99\n0.90\n0.60\n0.36\n0.04\n\n\n20\n1.00\n1.00\n1.00\n0.98\n0.85\n0.17\n\n\n100\n1.00\n1.00\n1.00\n1.00\n1.00\n0.99\n\n\n\n\n\n\n\n\n\nIf you insist in creating more work for yourself and do t-tests many times, one way to overcome the problem of committing Type I errors that stem from multiple comparisons is to apply a Bonferroni correction.\n\n\n\n\n\n\nNoteBonferonni Correction\n\n\n\nThe Bonferroni correction is used to adjust the significance level of multiple hypothesis tests, such as multiple paired t-tests among many groups, in order to reduce the risk of false positives, Type I errors. It is named after the Italian mathematician Carlo Emilio Bonferroni.\nThe Bonferroni correction is based on the principle that when multiple hypothesis tests are performed, the probability of observing at least one significant result due to random chance increases. To correct for this, the significance level (usually 0.05) is divided by the number of tests being performed. This results in a more stringent significance level for each individual test, it so reduces the risk of committing a Type I error.\nFor example, if we conduct ten hypothesis tests, the significance level for each test after Bonferonni correction would become 0.05/10 = 0.005. The implication is that each individual test would need to have a p-value less than 0.005 to be considered significant at the overall significance level of 0.05.\nOn the downside, this method can be overly conservative, we may then increase the risk of Type II errors and which are false negatives. If you really cannot avoid multiple tests, then also assess one of the alternatives to Bonferonni’s method, viz: the false discovery rate (FDR) correction, the Holm-Bonferroni correction, Benjamini-Hochberg’s procedure, the Sidak correction, or some of the Bayesian approaches.\n\n\nOr better still, we do an ANOVA that controls for these Type I errors so that it remains at 5%.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#single-factor",
    "href": "basic_stats/08-anova.html#single-factor",
    "title": "8. ANOVA",
    "section": "3.1 Single Factor",
    "text": "3.1 Single Factor\nWe continue with the chicken data. The t-test showed that Diets 1 and 2 resulted in the same chicken mass at Day 21. What about the other two diets? Our H0 is that, at Day 21, \\(\\mu_{1}=\\mu_{2}=\\mu_{3}=\\mu_{4}\\). Is there a statistical difference between chickens fed these four diets, or do we retain the H0? The R function for an ANOVA is aov(). To look for significant differences between all four diets on the last day of sampling we use this one line of code:\n\nchicks.aov1 &lt;- aov(weight ~ Diet, data = filter(chicks, Time == 21))\nsummary(chicks.aov1)\n\nR&gt;             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nR&gt; Diet         3  57164   19055   4.655 0.00686 **\nR&gt; Residuals   41 167839    4094                   \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantTask G.2: Do It Now!\n\n\n\n\nWhat does the outcome say about the chicken masses? Which ones are different from each other?\nDevise a graphical display of this outcome.\n\n\n\nIf this seems too easy to be true, it is because we are not quite done yet. You could use your graphical display to eyeball where the significant differences are, or we can turn to a more ‘precise’ approach. The next step one could take is to run a Tukey HSD test on the results of the ANOVA by wrapping tukeyHSD() around aov():\n\nTukeyHSD(chicks.aov1)\n\nR&gt;   Tukey multiple comparisons of means\nR&gt;     95% family-wise confidence level\nR&gt; \nR&gt; Fit: aov(formula = weight ~ Diet, data = filter(chicks, Time == 21))\nR&gt; \nR&gt; $Diet\nR&gt;          diff        lwr       upr     p adj\nR&gt; 2-1  36.95000  -32.11064 106.01064 0.4868095\nR&gt; 3-1  92.55000   23.48936 161.61064 0.0046959\nR&gt; 4-1  60.80556  -10.57710 132.18821 0.1192661\nR&gt; 3-2  55.60000  -21.01591 132.21591 0.2263918\nR&gt; 4-2  23.85556  -54.85981 102.57092 0.8486781\nR&gt; 4-3 -31.74444 -110.45981  46.97092 0.7036249\n\n\nThe output of tukeyHSD() shows us that pairwise comparisons of all of the groups we are comparing. We can also display this as a very rough figure (Figure 2):\n\nplot(TukeyHSD(chicks.aov1))\n\n\n\n\n\n\n\nFigure 2: A plot of the Tukey-HSD test showing the differences in means between chicks reared to 21 days old and fed four diets.\n\n\n\n\n\nWe may also produce a nicer looking graphical summary in the form of a box-and-whisker plot and/or a violin plot. Here I combine both (Figure 3):\n\nset.seed(666)\n\n## Parametric T-test and Box Plot\nggbetweenstats(\n  data = filter(chicks, Time == 21),\n  x = Diet,\n  y = weight,\n  xlab = \"Diet\",\n  ylab = \"Chick mass (g)\",\n  plot.type = \"box\",\n  boxplot.args = list(notch = TRUE),\n  type = \"parametric\",\n  results.subtitle = FALSE,\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"s\",\n  p.adjust.method = \"bonferroni\",\n  conf.level = 0.95,\n  title = \"ANOVA\",\n  ggtheme = ggthemes::theme_fivethirtyeight(),\n  package = \"basetheme\",\n  palette = \"ink\"\n)\n\n\n\n\n\n\n\nFigure 3: Box-and-whisker plot showing the differences in means between chicks reared to 21 days old and fed four diets. Shown is a notched box plot where the extent of the notches is 1.58 * IQR / sqrt(n). This is approximately equivalent to a 95% confidence interval and may be used for comparing medians.\n\n\n\n\n\n\n\n\n\n\n\nImportantTask G.3: Do It Now!\n\n\n\nLook at the help file for the TukeyHSD() function to better understand what the output means.\n\nHow does one interpret the results? What does this tell us about the effect that that different diets has on the chicken weights at Day 21?\nFigure out a way to plot the Tukey HSD outcomes in ggplot.\nWhy does the ANOVA return a significant result, but the Tukey test shows that not all of the groups are significantly different from one another?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#multiple-factors",
    "href": "basic_stats/08-anova.html#multiple-factors",
    "title": "8. ANOVA",
    "section": "3.2 Multiple Factors",
    "text": "3.2 Multiple Factors\nWhat if we have multiple grouping variables, and not just one? We would encounter this kind of situation in factorial designs. In the case of the chicken data, there is also time that seems to be having an effect.\n\n\n\n\n\n\nImportantTask G.4: Do It Now!\n\n\n\n\nHow is time having an effect? (/3)\nWhat hypotheses can we construct around time? (/2)\n\n\n\nLet us look at some variations around questions concerning time. We might ask, at a particular time step, are there differences amongst the effect due to diet on chicken mass? Let us see when diets are starting to have an effect by examining the outcomes at times 0, 2, 10, and 21:\n\n# effect at time = 0\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 0)))\n\nR&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\nR&gt; Diet         3   4.32   1.440   1.132  0.346\nR&gt; Residuals   46  58.50   1.272\n\n# effect at time = 2\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 2)))\n\nR&gt;             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nR&gt; Diet         3  158.4   52.81   4.781 0.00555 **\nR&gt; Residuals   46  508.1   11.05                   \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# effect at time = 10\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 10)))\n\nR&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nR&gt; Diet         3   8314    2771    6.46 0.000989 ***\nR&gt; Residuals   45  19304     429                     \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# effect at time = 21\nsummary(aov(weight ~ Diet, data = filter(chicks, Time == 21)))\n\nR&gt;             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nR&gt; Diet         3  57164   19055   4.655 0.00686 **\nR&gt; Residuals   41 167839    4094                   \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantTask G.5: Do It Now!\n\n\n\n\nWhat do you conclude from the above series of ANOVAs? (/3)\nWhat problem is associated with running multiple tests in the way that we have done here? (/2)\n\n\n\nOr we may ask, regardless of diet (i.e. disregarding the effect of diet by clumping all chickens together), is time having an effect?\n\nchicks.aov2 &lt;- aov(weight ~ as.factor(Time),\n                   data = filter(chicks, Time %in% c(0, 2, 10, 21)))\nsummary(chicks.aov2)\n\nR&gt;                  Df Sum Sq Mean Sq F value Pr(&gt;F)    \nR&gt; as.factor(Time)   3 939259  313086   234.8 &lt;2e-16 ***\nR&gt; Residuals       190 253352    1333                   \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantTask G.6: Do It Now!\n\n\n\n\nWrite out the hypotheses for this ANOVA. (/2)\nWhat do you conclude from the above ANOVA. (/3)\n\n\n\nOr, to save ourselves a lot of time, reduce the coding effort and we may simply run a two-way ANOVA and look at the effects of Diet and Time simultaneously. To specify the different factors we put them in our formula and separate them with a +:\n\nsummary(aov(weight ~ Diet + as.factor(Time),\n            data = filter(chicks, Time %in% c(0, 21))))\n\nR&gt;                 Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nR&gt; Diet             3  39595   13198   5.987 0.00091 ***\nR&gt; as.factor(Time)  1 734353  734353 333.120 &lt; 2e-16 ***\nR&gt; Residuals       90 198402    2204                    \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantTask G.7: Do It Now!\n\n\n\n\nWhat question are we asking with the above line of code? (/3)\nWhat is the answer? (/2)\nWhy did we wrap Time in as.factor()? (/2)\n\n\n\nIt is also possible to look at what the interaction effect between grouping variables (i.e. in this case the effect of time on diet — does the effect of time depend on which diet we are looking at?), and not just within the individual grouping variables. To do this we replace the + in our formula with *:\n\nsummary(aov(weight ~ Diet * as.factor(Time),\n            data = filter(chicks, Time %in% c(4, 21))))\n\nR&gt;                      Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nR&gt; Diet                  3  40914   13638   6.968 0.000298 ***\nR&gt; as.factor(Time)       1 582221  582221 297.472  &lt; 2e-16 ***\nR&gt; Diet:as.factor(Time)  3  25530    8510   4.348 0.006684 ** \nR&gt; Residuals            86 168322    1957                     \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantTask G.8: Do It Now!\n\n\n\nHow do these results differ from the previous set? (/3)\n\n\nOne may also run a post-hoc Tukey test on these results the same as for a single factor ANOVA:\n\nTukeyHSD(aov(weight ~ Diet * as.factor(Time),\n             data = filter(chicks, Time %in% c(20, 21))))\n\nR&gt;   Tukey multiple comparisons of means\nR&gt;     95% family-wise confidence level\nR&gt; \nR&gt; Fit: aov(formula = weight ~ Diet * as.factor(Time), data = filter(chicks, Time %in% c(20, 21)))\nR&gt; \nR&gt; $Diet\nR&gt;          diff        lwr       upr     p adj\nR&gt; 2-1  36.18030  -9.301330  81.66194 0.1663037\nR&gt; 3-1  90.63030  45.148670 136.11194 0.0000075\nR&gt; 4-1  62.25253  15.223937 109.28111 0.0045092\nR&gt; 3-2  54.45000   3.696023 105.20398 0.0305957\nR&gt; 4-2  26.07222 -26.072532  78.21698 0.5586643\nR&gt; 4-3 -28.37778 -80.522532  23.76698 0.4863940\nR&gt; \nR&gt; $`as.factor(Time)`\nR&gt;           diff       lwr      upr     p adj\nR&gt; 21-20 8.088223 -17.44017 33.61661 0.5303164\nR&gt; \nR&gt; $`Diet:as.factor(Time)`\nR&gt;                 diff        lwr        upr     p adj\nR&gt; 2:20-1:20  35.188235  -40.67378 111.050253 0.8347209\nR&gt; 3:20-1:20  88.488235   12.62622 164.350253 0.0111136\nR&gt; 4:20-1:20  63.477124  -14.99365 141.947897 0.2035951\nR&gt; 1:21-1:20   7.338235  -58.96573  73.642198 0.9999703\nR&gt; 2:21-1:20  44.288235  -31.57378 120.150253 0.6116081\nR&gt; 3:21-1:20  99.888235   24.02622 175.750253 0.0023872\nR&gt; 4:21-1:20  68.143791  -10.32698 146.614563 0.1371181\nR&gt; 3:20-2:20  53.300000  -31.82987 138.429869 0.5234263\nR&gt; 4:20-2:20  28.288889  -59.17374 115.751515 0.9723470\nR&gt; 1:21-2:20 -27.850000 -104.58503  48.885027 0.9486212\nR&gt; 2:21-2:20   9.100000  -76.02987  94.229869 0.9999766\nR&gt; 3:21-2:20  64.700000  -20.42987 149.829869 0.2732059\nR&gt; 4:21-2:20  32.955556  -54.50707 120.418182 0.9377007\nR&gt; 4:20-3:20 -25.011111 -112.47374  62.451515 0.9862822\nR&gt; 1:21-3:20 -81.150000 -157.88503  -4.414973 0.0305283\nR&gt; 2:21-3:20 -44.200000 -129.32987  40.929869 0.7402877\nR&gt; 3:21-3:20  11.400000  -73.72987  96.529869 0.9998919\nR&gt; 4:21-3:20 -20.344444 -107.80707  67.118182 0.9960548\nR&gt; 1:21-4:20 -56.138889 -135.45396  23.176184 0.3619622\nR&gt; 2:21-4:20 -19.188889 -106.65152  68.273738 0.9972631\nR&gt; 3:21-4:20  36.411111  -51.05152 123.873738 0.8984019\nR&gt; 4:21-4:20   4.666667  -85.06809  94.401428 0.9999998\nR&gt; 2:21-1:21  36.950000  -39.78503 113.685027 0.8067041\nR&gt; 3:21-1:21  92.550000   15.81497 169.285027 0.0075185\nR&gt; 4:21-1:21  60.805556  -18.50952 140.120628 0.2629945\nR&gt; 3:21-2:21  55.600000  -29.52987 140.729869 0.4679025\nR&gt; 4:21-2:21  23.855556  -63.60707 111.318182 0.9896157\nR&gt; 4:21-3:21 -31.744444 -119.20707  55.718182 0.9486128\n\n\n\n\n\n\n\n\nImportantTask G.9: Do It Now!\n\n\n\nYikes! That is a massive amount of results. What does all of this mean, and why is it so verbose? (/5)\n\n\n\n\n\n\n\n\n\n\nNoteSummary\n\n\n\nTo summarise t-tests, single-factor (1-way), multifactor (2- or 3-way and etc.) ANOVAs:\n\nA t-test is applied to situations where one wants to compare the means of only two groups of a response variable within one categorical independent variable (we say a factor with two levels).\nA 1-way ANOVA also looks at the means of a response variable belonging to one categorical independent variable, but the categorical response variable has more than two levels in it.\nFollowing on from there, a 2-way ANOVA compares the means of response variables belonging to all the levels within two categorical independent variables (e.g. Factor 1 might have three levels, and Factor 2 five levels). In the simplest formulation, it does so by looking at the main effects, which is the group differences between the three levels of Factor 1, disregarding the contribution due to the group membership to Factor 2 and also the group differences amongst the levels of Factor 2 but disregarding the group membership of Factor 1. In addition to looking at the main effects, a 2-way ANOVA can also consider the interaction (or combined effect) of Factors 1, 2 in influencing the means.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#wilcoxon-rank-sum-test",
    "href": "basic_stats/08-anova.html#wilcoxon-rank-sum-test",
    "title": "8. ANOVA",
    "section": "4.1 Wilcoxon Rank Sum Test",
    "text": "4.1 Wilcoxon Rank Sum Test\nThe non-parametric version of a t-test is a Wilcox rank sum test. To perform this test in R we may again use compare_means() and specify the test we want:\n\ncompare_means(weight ~ Diet,\n              data = filter(chicks, Time == 0,\n                            Diet %in% c(1, 2)),\n              method = \"wilcox.test\")\n\nR&gt; # A tibble: 1 × 8\nR&gt;   .y.    group1 group2     p p.adj p.format p.signif method  \nR&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \nR&gt; 1 weight 1      2      0.235  0.23 0.23     ns       Wilcoxon\n\n\nWhat do our results show?",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#kruskall-wallis-rank-sum-test",
    "href": "basic_stats/08-anova.html#kruskall-wallis-rank-sum-test",
    "title": "8. ANOVA",
    "section": "4.2 Kruskall-wallis Rank Sum Test",
    "text": "4.2 Kruskall-wallis Rank Sum Test\n\n4.2.1 Single factor\nThe non-parametric version of an ANOVA is a Kruskall-Wallis rank sum test. As you may have by now surmised, this may be done with compare_means() as seen below:\n\ncompare_means(weight ~ Diet,\n              data = filter(chicks, Time == 0),\n              method = \"kruskal.test\")\n\nR&gt; # A tibble: 1 × 6\nR&gt;   .y.        p p.adj p.format p.signif method        \nR&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         \nR&gt; 1 weight 0.475  0.48 0.48     ns       Kruskal-Wallis\n\n\nAs with the ANOVA, this first step with the Kruskall-Wallis test is not the last. We must again run a post-hoc test on our results. This time we will need to use pgirmess::kruskalmc(), which means we will need to load a new library.\n\nlibrary(pgirmess)\n\nkruskalmc(weight ~ Diet, data = filter(chicks, Time == 0))\n\nR&gt; Multiple comparison test after Kruskal-Wallis \nR&gt; alpha: 0.05 \nR&gt; Comparisons\nR&gt;     obs.dif critical.dif stat.signif\nR&gt; 1-2    6.95     14.89506       FALSE\nR&gt; 1-3    6.90     14.89506       FALSE\nR&gt; 1-4    4.15     14.89506       FALSE\nR&gt; 2-3    0.05     17.19933       FALSE\nR&gt; 2-4    2.80     17.19933       FALSE\nR&gt; 3-4    2.75     17.19933       FALSE\n\n\nLet us consult the help file for kruskalmc() to understand what this print-out means.\n\n\n4.2.2 Multiple factors\nThe water becomes murky quickly when one wants to perform multiple factor non-parametric comparison of means tests. To that end, we will not cover the few existing methods here. Rather, one should avoid the necessity for these types of tests when designing an experiment.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#the-sa-time-data",
    "href": "basic_stats/08-anova.html#the-sa-time-data",
    "title": "8. ANOVA",
    "section": "4.3 The Sa Time Data",
    "text": "4.3 The Sa Time Data\n\nsa_time &lt;- as_tibble(read_csv(here::here(\"data\", \"BCB744\", \"snakes.csv\"),\n                              col_types = list(col_double(),\n                                               col_double(),\n                                               col_double())))\nsa_time_long &lt;- sa_time %&gt;% \n  gather(key = \"term\", value = \"minutes\") %&gt;% \n  filter(minutes &lt; 300) %&gt;% \n  mutate(term = as.factor(term))\n\nmy_comparisons &lt;- list( c(\"now\", \"now_now\"),\n                        c(\"now_now\", \"just_now\"),\n                        c(\"now\", \"just_now\") )\n\nggboxplot(sa_time_long, x = \"term\", y = \"minutes\",\n          colour = \"term\", palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n          add = \"jitter\", shape = \"term\")\n\n\n\n\nTime is not a limited resource in South Africa.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/08-anova.html#snakes",
    "href": "basic_stats/08-anova.html#snakes",
    "title": "8. ANOVA",
    "section": "5.1 Snakes!",
    "text": "5.1 Snakes!\nThese data could be analysed by a two-way ANOVA without replication, or a repeated measures ANOVA. Here I will analyse it by using a two-way ANOVA without replication.\nPlace and Abramson (2008) placed diamondback rattlesnakes (Crotalus atrox) in a ‘rattlebox,’ a box with a lid that would slide open, shut every 5 minutes. At first and the snake would rattle its tail each time the box opened. After a while, the snake would become habituated to the box opening, stop rattling its tail. They counted the number of box openings until a snake stopped rattling; fewer box openings means the snake was more quickly habituated. They repeated this experiment on each snake on four successive days and which is treated as an influential variable here. Place and Abramson (2008) used 10 snakes, but some of them never became habituated; to simplify this example, data from the six snakes that did become habituated on each day are used.\nFirst, we read in the data, making sure to convert the column named day to a factor. Why? Because ANOVAs work with factor independent variables, while day as it is encoded by default is in fact a continuous variable.\n\nsnakes &lt;- read_csv(here::here(\"data\", \"BCB744\", \"snakes.csv\"))\nsnakes$day = as.factor(snakes$day)\n\nThe first thing we do is to create some summaries of the data. Refer to the summary statistics Chapter.\n\nsnakes.summary &lt;- snakes %&gt;% \n  group_by(day, snake) %&gt;% \n  summarise(mean_openings = mean(openings),\n            sd_openings = sd(openings)) %&gt;% \n  ungroup()\nsnakes.summary\n\nR&gt; # A tibble: 24 × 4\nR&gt;    day   snake mean_openings sd_openings\nR&gt;    &lt;fct&gt; &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;\nR&gt;  1 1     D1               85          NA\nR&gt;  2 1     D11              40          NA\nR&gt;  3 1     D12              65          NA\nR&gt;  4 1     D3              107          NA\nR&gt;  5 1     D5               61          NA\nR&gt;  6 1     D8               22          NA\nR&gt;  7 2     D1               58          NA\nR&gt;  8 2     D11              45          NA\nR&gt;  9 2     D12              27          NA\nR&gt; 10 2     D3               51          NA\nR&gt; # ℹ 14 more rows\n\n\n\n\n\n\n\n\nImportantTask G.9: Do It Now!\n\n\n\n\nSomething seems… off. What is going on here? Please explain this outcome.\n\n\n\nTo fix this problem, let us ignore the grouping by both snake and day.\n\nsnakes.summary &lt;- snakes %&gt;% \n  group_by(day) %&gt;% \n  summarise(mean_openings = mean(openings),\n            sd_openings = sd(openings)) %&gt;% \n  ungroup()\nsnakes.summary\n\nR&gt; # A tibble: 4 × 3\nR&gt;   day   mean_openings sd_openings\nR&gt;   &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\nR&gt; 1 1              63.3        30.5\nR&gt; 2 2              47          12.2\nR&gt; 3 3              34.5        26.0\nR&gt; 4 4              25.3        18.1\n\n\nRmisc::summarySE() offers a convenience function if your feeling less frisky about calculating the summary statistics yourself:\n\nlibrary(Rmisc)\nsnakes.summary2 &lt;- summarySE(data = snakes,\n                             measurevar = \"openings\",\n                             groupvars = c(\"day\"))\nsnakes.summary2\n\nR&gt;   day N openings       sd        se       ci\nR&gt; 1   1 6 63.33333 30.45434 12.432931 31.95987\nR&gt; 2   2 6 47.00000 12.21475  4.986649 12.81859\nR&gt; 3   3 6 34.50000 25.95958 10.597956 27.24291\nR&gt; 4   4 6 25.33333 18.08498  7.383164 18.97903\n\n\nNow we turn to some visual data summaries (Figure 4).\n\nggplot(data = snakes, aes(x = day, y = openings)) +\n  geom_segment(data = snakes.summary2, aes(x = day, xend = day,\n                                           y = openings - ci,\n                                           yend = openings + ci,\n                                           colour = day),\n              size = 2.0, linetype = \"solid\", show.legend = FALSE) +\n  geom_boxplot(aes(fill = day), alpha = 0.3, show.legend = FALSE) + \n  geom_jitter(width = 0.05) +\n  theme_pubclean()\n\n\n\n\n\n\n\nFigure 4: Boxplots showing the change in the snakes’ habituation to box opening over time.\n\n\n\n\n\nWhat are our null hypotheses?\n\nH0 There is no difference between snakes with respect to the number of openings at which they habituate.\nH0 There is no difference between days in terms of the number of openings at which the snakes habituate.\n\nFit the ANOVA model to test these hypotheses:\n\nsnakes.aov &lt;- aov(openings ~ day + snake, data = snakes)\nsummary(snakes.aov)\n\nR&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nR&gt; day          3   4878  1625.9   3.320 0.0487 *\nR&gt; snake        5   3042   608.4   1.242 0.3382  \nR&gt; Residuals   15   7346   489.7                 \nR&gt; ---\nR&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow we need to test of the assumptions hold true (i.e. errors are normally distributed and heteroscedastic) (Figure 5). Also, where are the differences (Figure 6)?\n\npar(mfrow = c(1, 2))\n# Checking assumptions...\n# make a histogram of the residuals;\n# they must be normal\nsnakes.res &lt;- residuals(snakes.aov)\nhist(snakes.res, col = \"red\")\n\n# make a plot of residuals and the fitted values;\n# # they must be normal and homoscedastic\nplot(fitted(snakes.aov), residuals(snakes.aov), col = \"red\")\n\n\n\n\n\n\n\nFigure 5: Exploring the assumptions visually.\n\n\n\n\n\n\nsnakes.tukey &lt;- TukeyHSD(snakes.aov, which = \"day\", conf.level = 0.90)\nplot(snakes.tukey, las = 1, col = \"red\")\n\n\n\n\n\n\n\nFigure 6: Exploring the differences between days.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "8. ANOVA"
    ]
  },
  {
    "objectID": "basic_stats/10-correlations.html#at-a-glance",
    "href": "basic_stats/10-correlations.html#at-a-glance",
    "title": "10. Correlations",
    "section": "1 At a Glance",
    "text": "1 At a Glance\nCorrelation analysis is used to quantify the strength and direction of the linear relationship between two continuous variables. The expectations about the data needed for a correlation analysis are:\n\nContinuous variables Both variables should be measured on a continuous scale (e.g., height, depth, income). Note that we do not have dependent, independent variables as no dependency of one variable upon the other is implied.\nBivariate relationship Correlation analysis is used to assess the relationship between two variables at a time. If you are interested in the relationship between multiple variables, you may need to consider pairwise correlations, or other multivariate techniques such as multiple regression, canonical correlation.\nLinear relationship The relationship between the two variables should be linear. This can be visually assessed using scatter plots. If the relationship is not linear, you may need to consider non-linear correlation measures, such as Spearman’s \\(\\rho\\) correlation or Kendall’s \\(\\tau\\).\nNo outliers Outliers can have a strong influence on the correlation coefficient, potentially leading to misleading conclusions. It is important to visually inspect the data using scatter plots, address any outliers before performing correlation analysis.\nNormality While not strictly required for correlation analysis, the assumption of bivariate normality can be important when making inferences about the population correlation coefficient. If the variables are not normally distributed, have a non-linear relationship or consider using non-parametric correlation measures like Spearman’s \\(\\rho\\) correlation or Kendall’s \\(\\tau\\).\nIndependence of observations The observations should be independent of each other. In the case of time series data or clustered data, this assumption may be violated, requiring specific techniques to account for the dependence (e.g., autocorrelation, cross-correlation).\nRandom sampling The data should be obtained through random sampling, ensuring that each observation has an equal chance of being included in the sample.\n\nKeep in mind that correlation does not imply causation; it only describes the association between variables without establishing a cause-and-effect relationship. When the intention is to model causation you will need to apply a regression.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "basic_stats/10-correlations.html#introduction-to-correlation",
    "href": "basic_stats/10-correlations.html#introduction-to-correlation",
    "title": "10. Correlations",
    "section": "2 Introduction to Correlation",
    "text": "2 Introduction to Correlation\nA correlation is performed when we want to investigate the potential association between two continuous quantitative variables, or between some ordinal variables. We assume that the association is linear, like in a linear regression, and that one variable increases, decreases by a constant amount for a corresponding unit increase or decrease in the other variable. This does not suggest that one variable explains the other — that is the purpose of regression or as seen in Chapter 9. Like all statistical tests, correlation requires a series of assumptions:\n\npair-wise data\nabsence of outliers\nlinearity\nnormality of distribution\nhomoscedasticity\nlevel (type) of measurement\ncontinuous data (Pearson \\(r\\))\nnon-parametric correlations (Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\))",
    "crumbs": [
      "Home",
      "Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "basic_stats/10-correlations.html#pearson-correlation",
    "href": "basic_stats/10-correlations.html#pearson-correlation",
    "title": "10. Correlations",
    "section": "3 Pearson Correlation",
    "text": "3 Pearson Correlation\n\n\nPearson’s \\(r\\):\n\\[r_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} \\tag{1}\\]\nwhere \\(r_{xy}\\) is the Pearson correlation coefficient, \\(x_i\\)\\(y_i\\) are the observed values of the two variables for each observation \\(i\\), \\(\\bar{x}\\)\\(\\bar{y}\\) are the sample means of the two variables,\\(n\\) is the sample size.\nPearson’s \\(r\\) is a measure of the linear relationship between two variables. It assumes that the relationship between the variables is linear, and is calculated as the ratio of the covariance between the variables to the product of their standard deviations (Equation 1).\nThe degree of association is measured by a correlation coefficient, denoted by \\(r\\) (note, in a regression we use the \\(r^{2}\\),\\(R^{2}\\)). The \\(r\\) statistic is a measure of linear association. The value for \\(r\\) varies from -1 to 1, with 0 indicating that there is absolutely no association, 1 showing a perfect positive association, and -1 a perfect inverse correlation.\nIn order to investigate correlations in biological data lets load the ecklonia dataset.\n\n\n\nTable 1\n\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(corrplot)\nlibrary(kableExtra)\n\n# Load data\necklonia &lt;- read.csv(here::here(\"data\", \"BCB744\", \"ecklonia.csv\"))\n\n\n\nWe will also create a subsetted version of our data by removing all of the categorical variables. If we have a dataframe where each column represents pair-wise continuous/ordinal measurements with all of the other columns we may very quickly and easily perform a much wider range of correlation analyses.\n\necklonia_sub &lt;- ecklonia %&gt;%\n  select(-species, - site, - ID)\n\n# order the columns alphabetically\necklonia_sub &lt;- ecklonia_sub[,order(colnames(ecklonia_sub))]\n\nWhen the values we are comparing are continuous, we may use a Pearson test. This is the default, so requires little work on our end. The resulting statistic from this test is known as the Pearson correlation coefficient:\n\n# Perform correlation analysis on two specific variables\n# Note that we do not need the final two arguments in this function to be stated\n# as they are the default settings.\n# They are only shown here to illustrate that they exist.\ncor.test(x = ecklonia$stipe_length, ecklonia$frond_length,\n         use = \"everything\", method = \"pearson\")\n\nR&gt; \nR&gt;  Pearson's product-moment correlation\nR&gt; \nR&gt; data:  ecklonia$stipe_length and ecklonia$frond_length\nR&gt; t = 4.2182, df = 24, p-value = 0.0003032\nR&gt; alternative hypothesis: true correlation is not equal to 0\nR&gt; 95 percent confidence interval:\nR&gt;  0.3548169 0.8300525\nR&gt; sample estimates:\nR&gt;       cor \nR&gt; 0.6524911\n\n\nAbove we have tested the correlation between the length of Ecklonia maxima stipes and the length of their fronds. A perfect positive (negative) relationship would produce a value of 1 (-1), whereas no relationship would produce a value of 0. The result above, cor = 0.65 is relatively strong.\nAs is the case with everything else we have learned thus far, a good visualisation can go a long way to help us understand what the statistics are doing. Below we visualise the stipe length to frond length relationship.\n\n# Calculate Pearson r beforehand for plotting\nr_print &lt;- paste0(\"r = \",\n                  round(cor(x = ecklonia$stipe_length, ecklonia$frond_length),2))\n\n# Then create a single panel showing one correlation\nggplot(data = ecklonia, aes(x = stipe_length, y = frond_length)) +\n  geom_smooth(method = \"lm\", colour = \"blue3\", se = FALSE, size = 1.2) +\n  geom_point(size = 3, col = \"red3\", shape = 16) +\n  geom_label(x = 300, y = 240, label = r_print) +\n  labs(x = \"Stipe length (cm)\", y = \"Frond length (cm)\") +\n  theme_pubclean()\n\n\n\n\n\n\n\nFigure 1: Scatterplot showing relationship between Ecklonia maxima stipe length (cm) and frond length (cm). The correlation coefficient (Pearson r) is shown in the top left corner. Note that the best fit blue line was produced by a linear model and that it is not responsible for generating the correlation coefficient; rather it is included to help visually demonstrate the strength of the relationship.\n\n\n\n\n\nJust by eye-balling this scatterplot it should be clear that these data tend to increase at a roughly similar rate. Our Pearson r value is an indication of what that is.\nShould our dataset contain multiple variables, as ecklonia does, we may investigate all of the correlations simultaneously. Remember that in order to do so we want to ensure that we may perform the same test on each of our paired variables. In this case we will use ecklonia_sub as we know that it contains only continuous data and so are appropriate for use with a Pearson test. By default R will use all of the data we give it and perform a Pearson test so we do not need to specify any further arguments. Note however that this will only output the correlation coefficients, and does not produce a full test of each correlation. This will however be useful for us to have just now.\n\necklonia_pearson &lt;- round(cor(ecklonia_sub), 2)\necklonia_pearson |&gt; \n  kbl(caption = \"A pairwise matrix of the *Ecklonia* dataset.\") %&gt;%\n  kable_classic(full_width = FALSE)\n\n\n\nTable 2: A pairwise matrix of the *Ecklonia* dataset.\n\n\n\n\n\n\n\ndigits\nepiphyte_length\nfrond_length\nfrond_mass\nprimary_blade_length\nprimary_blade_width\nstipe_diameter\nstipe_length\nstipe_mass\n\n\n\n\ndigits\n1.00\n0.05\n0.36\n0.28\n0.10\n0.14\n0.24\n0.24\n0.07\n\n\nepiphyte_length\n0.05\n1.00\n0.61\n0.44\n0.26\n0.41\n0.54\n0.61\n0.51\n\n\nfrond_length\n0.36\n0.61\n1.00\n0.57\n-0.02\n0.28\n0.39\n0.65\n0.39\n\n\nfrond_mass\n0.28\n0.44\n0.57\n1.00\n0.15\n0.36\n0.51\n0.51\n0.47\n\n\nprimary_blade_length\n0.10\n0.26\n-0.02\n0.15\n1.00\n0.34\n0.32\n0.13\n0.16\n\n\nprimary_blade_width\n0.14\n0.41\n0.28\n0.36\n0.34\n1.00\n0.83\n0.34\n0.83\n\n\nstipe_diameter\n0.24\n0.54\n0.39\n0.51\n0.32\n0.83\n1.00\n0.59\n0.82\n\n\nstipe_length\n0.24\n0.61\n0.65\n0.51\n0.13\n0.34\n0.59\n1.00\n0.58\n\n\nstipe_mass\n0.07\n0.51\n0.39\n0.47\n0.16\n0.83\n0.82\n0.58\n1.00\n\n\n\n\n\n\n\n\nHow would we visualise this matrix of correlations? It is relatively straightforward to quickly plot correlation results for all of our variables in one go. In order to show which variables associate most with which other variables all at once, without creating chaos, we will create what is known as a pairwise correlation plot. This visualisation uses a range of colours, usually blue to red, to demonstrate where more of something is. In this case, we use it to show where more correlation is occurring between morphometric properties of the kelp Ecklonia maxima.\n\n# extract the lower triangle and plot\necklonia_pearson[upper.tri(ecklonia_pearson)] &lt;- NA\ncorrplot(ecklonia_pearson, method = \"circle\", na.label.col = \"white\")\n\n\n\n\nPlot of pairwise correlations showing the strength of all correlations between all variables as a scale from red (negative) to blue (positive).\n\n\n\n\nLet us do it is ggplot2 (Figure 2). Here I use the geom_tile() function. However, before I can use the data in ggplot2, I need to create a long dataframe from the correlation matrix, and I can do this with the pivot_longer() function. There are several other methods for plotting pairwise correlations available — please feel free to scratch around the internet for options you like. This graph is called a heatmap, which is not dissimilar to the heatmaps, Hovmöller diagrams created in Chapter 2.\nPairwise correlations are useful for identifying patterns and relationships between variables that may be hidden in the overall correlation structure of the dataset. This is particularly useful in a large dataset with many variables, where this type of analysis — especially when coupled with a suitable visualisation — can help identify subsets of variables that are strongly related to each other, which can then point the path to further analysis, modelling.\n\necklonia_pearson |&gt; \n  as.data.frame() |&gt; \n  mutate(x = rownames(ecklonia_pearson)) |&gt; \n  pivot_longer(cols = stipe_length:epiphyte_length,\n               names_to = \"y\",\n               values_to = \"r\") |&gt; \n  filter(x != \"digits\") |&gt; \n  ggplot(aes(x, y, fill = r)) +\n    geom_tile(colour = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                         midpoint = 0, limit = c(-1, 1),\n                          na.value = \"grey95\",, space = \"Lab\",\n                         name = \"r\") +\n    xlab(NULL) + ylab(NULL) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1,\n                                     hjust = 1)) +\n    coord_fixed() \n\n\n\n\n\n\n\nFigure 2: Pairwise of the Ecklonia dataset correlations created in ggplot2.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "basic_stats/10-correlations.html#spearman-rank-correlation",
    "href": "basic_stats/10-correlations.html#spearman-rank-correlation",
    "title": "10. Correlations",
    "section": "4 Spearman Rank Correlation",
    "text": "4 Spearman Rank Correlation\nSpearman correlation is used to measure the strength and direction of the relationship between two variables, based on their rank order. Unlike Pearson correlation, which assumes that the relationship between two variables is linear, Spearman correlation can be used to measure the strength of any monotonic relationship, whether it is linear, not. Additionally or this correlation is useful even when the data are not normally distributed, or contain outliers.\nTo calculate the Spearman correlation coefficient, \\(\\rho\\), the values of both variables are first ranked from lowest to highest and each value is assigned a numerical rank based on its position in the ordered list. Then, the difference between the ranks of the two variables is calculated for each observation, and the squared differences are summed across all observations. The Spearman correlation coefficient is then calculated as the ratio of the sum of the squared differences to the total number of observations, adjusted for ties (Equation 2). Like the Pearson correlation coefficient, \\(\\rho\\) can also range from -1 to +1.\n\n\nSpearman’s \\(\\rho\\): \\[\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2-1)} \\tag{2}\\]\nwhere \\(\\rho\\) is the Spearman correlation, \\(d_i\\) is the difference between the ranks of the two variables for the \\(i^{th}\\) observation,\\(n\\) is the sample size. The factor of 6 in the equation is a normalisation constant that adjusts the range of possible values of the correlation coefficient to be between -1 and +1.\nIn the code below we will add a column of ordinal data to our ecklonia data to so that we may look at this test.\n\n# Create ordinal data\necklonia$length &lt;- as.numeric(cut((ecklonia$stipe_length + ecklonia$frond_length), breaks = 3))\n\n# What does this new column look like?\nhead(select(ecklonia, c(species, site, stipe_length, frond_length, length)), 10)\n\nR&gt;    species           site stipe_length frond_length length\nR&gt; 1   maxima Boulders Beach          456          116      1\nR&gt; 2   maxima Boulders Beach          477          141      2\nR&gt; 3   maxima Boulders Beach          427          144      1\nR&gt; 4   maxima Boulders Beach          347          127      1\nR&gt; 5   maxima Boulders Beach          470          160      2\nR&gt; 6   maxima Boulders Beach          478          181      2\nR&gt; 7   maxima Boulders Beach          472          174      2\nR&gt; 8   maxima Boulders Beach          459           95      1\nR&gt; 9   maxima Boulders Beach          397           87      1\nR&gt; 10  maxima Boulders Beach          541          127      2\n\n\nNow let us correlate the new length variable with any one of the other variables:\n\ncor.test(ecklonia$length, ecklonia$digits, method = \"spearman\")\n\nR&gt; \nR&gt;  Spearman's rank correlation rho\nR&gt; \nR&gt; data:  ecklonia$length and ecklonia$digits\nR&gt; S = 1930, p-value = 0.08906\nR&gt; alternative hypothesis: true rho is not equal to 0\nR&gt; sample estimates:\nR&gt;       rho \nR&gt; 0.3401765",
    "crumbs": [
      "Home",
      "Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "basic_stats/10-correlations.html#kendall-rank-correlation",
    "href": "basic_stats/10-correlations.html#kendall-rank-correlation",
    "title": "10. Correlations",
    "section": "5 Kendall Rank Correlation",
    "text": "5 Kendall Rank Correlation\nKendall’s correlation, also known as Kendall’s \\(\\tau\\), is a non-parametric correlation method for assessing the strength and direction of the relationship between two variables. It is similar to Spearman’s rank correlation, but it is calculated differently.\nKendall’s \\(\\tau\\) is calculated based on the number of concordant and discordant pairs of observations between the two variables being correlated. A concordant pair is one in which the values of both variables have the same order, meaning that if the value of one variable is higher than the other for one observation, it is also higher for the other observation. A discordant pair is one in which the values of the two variables have different order, meaning that if one variable is higher than the other for one observation, it is lower for the other observation.\n\\(\\tau\\) is calculated as the difference between the number of concordant and discordant pairs of observations, divided by the total number of possible pairs (Equation 3). As in Pearson’s, Spearman’s correlations and the result also ranges from -1 and +1.\n\n\nKendal’s \\(\\tau\\): \\[\\tau = \\frac{n_c - n_d}{\\binom{n}{2}} \\tag{3}\\]\nwhere \\(\\tau\\) is Kendall’s \\(\\tau\\) correlation coefficient, \\(n\\) is the sample size, \\(n_c\\) is the number of concordant pairs of observations, \\(n_d\\) is the number of discordant pairs of observations,\\(\\binom{n}{2}\\) is the number of possible pairs of observations in the sample.\nKendall’s \\(\\tau\\) is a useful correlation statistic for non-parametric data, such as ordinal, categorical data and is robust to outliers and non-normal distributions.\nLet us look at the normality of our ecklonia variables and pull out those that are not normal in order to see how the results of this test may differ from our Pearson tests.\n\necklonia_norm &lt;- ecklonia_sub %&gt;%\n  gather(key = \"variable\") %&gt;%\n  group_by(variable) %&gt;%\n  summarise(variable_norm = as.numeric(shapiro.test(value)[2]))\necklonia_norm\n\nR&gt; # A tibble: 9 × 2\nR&gt;   variable             variable_norm\nR&gt;   &lt;chr&gt;                        &lt;dbl&gt;\nR&gt; 1 digits                     0.0671 \nR&gt; 2 epiphyte_length            0.626  \nR&gt; 3 frond_length               0.202  \nR&gt; 4 frond_mass                 0.277  \nR&gt; 5 primary_blade_length       0.00393\nR&gt; 6 primary_blade_width        0.314  \nR&gt; 7 stipe_diameter             0.170  \nR&gt; 8 stipe_length               0.213  \nR&gt; 9 stipe_mass                 0.817\n\n\nFrom this analysis we may see that the values for primary blade length are not normally distributed. In order to make up for this violation of our assumption of normality we may use the Kendall test.\n\ncor.test(ecklonia$primary_blade_length, ecklonia$primary_blade_width, method = \"kendall\")\n\nR&gt; \nR&gt;  Kendall's rank correlation tau\nR&gt; \nR&gt; data:  ecklonia$primary_blade_length and ecklonia$primary_blade_width\nR&gt; z = 2.3601, p-value = 0.01827\nR&gt; alternative hypothesis: true tau is not equal to 0\nR&gt; sample estimates:\nR&gt;       tau \nR&gt; 0.3426171\n\n\nHere the correlation coefficient is called Kendall’s \\(\\tau\\) but it is interpreted as we would Pearson’s.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "10. Correlations"
    ]
  },
  {
    "objectID": "basic_stats/12-glance.html",
    "href": "basic_stats/12-glance.html",
    "title": "12. Non-Parametric Tests",
    "section": "",
    "text": "In Chapters 7, 8, 9, and 10 we have seen t-tests, ANOVAs, simple linear regressions, and correlations. These tests may be substituted with non-parametric tests if our assumptions about our data fail us. The most commonly encountered non-parametric methods include the following:\n\nWilcoxon rank-sum test The test is used when the two samples being compared are related, meaning that each observation in one sample is paired with a corresponding observation in the other sample. The test is designed to detect whether there is a difference between the paired observations. Specifically, the Wilcoxon signed-rank test ranks the absolute differences between the pairs of observations, and then compares the sum of the ranks for positive differences to the sum of the ranks for negative differences. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the null hypothesis that there is no difference between the paired observations. Use the Wilcoxon test as a non-parametric substitute for a paired sample t-test. See wilcox.test().\nMann-Whitney \\(U\\) test This test is used when the two samples being compared are independent, meaning that there is no pairing between observations in the two samples. The test is designed to detect whether there is a difference between the two groups based on the ranks of the observations. Specifically, the Mann-Whitney \\(U\\) test ranks all observations from both samples, combines the ranks across the two samples, and calculates a test statistic (\\(U\\)) that indicates whether one sample tends to have higher ranks than the other sample. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference between the two groups. Use this test in stead of a one- or two-sample t-test when assumptions of normality or homoscedasticity are not met. See wilcox.test().\nKruskal-Wallis test The Kruskal-Wallis test is a non-parametric statistical test used to compare three or more independent groups on a continuous outcome variable. The test is designed to detect whether there is a difference in the medians of the groups. The Kruskal-Wallis test works by ranking all the observations from all the groups, then calculating a test statistic (\\(H\\)) that measures the degree of difference in the ranked values between the groups. The test produces a p-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference in the medians of the groups. The Kruskal-Wallis test is often used as a non-parametric alternative to the one-way ANOVA. See kruskal.test().\nFriedman test This test is a non-parametric statistical test used to compare three or more related (i.e. not-independent) groups on a continuous outcome variable. The test is designed to detect whether there is a difference in the medians of the groups. The Friedman test works by ranking all the observations within each group, then calculating a test statistic (\\(\\chi^2\\)) that measures the degree of difference in the ranked values between the groups. The test produces a \\(p\\)-value indicating the probability of observing such a difference by chance, assuming the \\(H_0\\) that there is no difference in the medians of the groups. The Friedman test is often used as a non-parametric alternative to the repeated measures ANOVA. You can use the friedman_._test() in the rstatix package or the friedman.test() in Base R.\n\nTables 1 and 2 summarise common parametric and non-parametric statistical tests, along with a brief explanation of each test, the most common R function used to perform the test. Non-parametric tests are robust alternatives to parametric tests when the assumptions of the parametric test are not met. Also provided is additional information on the nature of the independent (IV) and dependent variables (DV) for each test.\n\nTable 1: When our data are normal with equal variances across groups, choose the suitable parametric test\n\n\n\n\n\n\n\n\n\n\nStatistical Test\nExplanation\nVariables\nR Function\nNon-Parametric Substitute\n\n\n\n\nParametric Tests\n\n\n\n\n\n\nPaired-sample t-test\nTests if the difference in means between paired samples is significantly different from zero. Assumes normality and equal variances.\nContinuous (DV)\nt.test(..., var.equal = TRUE)\nWilcoxon signed-rank test\n\n\nStudent’s t-test\nTests if the means of two independent groups are significantly different. Assumes normality and equal variances.\nContinuous (DV) and categorical (IV)\nt.test(..., var.equal = TRUE)\nMann-Whitney U test\n\n\nWelch’s t-test (unequal variances)\nUse this test when data are normal but variances differ between the two groups. It can be used for paired- and two-sample t-tests.\nContinuous (DV) and categorical (IV)\nt.test()\nMann-Whitney U test or Wilcoxon signed-rank test\n\n\nANOVA (one-way ANOVA only; ANOVAs with interactions do not have non-parametric tests)\nTests if the means of three or more independent groups are significantly different. Assumes normality, equal variances, and independence.\nContinuous (DV), categorical (IV)\naov()\nKruskal-Wallis test\n\n\nANOVA with Welch’s approximation of variances\nTests if the means of three or more independent groups are significantly different. Assumes normality but variances may differ.\nContinuous (DV) and categorical (IV)\noneway.test()\nKruskal-Wallis test\n\n\nRegression Analysis\nModels the relationship between two continuous variables. Assumes linearity, normality, and equal variances of errors.\nContinuous (DV), continuous (IV)\nlm()\nGeneralised Linear Models\n\n\nPearson Correlation\nMeasures the strength and direction of the linear relationship between two continuous variables. Assumes normality and linearity.\nContinuous (DV) and continuous (IV)\ncor.test()\nSpearman’s \\(\\rho\\) or Kendall’s \\(\\tau\\) rank correlation\n\n\n\n\nTable 2: Should the data not be normal and/or are heteroscedastic, substitute the parametric test with a non-parametric option.\n\n\n\n\n\n\n\n\n\n\n\nStatistical Test\nExplanation\nVariables\nR Function\nParametric Equivalent\n\n\n\n\nNon-Parametric Tests\n\n\n\n\n\n\nWilcoxon signed-rank test\nTests if the medians of two related samples are significantly different. Does not assume normality.\nContinuous (DV)\nwilcox.test()\nPaired-sample t-test\n\n\nMann-Whitney U test\nTests if the medians of two independent groups are significantly different. Does not assume normality or equal variances.\nContinuous (DV) and categorical (IV)\nwilcox.test()\nStudent’s t-test\n\n\nKruskal-Wallis test\nTests if the medians of three or more independent groups are significantly different. Does not assume normality or equal variances.\nContinuous (DV) and categorical (IV)\nkruskal.test()\nANOVA, or ANOVA with Welch’s approximation of variances\n\n\nFriedman test\nTests if the medians of three or more related samples are significantly different. Use when assumption of independence of data cannot be accepted and data might therefore be non-normal (such as repeated measures or unreplicated full-block design).\nContinuous (DV) and categorical (IV)\nfriedman.test()\nRepeated measures ANOVA\n\n\nSpearman’s rank correlation\nMeasures the strength and direction of the monotonic relationship between two continuous variables. Does not assume normality or linearity.\nContinuous (DV) and continuous (IV)\ncor.test(method = \"spearman\")\nPearson correlation\n\n\n\n\n&lt;!–\nHere is a quick guide to apply to paired tests, one-, two-sample tests and as well as one- and two-sided hypotheses (i.e. t-tests and their ilk). Also see the CheatSheet.\nAssumption | R function | Note |\n|: — — — — — — |: — — — — — — — — — — |: — — — — — — — — — — — — — — — — — –| | Equal variances | t.test(..., var.equal=TRUE) | Student’s t-test | | Unequal variances | t.test(...) | Using Welch’s approximation of variances | | Normal data | t.test(...) | As per equal/unequal variance cases, above | | Data not normal | wilcox.test(...) | Wilcoxon (1-sample) or Mann-Whitney (2-sample) tests |\nWhen we compare more than two groups we usually do an ANOVA, and the same situation is true. For ANOVAs our options include (but are not limited to):\nAssumption | R function | Note |\n|: — — — — — — — — — — — — |: — — — — — — –|: — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -| | Normal data, equal variances | aov(...) | A vanilla analysis of variance | | Normal data, unequal variances | oneway.test(...) | Using Welch’s approximation of variances, if needed, but robust if variances differ no more than 4-fold; could also stabilise variances using a square-root transformation; may also use kruskal.test() | | Data not normal (and/or non-normal) | kruskal.test(...) | Kruskal-Wallis rank sum test |\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {12. {Non-Parametric} {Tests}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/basic_stats/12-glance.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 12. Non-Parametric Tests. http://samos-r.netlify.app/basic_stats/12-glance.html.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "12. Non-Parametric Tests"
    ]
  },
  {
    "objectID": "basic_stats/14-transformations.html#log-transformation",
    "href": "basic_stats/14-transformations.html#log-transformation",
    "title": "14. Data Transformations",
    "section": "1.1 Log Transformation",
    "text": "1.1 Log Transformation\nLog transformation is often applied to positively skewed data. It consists of taking the log of each observation. You can use either base-10 logs (log10(x)) or base-\\(e\\) logs, also known as natural logs (log(x)). It makes no difference for a statistical test whether you use base-10 logs or natural logs, because they differ by a constant factor; the base- 10 log of a number is just 2.303…× the natural log of the number. You should specify which log you are using when you write up the results, as it will affect things like the slope, intercept in a regression. I prefer base-10 logs and because it is possible to look at them and see the magnitude of the original number: \\(log(1) = 0\\), \\(log(10) = 1\\), \\(log(100) = 2\\), etc.\nThe back transformation is to raise 10\\(e\\) to the power of the number; if the mean of your base-10 log-transformed data is 1.43, the back transformed mean is \\(10^{1.43} = 26.9\\) (in R, 10^1.43). If the mean of your base-\\(e\\) log-transformed data is 3.65, the back transformed mean is \\(e^{3.65} = 38.5\\) (in R, exp(3.65)). If you have zeros or negative numbers, you cannot take the log; you should add a constant to each number to make them positive, non-zero (i.e.log10(x + 1)). If you have count data, and some of the counts are zero, the convention is to add 0.5 to each number.\nMany variables in biology have log-normal distributions, meaning that after log-transformation, the values are normally distributed. This is because if you take a bunch of independent factors, multiply them together and the resulting product is log-normal. For example, let us say you have planted a bunch of weed seeds, then 10 years later you see how tall the trees are. The height of an individual tree would be affected by the nitrogen in the soil, the amount of water, amount of sunlight, amount of insect damage, etc. Having more nitrogen might make a tree 10% larger than one with less nitrogen; the right amount of water might make it 30% larger than one with too much, too little water; more sunlight might make it 20% larger; less insect damage might make it 15% larger or etc. Thus the final size of a tree would be a function of nitrogen × water × sunlight × insects, and mathematically, this kind of function turns out to be log-normal.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "basic_stats/14-transformations.html#arcsine-transformation",
    "href": "basic_stats/14-transformations.html#arcsine-transformation",
    "title": "14. Data Transformations",
    "section": "1.2 Arcsine Transformation",
    "text": "1.2 Arcsine Transformation\nArcsine transformation is commonly used for proportions, which range from 0 to 1, or percentages that go from 0 to 100. Specifically, this transformation is quite useful when the data follow a binomial distribution, have extreme proportions close to 0 or 1.\nA biological example of the type of data suitable for arcsine transformation is the proportion of offspring that survives or the proportion of plants that succumbs to a disease; such data often follow a binomial distribution.\nThis transformation involves of taking the arcsine of the square root of a number (in R, arcsin(sqrt(x))). (The result is given in radians, not degrees, and can range from −π/2 to π/2). The numbers to be arcsine transformed must be in the range 0 to 1. […] the back-transformation is to square the sine of the number (in R, sin(x)^2).",
    "crumbs": [
      "Home",
      "Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "basic_stats/14-transformations.html#square-root-transformation",
    "href": "basic_stats/14-transformations.html#square-root-transformation",
    "title": "14. Data Transformations",
    "section": "1.3 Square Root Transformation",
    "text": "1.3 Square Root Transformation\nThe square root transformation (in R, sqrt(x)) is often used to stabilise the variance of data that have a non-linear relationship between the mean and variance (heteroscedasticity). It is effective for reducing right-skewness (positively skewed). Taking the square root of each observation has the effect of compressing the data towards zero and reducing the impact of extreme values. It is a monotonic transformation, which means that it preserves the order of the data, does not change the relative rankings of the observations.\nThe square root transformation does not work with negative values, but one could add a constant to each number to make them positive.\nA square root transformation is most frequently applied where the data are counts or frequencies, such as the number of individuals in a population, the number of events in a certain time period. Count data are prone to the variance increasing with the mean due to the discrete nature of the data. In these cases or the data tend to follow a Poisson distribution, which is characterised by a variance that is equal to the mean. The same applies to some environmental data, such as rainfall, wind; these may also exhibit heteroscedasticity due to extreme weather phenomena.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "basic_stats/14-transformations.html#square-transformation",
    "href": "basic_stats/14-transformations.html#square-transformation",
    "title": "14. Data Transformations",
    "section": "1.4 Square Transformation",
    "text": "1.4 Square Transformation\nAnother transformation available for dealing with heteroscedasticity is the square transformation. As the name suggests, it involves taking the square of each observation in a dataset (x^2). The effect sought is to reduce left skewness.\nThis transformation has the effect of magnifying the differences between values and so increasing the influence of extreme values. However, this can make outliers more prominent, can make it more challenging to interpret the results of statistical analysis.\nThe square transformation is often used in situations where the data are related to areas or volumes, such as the size of cells, the volume of an organ or where the data may follow a nonlinear relationship between the mean and variance.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "basic_stats/14-transformations.html#cube-transformation",
    "href": "basic_stats/14-transformations.html#cube-transformation",
    "title": "14. Data Transformations",
    "section": "1.5 Cube Transformation",
    "text": "1.5 Cube Transformation\nThis transformation also applies to heteroscedastic data. It is sometimes used with moderately left skewed data. This transformation is more drastic than a square transformation, and the drawback are more severe.\nThe cube transformation is less commonly used than other data transformations such as square-root or log transformation. Use with caution.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "basic_stats/14-transformations.html#reciprocal-transformation",
    "href": "basic_stats/14-transformations.html#reciprocal-transformation",
    "title": "14. Data Transformations",
    "section": "1.6 Reciprocal Transformation",
    "text": "1.6 Reciprocal Transformation\nIt involves taking the reciprocal or inverse of each observation in a dataset (1/x). It is another variance stabilising transformation and is used with severely positively skewed data.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "basic_stats/14-transformations.html#anscombe-transformation",
    "href": "basic_stats/14-transformations.html#anscombe-transformation",
    "title": "14. Data Transformations",
    "section": "1.7 Anscombe Transformation",
    "text": "1.7 Anscombe Transformation\nAnother variance stabilising transformation is the Anscombe transformation, sqrt(max(x+1)-x). It is applied to negatively skewed data. This transformation can be used to shift the data and compress it towards zero, and remove the influence of extreme values. It is a monotonic transformation, which means that it preserves the order of the data, does not change the relative rankings of the observations.\nThe Anscombe transformation is useful when dealing with count or frequency data that have a non-linear relationship between the mean and variance; such data are characteristic of Poisson-distributed count data.",
    "crumbs": [
      "Home",
      "Biostatistics",
      "14. Data Transformations"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html",
    "href": "intro_r/01-RStudio.html",
    "title": "1. R and RStudio",
    "section": "",
    "text": "In this Lecture we will cover:",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html#general-settings",
    "href": "intro_r/01-RStudio.html#general-settings",
    "title": "1. R and RStudio",
    "section": "3.1 General Settings",
    "text": "3.1 General Settings\nBefore we start using RStudio (which is a code editor and environment that runs R) let us first set it up properly. Find the ‘Tools’ (‘Preferences’) menu item, navigate to ‘Global Options’ (‘Code Editing’), select the tick boxes as shown in the figure below.\n\n\n\nRStudio preferences",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html#customising-appearance",
    "href": "intro_r/01-RStudio.html#customising-appearance",
    "title": "1. R and RStudio",
    "section": "3.2 Customising Appearance",
    "text": "3.2 Customising Appearance\nRStudio is highly customisable. Under the Appearance tab under ‘Tools’/‘Global Options’ you can see all of the different themes that come with RStudio. We recommend choosing a theme with a black background (e.g., Chaos) as this will be easier on your eyes and your computer. It is also good to choose a theme with a sufficient amount of contrast between the different colours used to denote different types of objects/values in your code.\n\n\n\nAppearance settings",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html#configuring-panes",
    "href": "intro_r/01-RStudio.html#configuring-panes",
    "title": "1. R and RStudio",
    "section": "3.3 Configuring Panes",
    "text": "3.3 Configuring Panes\nYou cannot rearrange panes (see below) in RStudio by dragging them, but you can alter their position via the Pane Layout tab in the ‘Tools’/‘Global Options’ (‘RStudio’/‘Preferences’ — for Mac). You may arrange the panes as you would prefer; however, we recommend that during the duration of this workshop you leave them in the default layout.\n\n\n\nRearranging the panes",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html#source-editor",
    "href": "intro_r/01-RStudio.html#source-editor",
    "title": "1. R and RStudio",
    "section": "6.1 Source Editor",
    "text": "6.1 Source Editor\nGenerally we will want to write programs longer than a few lines. The Source Editor can help you open, edit, execute these programs. Let us open a simple program:\n\nUse Windows Explorer (Finder on Mac) and navigate to the file BONUS/the_new_age.R.\nNow make RStudio the default application to open .R files (right click on the file Name and set RStudio to open it as the default if it is not already)\nNow double click on the file — this will open it in RStudio in the Source Editor in the top left pane.\n\nNote .R files are simply standard text files and can be created in any text editor and saved with a .R (or .r) extension, but the Source editor in RStudio has the advantage of providing syntax highlighting, code completion, and smart indentation. You can see the different colours for numbers, there is also highlighting to help you count brackets (click your cursor next to a bracket and push the right arrow and you will see its partner bracket highlighted). We can execute R code directly from the Source Editor. Try the following (on Macs replace Ctrl with Cmd):\n\nExecute a single line (Run icon or Ctrl+Enter). Note that the cursor can be anywhere on the line and one does not need to highlight anything — do this for the code on line 2\nExecute multiple lines (Highlight lines with the cursor, then Run icon, Ctrl+Enter) — do this for line 3 to 6\nExecute the whole script (Source icon or Ctrl+Shift+Enter)\n\nNow, try changing the x and/or y axis labels on line 18, re-run the script.\nNow let us save the program in the Source Editor by clicking on the file symbol (note that the file symbol is greyed out when the file has not been changed since it was last saved).\nAt this point, it might be worth thinking a bit about what the program is doing. R requires one to think about what you are doing, not simply clicking buttons like in some other software systems which shall remain nameless for now. Scripts execute sequentially from top to bottom. Try, work out what each line of the program is doing and discuss it with your neighbour. Note and if you get stuck, try using R’s help system; accessing the help system is especially easy within RStudio — see if you can figure out how to use that too.\n\n\n\n\n\n\nNoteThe # Symbol\n\n\n\nThe hash (#) tells R not to run any of the text on that line to the right of the symbol. This is the standard way of commenting R code; it is VERY good practice to comment in detail so that you can understand later what you have done.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html#console",
    "href": "intro_r/01-RStudio.html#console",
    "title": "1. R and RStudio",
    "section": "6.2 Console",
    "text": "6.2 Console\nThis is where you can type code that executes immediately. This is also known as the command line. Throughout the notes, we will represent code for you to execute in R as a different font.\n\n\n\n\n\n\nNoteType It In!\n\n\n\nAlthough it may appear that one could copy code from this PDF into the Console, you really should not. The first reason is that you might unwittingly copy invisible PDF formatting codes into R, which will make your script fail. But more importantly, typing code into the Console yourself gives you the practice you need, and allows you to make (and correct) your errors. This is an invaluable way of learning, taking shortcuts now will only hurt you in the long run.\n\n\nEntering code in the command line is intuitive and easy. For example, we can use R as a calculator by typing into the Console (and pressing Enter after each line). Note that the output for every line of calculation (e.g. 6 * 3) is indicated by R&gt;, as we see here:\n\n6 * 3\n\nR&gt; [1] 18\n\n5 + 4\n\nR&gt; [1] 9\n\n2^3\n\nR&gt; [1] 8\n\n\nNote that spaces are optional around simple calculations, but I encourage their use to adhere to the R style guidelines.\nWe can also use the assignment operator &lt;- to assign any calculation to a variable so we can access it later (the = sign would work, too, but it is bad practice to use it; we will talk about this as we go):\n\na &lt;- 2\nb &lt;- 7\na + b\n\nR&gt; [1] 9\n\n\nTo type the assignment operator (&lt;-) push the following two keys together: alt -. There are many keyboard shortcuts in R and we will introduce them as we go along.\nSpaces are also optional around assignment operators. It is good practice to use single spaces in your R scripts, and the alt - shortcut will do this for you automagically. Spaces are not only there to make the code more readable to the human eye, but also to the machine. Try this:\n\nd &lt;- 2\nd &lt; -2\n\nNote that the first line of code assigns d a value of 2, whereas the second statement asks R whether this variable has a value less than 2. When asked, it responds with FALSE. If we had not used spaces, how would R have known what we meant?\nAnother important question here is, is R case sensitive? Is A the same as a? Figure out a way to check for yourself.\nWe can create a vector in R by using the combine c() function:\n\napples &lt;- c(5.3, 3.8, 4.5)\n\nA vector is a one-dimensional array (i.e., a list of numbers), and this is the simplest form of data used in R (you can think of a single value in R as just a very short vector). We will talk about more complex (and therefore more powerful) types of data structures as we go along.\nIf you want to display the value of apples type:\n\napples\n\nR&gt; [1] 5.3 3.8 4.5\n\n\nFinally, there are default functions in R for nearly all basic statistical analyses, including mean() and sd() (standard deviation):\n\nmean(apples)\n\nR&gt; [1] 4.533333\n\nsd(apples)\n\nR&gt; [1] 0.7505553\n\n\n\n\n\n\n\n\nNoteVariable Names\n\n\n\nIt is best not to use c as the name of a value or array. Why? What other words might not be good to use?\n\n\nOr try this:\n\nround(sd(apples), 2)\n\nR&gt; [1] 0.75\n\n\nRStudio supports the automatic completion of code using the Tab key. For example, type the three letters app and then the Tab key. What happens?\nThe code completion feature also provides brief inline help for functions whenever possible. For example, type mean() and press the Tab key.\nThe RStudio Console automagically maintains a ‘history’ so that you can retrieve previous commands, a bit like your Internet browser, Google (see the code in: BONUS/mapping_yourself.Rmd). On a blank line in the Console or press the up arrow, and see what happens.\nIf you wish to review a list of your recent commands and then select a command from this list you can use Ctrl+Up to review the list (Cmd+Up on the Mac). If you prefer a ‘bird’s eye’ overview of the R command history, you may also use the RStudio History pane (see below).\nThe Console title bar has a few useful features:\n\nIt displays the current R working directory (more on this later)\nIt provides the ability to interrupt R during a long computation (a stop sign will appear whilst code is running)\nIt allows you to minimise and maximise the Console in relation to the Source pane using the buttons at the top-right or by double-clicking the title bar)",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html#environment-and-history-panes",
    "href": "intro_r/01-RStudio.html#environment-and-history-panes",
    "title": "1. R and RStudio",
    "section": "6.3 Environment and History Panes",
    "text": "6.3 Environment and History Panes\nThe Environment pane is very useful as it shows you what objects (i.e., dataframes, arrays, values, functions) you have in your environment (workspace). You can see the values for objects with a single value and for those that are longer R will tell you their class. When you have data in your environment that have two dimensions (rows and columns) you may click on them and they will appear in the Source Editor pane like a spreadsheet.\nYou can then go back to your program in the Source Editor by clicking its tab or closing the tab for the object you opened. Also in the Environment is the History tab, where you can see all of the code executed for the session. If you double-click a line, highlight a block of lines and then double-click those or you can send it to the Console (i.e., run them).\nTyping the following into the Console will list everything you have loaded into the Environment:\n\nls()\n\nR&gt; [1] \"a\"               \"apples\"          \"b\"               \"pandoc_dir\"     \nR&gt; [5] \"pkgs_count\"      \"pkgs_lst\"        \"quarto_bin_path\" \"url\"\n\n\nWhat do we have loaded into our environment? Did all of these objects come from one script, or more than one? How can we tell where an object was generated?",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/01-RStudio.html#files-plots-packages-help-and-viewer-panes",
    "href": "intro_r/01-RStudio.html#files-plots-packages-help-and-viewer-panes",
    "title": "1. R and RStudio",
    "section": "6.4 Files, Plots, Packages, Help, and Viewer Panes",
    "text": "6.4 Files, Plots, Packages, Help, and Viewer Panes\nThe last pane has a number of different tabs. The Files tab has a navigable file manager, just like the file system on your operating system. The Plot tab is where graphics you create will appear. The Packages tab shows you the packages that are installed, those that can be installed (more on this just now). The Help tab allows you to search the R documentation for help and is where the help appears when you ask for it from the Console.\nMethods of getting help from the Console include:\n\n?mean\n\n…or:\n\nhelp(mean)\n\nWe will go into this in more detail in the next session.\nTo reproduce Figure Figure 1 in the Plot tab, simply copy, paste the following code into the Console:\n\nlibrary(tidyverse)\nx &lt;- seq(0, 2, by = 0.01)\ny &lt;- 2 * sin(2 * pi * (x - 1 / 4))\nggplot() +\n  geom_point(aes(x = x, y = y), shape = 21, col = \"salmon\", fill = \"white\")\n\n\n\n\n\n\n\nFigure 1: Example plot assembled with ggplot2.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "1. R and RStudio"
    ]
  },
  {
    "objectID": "intro_r/03-quarto.html#introduction-to-r-markdown-in-quarto",
    "href": "intro_r/03-quarto.html#introduction-to-r-markdown-in-quarto",
    "title": "3. R Markdown and Quarto",
    "section": "1 Introduction to R Markdown in Quarto",
    "text": "1 Introduction to R Markdown in Quarto\n\n1.1 What Is Markdown?\nMarkdown is a lightweight markup language that is designed for formatting text in a simple and readable way. It allows you to create formatted documents using a plain text editor, using symbols like # for headings, * or - for bullet lists, and other intuitive shortcuts. This makes it much easier to write well-structured documents compared to traditional word processors, especially for scientific and academic writing.\n\nBasic Markdown Formatting (Quick Reference)\nThe table below summarises the main Markdown formatting options used throughout this course, but you can find a more compelte overview here. The middle column shows the literal Markdown syntax as it would appear in your Quarto source file, while the final column shows how that syntax is rendered in the output document.\n\n\n\nPurpose\nMarkdown source\nRendered output\n\n\n\n\nSection heading (level 1)\n# Introduction\nIntroduction\n\n\nSection heading (level 2)\n## Methods\nMethods\n\n\nEmphasis (italic)\n*italic text*\nitalic text\n\n\nEmphasis (bold)\n**bold text**\nbold text\n\n\nInline code\n`mean(x)`\nmean(x)\n\n\nBullet list\n- first item- second item\n• first item• second item\n\n\nNumbered list\n1. first step2. second step\n1. first step2. second step\n\n\nInline mathematical symbol\n$\\alpha$\nα\n\n\nHyperlink\n[Quarto](https://quarto.org)\nQuarto\n\n\nFigure reference\n@fig-airquality\nFigure reference in text\n\n\nSuperscript\nX^2^\nX2\n\n\nSubrscript\nH~2~O^\nH2O\n\n\n\nMarkdown is intentionally minimal. It is not meant to reproduce the full layout controls of a word processor (although you can!) but to let document structure and emphasis be expressed clearly, consistently, and in a way that remains readable in plain text.\nAs you progress through the course, you will see how this small set of conventions expands easily to longer reports, figures with cross-references, tables, and formal statistical writing.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "3. R Markdown and Quarto"
    ]
  },
  {
    "objectID": "intro_r/03-quarto.html#r-markdown-integrating-code-and-text",
    "href": "intro_r/03-quarto.html#r-markdown-integrating-code-and-text",
    "title": "3. R Markdown and Quarto",
    "section": "2 R Markdown: Integrating Code and Text",
    "text": "2 R Markdown: Integrating Code and Text\nR Markdown is an extension of Markdown that allows you to embed code — such as R, Python, or Julia — directly into your text. This means you can integrate both your narrative (your explanations, interpretation, and discussion) and your code (data analysis, statistics, plots) into a single document. When this document is rendered, both the text and the outputs of your code (including tables and figures) are combined together into a final report.\nR Markdown is very useful in all areas of research because it allows you to:\n\nWrite content in a human-readable format using Markdown\nPrepare transparent, reproducible reports\nEmbed statistically detailed analyses directly alongside your commentary\nEasily incorporate tables, graphics, and other output generated by R into your document\nWrite entire books or theses\nEven this website, Tangled Bank, was written entirely in R Markdown (in Quarto–see below)",
    "crumbs": [
      "Home",
      "Introduction to R",
      "3. R Markdown and Quarto"
    ]
  },
  {
    "objectID": "intro_r/03-quarto.html#using-r-markdown-in-quarto",
    "href": "intro_r/03-quarto.html#using-r-markdown-in-quarto",
    "title": "3. R Markdown and Quarto",
    "section": "3 Using R Markdown in Quarto",
    "text": "3 Using R Markdown in Quarto\nQuarto is a modern open-source scientific and technical publishing system. It is essentially the successor to the older R Markdown system, and supports a range of programming languages in addition to R.\nQuarto follows from an older tradition of literate programming that runs from Donald Knuth’s original formulation (as TeX and later LaTeX), through Sweave and knitr, through to today’s notebook-based systems that interleave narrative, code chunks for computation, and output. Quarto is distinguished from other ways of doing similar things in that it formalises this workflow into an versatile and deliberately practical publishing system that aids the user to do reproducible research. Quarto is built around idealised assumptions about how analyses should be structured, how results should be regenerated, and how documents should circulate across formats and platforms. The apparent strictness of elements such as YAML syntax, indentation, and resource embedding is what makes this possible.\nA basic R Markdown (as implemented in Quarto) document has three main elements:\n\nYAML Header: YAML stands for “Yet Another Markdown Language”. It is at the very top of every Quarto document, enclosed by three dashes ---, specifying basic document metadata (such as title, author, output format). The YAML header is deliberately strict because it is a whitespace-sensitive language. Its structure is inferred from indentation and line position rather than from explicit delimiters, which means that seemingly minor formatting changes can alter how the header is parsed. In practice, this has several consequences that are worth keeping in mind when you are writing or modifying a YAML block:\n\nthe opening and closing dashes are part of the syntax and must be present;\nentries must follow the expected key — value structure;\nindentation marks hierarchical relationships between options — each deeper level in the hierarchy is accomplished by an additional two spaces; and\nblank lines or inconsistent spacing are not inherently “wrong”, but they can change how the parser interprets the document and may therefore lead to errors that are difficult to diagnose.\n\nNarrative Text: Written in Markdown, supporting headings, lists, emphasis, tables, and more\nCode Chunks: Segments of code embedded in the narrative and enclosed using triple back ticks with curly braces indicating the language — e.g., ```{r} at the start and ``` at the end of the chunk for R to insert tables, figures, and other outputs from an R analysis directly into your document\n\nRemember, Quarto documents are not containers into which results are placed after the fact. They are structured workflows in which the narrative determines analytical order and code exists to support specific claims. Figures, tables, and statistical output are therefore generated exactly where they are needed to justify, illustrate, or qualify what is being said.\nThe example below introduces several Quarto features at once. This reflects how experienced users tend to work, but it can obscure the learning sequence for newcomers. To make the structure of a Quarto document easier to grasp, think of the document as something that is built up in layers, with each layer adding a new capability.\n\nAt its simplest, a Quarto document requires only three components to render successfully: a minimal YAML header that specifies the document type, plain Markdown text to provide structure and explanation, and at least one executable code chunk. If this stripped-down document renders without error, you have established that your environment is correctly configured and that the basic relationship between text, code, and output is working as intended.\nOnce this foundation is secure, additional structure can be introduced. Figure labels, captions, and chunk options extend the document from a sequence of outputs into a navigable analytical narrative (or even a full-blown scientific paper of a thesis). Labels allow figures to be referenced explicitly from the text, captions situate outputs interpretively, and chunk options control how results are displayed. These additions do not change the logic of the analysis; they refine how that logic is communicated.\nOnly after this framework is in place does it make sense to introduce citations and bibliographic metadata. Citations depend on external files and additional YAML configuration, and they presuppose that the document itself already renders cleanly. Introducing them last reflects how they function in practice: as an overlay that connects your analysis to the scholarly record, rather than as a prerequisite for computation.\n\nRead the full example with this sequence in mind. You are not expected to absorb every feature at once. Instead, focus first on understanding what is minimally required to produce a working document, then on how Quarto supports clearer presentation, and finally on how it integrates formal scholarly conventions such as citation and cross-referencing.\n---\ntitle: \"R Markdown and Quarto Demo\"\nauthor: \"AJ Smit\"\ndate: \"29/07/2025\"\nbibliography: ../references.bib\ncitation: true\ncsl: styles/marine-biology.csl\nformat: \n  html:\n    code-fold: false\n    embed-resources: true\n    number-depth: 3\n    number-sections: true\n  docx: default\n---\n\n## Introduction\nThis study is about air quality.\n\n## Methods\n### Data\n\nThe dataset used in this study is the `airquality` dataset from R, which contains daily\nair quality measurements in New York from May to September 1973. The dataset includes\nvariables such as ozone levels, solar radiation, wind speed, and temperature.\n\n### Analysis\n\nThe R script in the code chunk further explores the impact of temperature on ozone level.\nAll analyses were done in R [@R2025].\n\nThis is **bold** text. This is *italicised* text.\n\n```{r}\n#| label: fig-airquality\n#| fig-width: 6\n#| fig-height: 4\n#| fig-cap-location: bottom\n#| fig-cap: \"Temperature and ozone level.\"\n#| warning: false\n\nlibrary(**ggplot2**)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n```\n\n## Results\nThe results show that air has quality (@fig-airquality).\n\n## Discussion\nWe used R for the analyses [@R2017]. The results confirm that of Schlegel [-@Schlegel2017].\nNotes:\n\nLines 1-14: YAML header\nLine 16: A level 2 header\nLine 22: A level 3 header\nLine 35-48: An R code chunk\n\nIn the above example, Lines 1-14 are called the YAML header, which contains metadata about the document. Initially, you’ll not want to include the YAML lines bibliography: ../references.bib and citation: true since you will not have a bibliography file set up yet. The citation: true option is used to enable citations in the document, and you may read more about it elsewhere. A very important part of the YAML header is the statement embed-resources: true which ensures that any images or other resources such as the theming styles etc. used in the document are embedded directly into the HTML output, making it self-contained and portable — this is essential when you want to share your document with others, publish it online, or submit it for grading.\nLines 16-33 are the first two sections (two level one headings, “Introduction” and “Methods”, the latter with two level three headings beneath is, i.e., “Data” and “Analysis”) of the document.\nLines 35-48 make the code chunk, which is where we embed our R code. The #| fig-cap option in the code chunk specifies the caption for the figure that will be generated, and you can cross reference this from the body (see Line 52) text using @fig-airquality — this references the name specified by #| label: fig-airquality. The code chunk itself generates a figure that shows the relationship between temperature and ozone level in the airquality dataset. Notice also how I have cited a reference, @R2025, which is a reference to the R software itself, which is specified in the bibliography file references.bib (which you will need to create in your own time).\n\n\n\n\n\n\nImportantDo This Now\n\n\n\n\nGo to the RStudio menu and find ‘File’ &gt; ‘New File’ &gt; ‘Quarto Document’ and create your own Quarto file.\nSave it using a descriptive name inside of your R Project directory.\nCopy and paste the example skeleton into your own, fresh Quarto file.\nMake a few edits to your taste, such as at least changing the author’s name.\nRender it (via the ‘Render’ button above the Code Editor).\n\n\n\nAfter you’ve rendered this file, you’ll see the following output (the HTML output shown):\n\n\n\nThe HTML output of the above Quarto document.\n\n\n\n3.1 Supported output formats\nBy changing the format option in the YAML header, you can export your report to different types including:\n\nPDF documents (provided you have LaTeX installed)\nHTML web pages\nWord (.docx) documents\n\nFor example:\nformat: pdf\nor\nformat: docx\n\n\n3.2 Rendering the document\n\nIn RStudio or Positron, you can click the ‘Render’ or ‘Preview’ button, respectively, to produce your desired output.\nYou can also use the command line: $ quarto render my_file.qmd$\n\nEach time a document is rendered, Quarto re-executes the entire analysis from top to bottom in a clean session. This process tests:\n\nwhether objects are created in the order you assume,\nwhether variables exist where you think they do, and\nwhether your results really follow from the code as written rather than from residual state in the interactive environment.\n\nSo, rendering early and often is a useful and time-saving habit. Errors encountered during rendering are signs that direct you to hidden dependencies, misplaced assumptions about object scope, or inconsistencies between narrative and computation. A plot that appears correctly in the console but fails during rendering may reveal that it relies on objects created interactively rather than within the document itself. Similarly, warnings or failures triggered only at render time frequently indicate that code chunks are implicitly dependent on earlier chunks in ways that are not yet explicit.\nThus, rendering is part of analysis and it brings analytical claims, code structure, and computational order into alignment. Develop the habit of rendering regularly. This helps ensure that your document remains executable, interpretable, and resilient to change. This practice will become increasingly important in later sections of the course, where analyses grow longer. In real life, projects may become more modular, and assumptions that once remained implicit must be made visible and defensible.\nYou will thank me later for my insistence to perform frequent rendering when it comes to tests and exams!\n\n\n\n\n\n\nNoteWhat Went wrong?\n\n\n\nA common early error occurs when a document that previously rendered successfully begins to fail after a small, seemingly innocent edit to the YAML header. For example, placing a blank line between two entries, or misaligning an option such as format: by a single space, can cause Quarto to misinterpret the document structure. The resulting error messages may be indirect, such as complaints about missing formats, unknown options, or failures later in the render process. This is because the problem does not sit with the analysis, but with how the YAML metadata was parsed.\nWhen this happens, the most effective response is to compare the YAML carefully against a known working example and restore its structural alignment. In practice, many YAML issues are resolved by undoing well-intentioned formatting “tidy-ups” that the parser does not understand.\n\n\nThe restrictions you encounter here (minimal YAML headers, limited citation use, and carefully staged document structure) are not endpoints. The are the framework. In later sections of the course, these same elements are revisited and extended: bibliographies and cross-referencing become central to statistical reporting, YAML options expand document capabilities and format, and Quarto documents evolve from simple reports into modular, project-level artefacts.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "3. R Markdown and Quarto"
    ]
  },
  {
    "objectID": "intro_r/03-quarto.html#more-detailed-information",
    "href": "intro_r/03-quarto.html#more-detailed-information",
    "title": "3. R Markdown and Quarto",
    "section": "4 More Detailed Information",
    "text": "4 More Detailed Information\nPlease refer to the Markdown Basics page on the Quarto website for much more information about Markdown.\nQuarto is extremely powerful and you’ll want to explore the Markdown Basics page thoroughly in your own time. Of immediate interest to most of you will be the page on Citations, or the other information under “Authoring” that you may access on the Quarto Guide.\nYou will also have to explore the various YAML options. You can specify these at the beginning of your document in the YAML block at the top, which allows you to define various options for how your HTML, Word document, PDF, or any of the many formats that Quarto can produce, will look. Please consult the reference section on the Quarto website for the various YAML options available (e.g., here the HTML YAML options), so that you can set up your document in the way you would like it to appear.\nBe reminded that YAML is very particular about the way the various levels of indentation must appear in order for the it to be read correctly by your Quarto system and for the code to execute correctly. So, this is an excellent opportunity for you to pay attention to detail and ensure that your YAML is precisely structured according to the expectations of the example document provided.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "3. R Markdown and Quarto"
    ]
  },
  {
    "objectID": "intro_r/03-quarto.html#reproducibility-as-practice",
    "href": "intro_r/03-quarto.html#reproducibility-as-practice",
    "title": "3. R Markdown and Quarto",
    "section": "5 Reproducibility as Practice",
    "text": "5 Reproducibility as Practice\nReproducibility, in the context of Quarto and R Markdown, is a disciplined workflow. Quarto supports reproducible analysis by requiring that code, narrative, and outputs be generated together from a single source document, and so reduces the scope for undocumented intermediate steps or post hoc modification of results. When a document is rendered, all analyses are re-executed in sequence, forcing dependencies, object creation, and analytical order to be made explicit. This makes hidden assumptions (e.g., about data preparation, parameter choices, or plotting defaults) visible in a way that conventional copy-and-paste workflows do not.\nAt the same time, reproducibility remains contingent. It depends on stable input data, explicit control of randomness, and awareness of the computer system on which the document is rendered. Quarto does not freeze package versions, manage software dependencies, or archive external data sources on your behalf (but you can make it do so with some know-how). It provides instead a structured setting in which such practices can be adopted consistently. So, Quarto should be seen as an enabling infrastructure… it makes reproducible work tractable, inspectable, and shareable, but it does not absolve the user from methodological responsibility. Treating reproducibility as an active practice (one that must be maintained rather than assumed) is implicit to using Quarto well.\nWe will cover reproducibility in some more detail under section 2. Working with Data & Code.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "3. R Markdown and Quarto"
    ]
  },
  {
    "objectID": "intro_r/03-quarto.html#why-use-r-markdown",
    "href": "intro_r/03-quarto.html#why-use-r-markdown",
    "title": "3. R Markdown and Quarto",
    "section": "6 Why Use R Markdown?",
    "text": "6 Why Use R Markdown?\nUsing R Markdown within Quarto allows your analysis to be conducted, documented, and communicated within a single, coherent structure. Data preparation, statistical analysis, figures, and interpretation are written together, rather than assembled after the fact from disconnected sources or from memory. This integration makes analytical decisions visible and obvious, both to others and to yourself when you return to the work weeks or months later. Because results are regenerated directly from code at render time, documents remain reusable: the same analysis can be re-run on updated data, revised parameters, or alternative subsets without reconstructing the surrounding narrative. These properties will become increasingly important in the next chapter, where we shift from individual scripts to project-based workflows and begin treating analyses as evolving objects rather than one-off tasks.\nThe value of this approach becomes clearer when compared to a more ad-hoc way of working. Think about an analysis in which data are cleaned interactively in the console (or, shock-horror, in a spreadsheet!), figures are produced through incremental trial-and-error, and only the final plots are copied into a word processor alongside earlier descriptions of what was done. In such cases, the analytical pathway exists mostly in the user’s memory. Intermediate decisions are discarded and reproducing the analysis (even by the original author) requires reconstruction rather than execution. Small changes in data or software can change results without leaving an obvious trace and make it difficult to diagnose differences or defend analytical choices.\nThe table below summarises these contrasts schematically:\n\n\n\n\n\n\n\n\nAspect\nQuarto-based workflow\nAd-hoc workflow\n\n\n\n\nCode and narrative\nWritten together in a single source document\nSeparated across scripts, consoles, and documents\n\n\nGeneration of results\nRe-executed automatically on render\nManually copied and pasted\n\n\nAnalytical order\nExplicit and enforced\nImplicit and often undocumented\n\n\nReuse with new data\nStraightforward re-rendering\nRequires partial or full reconstruction\n\n\nTransparency\nDecisions visible in code\nDecisions inferred after the fact\n\n\nSuitability for collaboration\nHigh\nLimited\n\n\n\nQuarto-oriented workflows are also designed to align smoothly with the material introduced later on workflow management and version control. When analyses live inside a structured project directory and are written as executable documents, they can be tracked over time using tools such as Git, which record how files change rather than just storing their older versions and their final state. You do not need to master version control immediately for this to be useful. Even a basic awareness that your analysis has a history (that it can be inspected, compared, and, if necessary, revisited) reinforces good analytical habits. Reproducibility now extends beyond the moment of rendering a document. It becomes a property of the entire analytical lifecycle, linking day-to-day coding practice with longer-term standards of transparency and accountability that underpin credible quantitative work.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "3. R Markdown and Quarto"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#preparing-data-for-r",
    "href": "intro_r/05-workflow.html#preparing-data-for-r",
    "title": "5. R Workflows",
    "section": "3.1 Preparing Data for R",
    "text": "3.1 Preparing Data for R\nImporting data can actually take longer than the statistical analysis itself! In order to avoid as much frustration as possible it is important to remember that for R to be able to analyse your data they need to be in a consistent format, with each variable in a column, each sample in a row. The format within each variable (column) needs to be consistent and is commonly one of the following types: a continuous numeric variable (e.g., and fish length (m): 0.133, 0.145); a factor or categorical variable (e.g., Month: Jan, Feb and 1, 2, …, 12); a nominal variable (e.g., algal colour: red, green, brown); or a logical variable (i.e., TRUE and FALSE). You can also use other more specific formats such as dates and times, and more general text formats.\nYou will learn more about working with data in R — specifically, you will teach you about the tidyverse principles, the distinction between long and wide format data in more detail on Day 4. For most of our work in R you require our data to be in the long format but Excel users (poor things!) are more familiar with data stored in the wide format.\n(The problem with Excel is that it without warning alters data and obscures provenance, making analytical state difficult to reconstruct.)\nFor now let us bring some data into R and not worry too much about the data being tidy.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#converting-data",
    "href": "intro_r/05-workflow.html#converting-data",
    "title": "5. R Workflows",
    "section": "3.2 Converting Data",
    "text": "3.2 Converting Data\nBefore you can read in the Laminaria dataset provided for the following exercises, you need to convert the Excel file supplied into a .csv file. Open laminaria.xlsx in Excel, then select ‘Save As’ from the File menu. In the ‘Format’ drop-down menu, select the option called ‘Comma Separated Values’, then hit ‘Save’. You will get a warning that formatting will be removed, that only one sheet will be exported; simply ‘Continue’. Your working directory should now contain a file called laminaria.csv.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#importing-data",
    "href": "intro_r/05-workflow.html#importing-data",
    "title": "5. R Workflows",
    "section": "3.3 Importing Data",
    "text": "3.3 Importing Data\nAt this point, successful data import depends on whether the workflow has a fixed point of reference on the file system.\nThe easiest way to import data into R is by changing your working directory to be the same as the file path where the file(s) are you want to load. A file path is effectively an address. In most operating systems, if you open the folder where your files are you may click on the navigation bar, it will show you the complete file path. Many people develop the nasty habit of squirrelling away their files within folders within folders within folders within folders… within folders within folders. Please do not do that.\nThe concept of file paths is either one that you are familiar with, or you have never heard of before. There tends to be little middle ground. Happily, RStudio allows us to circumvent this issue. You do this by using the Intro_R_Workshop.Rproj that you may find in the files downloaded for this workshop. If you have not already switched to the Intro_R_Workshop.Rproj as outlined in Chapter 2, click on the project button in the top right corner your RStudio window. Then navigate to where you saved Intro_R_Workshop.Rproj and select it. Notice that your RStudio has changed a bit and all of the objects you may have previously created in your environment have been removed and any tabs in the source editor pane have been closed. That is fine for now, but it may mean you need to re-open the Day_1.R script you just created.\nOnce you have the working directory set, either by doing it manually with setwd() (but refrain from doing this unless absolutely necessary) or by loading a project, R will now know where to look for the files you want to read. The function read_csv() is the most convenient way to read in raw data. There are several other ways to read in data, but for the purposes of this workshop we will stick to this one, for now. To find out what it does, you will go to its help entry in the usual way (i.e., ?read_csv).\nAll R Help items are in the same format. A short Description (of what it does), Usage, Arguments (the different inputs it requires), Details (of what it does), Value (what it returns), Examples. Arguments (the parameters that are passed to the function) are the lifeblood of any function and as this is how you provide information to R. You do not need to specify all arguments, as most have appropriate default values for your requirements, and others might not be needed for your particular case.\n\n\n\n\n\n\nNoteData Formats\n\n\n\nR has pedantic requirements for naming variables. It is safest to not use spaces, special characters (e.g., commas, semicolons, any of the shift characters above the numbers), or function names (e.g., mean). One can use ‘camelCase’, such as myFirstVariable, or simply separate the ‘parts’ of the variable name using an underscore such as in my_first_variable. Always make sure to use meaningful names; eventually you will learn to find a balance between meaningfulness and something short that is easy enough to retype repeatedly (although R’s ability to use tab completion helps with not having to type long names to often).\n\n\n\n\n\n\n\n\nNoteImport\n\n\n\nread_csv() is simply a ‘wrapper’ (i.e., a command that modifies) a more basic command called read_delim(), which itself allows you to read in many types of files besides .csv. To find out more, type ?read_delim().",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#loading-a-file",
    "href": "intro_r/05-workflow.html#loading-a-file",
    "title": "5. R Workflows",
    "section": "3.4 Loading a File",
    "text": "3.4 Loading a File\nTo load the laminaria.csv file you created, and assign it to an object name in R, you will use the read_csv() function from the tidyverse package, so let us make sure it is activated.\n\nlibrary(tidyverse)\n\nDepending on the version of Excel you are using, or perhaps the settings within it, the laminaria.csv file you created may be corrupted in different ways. Generally Excel likes to replace the , between columns in our .csv files with ;. This may seem like a triviality but sadly it is not. Lucky for use, the tidyverse knows about this problem, they have made a plan. Please open yourlaminaria.csv file and look at which character is being used to separate columns. If it is , then you will load the data with read_csv(). If the columns are separated with ; you will use read_csv2().\n\n# Run this if 'laminaria.csv` has columns separated by ','\nlaminaria &lt;- read_csv(here::here(\"data\", \"BCB744\", \"laminaria.csv\"))\n# Run this if 'laminaria.csv` has columns separated by ';'\nlaminaria &lt;- read_csv2(here::here(\"data\", \"BCB744\", \"laminaria.csv\"))\n\n\n\n\n\n\n\nNoteUsing here::here()\n\n\n\nhere::here() builds a file path from the project root, so you do not have to set or guess the working directory. It exists to make your code portable: the same script works on different computers as long as the project folder stays intact.\nThe default way is to give a plain path string, for example \"data/BCB744/laminaria.csv\". here::here() just constructs that path for you. So:\n\nhere::here(\"data\", \"BCB744\", \"laminaria.csv\")\nis equivalent to \"data/BCB744/laminaria.csv\"\n\nIf you prefer the default style, you can replace any here::here(...) call with the plain path string it would build.\n\n\nThe workflow now depends on a single named object in memory. Moving onward, all results depend on the contents, structure, and provenance of laminaria.\nIf one clicks on the newly created laminaria object in the Environment pane it will open a new panel that shows the information as a spreadsheet. To go back to your script click the appropriate tab in the Source Editor pane. With these data loaded you may now perform analyses on them.\nAt any point when working in R, you can see exactly what objects are in memory in several ways. First, you can look at the Environment tab in RStudio, then Workspace Browser. Alternatively you can type either of the following:\n\nls()\n# or\nobjects()\n\nYou can delete an object from memory by specifying the rm() function with the name of the object:\n\nrm(laminaria)\n\nThis will of course delete our variable, so you will import it in again using whichever of the following two lines of code matched our Excel situation. The workflow has now lost access to its primary data object. Any subsequent analysis thus depends on recreating it by important the data again.\n\nlaminaria &lt;- read.csv(here::here(\"data\", \"BCB744\", \"laminaria.csv\"))\n\n\n\n\n\n\n\nNoteManaging Variables\n\n\n\nIt is good practice to remove variables from memory that you are not using, especially if they are large.\n\n\nThe workflow has now reached an important point where the analysis depends, for the first time in the workflow, on a single, named object whose contents, structure, and provenance (origin, history) must remain stable for subsequent steps to be interpretable.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#examine-your-data",
    "href": "intro_r/05-workflow.html#examine-your-data",
    "title": "5. R Workflows",
    "section": "4.1 Examine Your Data",
    "text": "4.1 Examine Your Data\nOnce the data are in R, you need to check there are no glaring errors. It is useful to call up the first few lines of the dataframe using the function head(). Add the following lines to your script and run them:\n\nhead(laminaria)\n\nR&gt; # A tibble: 6 × 12\nR&gt;   region site        Ind blade_weight blade_length blade_thickness stipe_mass\nR&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\nR&gt; 1 WC     Kommetjie     2         1.9           160            2          1.5 \nR&gt; 2 WC     Kommetjie     3         1.5           120            1.4        2.25\nR&gt; 3 WC     Kommetjie     4         0.55          110            1.5        1.15\nR&gt; 4 WC     Kommetjie     5         1             159            1.5        2.6 \nR&gt; 5 WC     Kommetjie     6         2.3           149            2         NA   \nR&gt; 6 WC     Kommetjie     7         1.6           107            1.75       2.9 \nR&gt; # ℹ 5 more variables: stipe_length &lt;dbl&gt;, stipe_diameter &lt;dbl&gt;, digits &lt;dbl&gt;,\nR&gt; #   thallus_mass &lt;dbl&gt;, total_length &lt;dbl&gt;\n\n\nThis lists the first six lines of each of the variables in the dataframe as a table. You can similarly retrieve the last six lines of a dataframe by an identical call to the function tail(). Of course, this works better when you have fewer than 10, so variables (columns); for larger data sets or things can get a little messy. If you want more or fewer rows in your head or tail, tell R how many rows it is you want by adding this information to your function call. Add the following lines to your script and run them:\n\nhead(laminaria, n = 3)\n\nR&gt; # A tibble: 3 × 12\nR&gt;   region site        Ind blade_weight blade_length blade_thickness stipe_mass\nR&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\nR&gt; 1 WC     Kommetjie     2         1.9           160             2         1.5 \nR&gt; 2 WC     Kommetjie     3         1.5           120             1.4       2.25\nR&gt; 3 WC     Kommetjie     4         0.55          110             1.5       1.15\nR&gt; # ℹ 5 more variables: stipe_length &lt;dbl&gt;, stipe_diameter &lt;dbl&gt;, digits &lt;dbl&gt;,\nR&gt; #   thallus_mass &lt;dbl&gt;, total_length &lt;dbl&gt;\n\ntail(laminaria, n = 2)\n\nR&gt; # A tibble: 2 × 12\nR&gt;   region site         Ind blade_weight blade_length blade_thickness stipe_mass\nR&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\nR&gt; 1 WC     Rocky Bank    12          2.1          194             1.4       3.75\nR&gt; 2 WC     Rocky Bank    13          1.3          160             1.9       2.45\nR&gt; # ℹ 5 more variables: stipe_length &lt;dbl&gt;, stipe_diameter &lt;dbl&gt;, digits &lt;dbl&gt;,\nR&gt; #   thallus_mass &lt;dbl&gt;, total_length &lt;dbl&gt;\n\n\nYou can also check the structure of your data by using the glimpse() function:\n\nglimpse(laminaria)\n\nR&gt; Rows: 140\nR&gt; Columns: 12\nR&gt; $ region          &lt;chr&gt; \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", \"WC\", …\nR&gt; $ site            &lt;chr&gt; \"Kommetjie\", \"Kommetjie\", \"Kommetjie\", \"Kommetjie\", \"K…\nR&gt; $ Ind             &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 10, 11, 1, 3, 4, 5, 6, 7, 8, 9, 1…\nR&gt; $ blade_weight    &lt;dbl&gt; 1.90, 1.50, 0.55, 1.00, 2.30, 1.60, 0.65, 0.95, 2.30, …\nR&gt; $ blade_length    &lt;dbl&gt; 160, 120, 110, 159, 149, 107, 104, 111, 178, 145, 146,…\nR&gt; $ blade_thickness &lt;dbl&gt; 2.00, 1.40, 1.50, 1.50, 2.00, 1.75, 2.00, 1.25, 2.50, …\nR&gt; $ stipe_mass      &lt;dbl&gt; 1.50, 2.25, 1.15, 2.60, NA, 2.90, 0.75, 1.60, 4.20, 0.…\nR&gt; $ stipe_length    &lt;dbl&gt; 120, 149, 97, 167, 146, 161, 110, 136, 176, 82, 118, 1…\nR&gt; $ stipe_diameter  &lt;dbl&gt; 56.0, 68.5, 69.0, 60.0, 73.0, 63.0, 51.0, 56.0, 76.0, …\nR&gt; $ digits          &lt;dbl&gt; 12, 12, 13, 8, 15, 17, 11, 11, 8, 19, 20, 23, 20, 24, …\nR&gt; $ thallus_mass    &lt;dbl&gt; 3000, 3750, 1700, 3600, 5100, 4500, 1400, 2550, 6500, …\nR&gt; $ total_length    &lt;dbl&gt; 256, 269, 207, 326, 295, 268, 214, 247, 354, 227, 264,…\n\n\nThis very handy function lists the variables in your dataframe by name, tells you what sorts of data are contained in each variable (e.g., continuous number, discrete factor), provides an indication of the actual contents of each.\nIf you wanted only the names of the variables (columns) in the dataframe, you could use:\n\nnames(laminaria)\n\nR&gt;  [1] \"region\"          \"site\"            \"Ind\"             \"blade_weight\"   \nR&gt;  [5] \"blade_length\"    \"blade_thickness\" \"stipe_mass\"      \"stipe_length\"   \nR&gt;  [9] \"stipe_diameter\"  \"digits\"          \"thallus_mass\"    \"total_length\"\n\n\nAnother option, but by no means the only one remaining, is to install a library called skimr, to use the skim() function:\n\nlibrary(skimr)\nskim(iris) # using built-in `iris` data\n\n\nData summary\n\n\nName\niris\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#tidyverse-sneak-peek",
    "href": "intro_r/05-workflow.html#tidyverse-sneak-peek",
    "title": "5. R Workflows",
    "section": "4.2 Tidyverse Sneak Peek",
    "text": "4.2 Tidyverse Sneak Peek\nBefore you begin to manipulate our data further I need to briefly introduce you to the tidyverse. And no introduction can be complete within learning about the pipe command, %&gt;%. You may type this by pushing the following keys together: ctrl-shift-m. The pipe (%&gt;%, |&gt; if you selected to use the native pipe operator under ‘Global Options’) allows you to perform calculations sequentially, which helps us to avoid making errors.\n\n\n\n\n\n\nNoteThe Pipe Operator\n\n\n\nThe pipe operator allows you to take the output of one function and pass it directly as the input to the next function. This creates a more intuitive and readable way to string together a series of data operations. Instead of nesting functions inside one another, which can quickly become confusing, hard to read and the pipe operator lets you lay out your data processing steps sequentially. This makes your code cleaner and easier to understand, as it clearly outlines the workflow from start to finish, almost like a step-by-step recipe for your data analysis.\n\n\nThe pipe works best in tandem with the following common functions:\n\nArrange observations (rows) with arrange()\nFilter observations (rows) with filter()\nSelect variables (columns) with select()\nCreate new variables (columns) with mutate()\nSummarise variables (columns) with summarise()\nGroup observations (rows) with group_by()\n\nYou will cover these functions in more detail on Day 4. For now you will ease ourselves into the code with some simple examples.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#subsetting",
    "href": "intro_r/05-workflow.html#subsetting",
    "title": "5. R Workflows",
    "section": "4.3 Subsetting",
    "text": "4.3 Subsetting\nNow let us have a look at specific parts of the data. You will likely need to do this in almost every script you write. If you want to refer to a variable, you specify the dataframe then the column name within the select() function. In your script type:\n\n# Note: output NOT assigned to an object (no `&lt;-`):\nlaminaria %&gt;% # Tell R which dataframe you are using\n  select(site, total_length) # Select only specific columns\n\nThis operation produces a temporary result only. Because it is not assigned to a name, the state of the workflow is unchanged.\nIf you want to only select values from specific columns you insert one more line of code.\n\nlaminaria %&gt;% \n  select(site, total_length) %&gt;% # Select specific columns first\n  slice(56:78)\n# what does the '56:78' do? Change some numbers and run the code again. What happens?\n\nIf you wanted to select only the rows of data belonging to the Kommetjie site, you could type:\n\nlaminaria %&gt;%\n  filter(site == \"Kommetjie\")\n\nThe function filter() has two arguments: the first is a dataframe (we specify laminaria in the previous line and the pipe supplies this for us) and the second is an expression that relates to which rows of a particular variable you want to include. Here you include all rows for Kommetjie and you find that in the variable site. It returns a subset that is actually a dataframe itself; it is in the same form as the original dataframe. You could assign that subset of the full dataframe to a new dataframe if you wanted to.\n\nlam_kom &lt;- laminaria %&gt;% \n  filter(site == \"Kommetjie\")\n\nAt this point the workflow has branched. There are now two data objects in memory (the original unmodified laminaria and the new lam_kom), each with a distinct role and purpose.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#basic-stats",
    "href": "intro_r/05-workflow.html#basic-stats",
    "title": "5. R Workflows",
    "section": "4.4 Basic Stats",
    "text": "4.4 Basic Stats\nStraight out of the box it is possible in R to perform a broad range of statistical calculations on a dataframe. If you wanted to know how many samples you have at Kommetjie, you simply type the following:\n\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(site == \"Kommetjie\") %&gt;% # Filter out only records from Kommetjie\n  nrow() # Count the number of remaining rows\n\nOr, if you want to select only the row with the greatest total length:\n\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(total_length == max(total_length)) # Select row with max total length\n\n\n\n\n\n\n\nImportantDo This Now\n\n\n\nPurpose: to demonstrate that the analysis can be reconstructed from the script alone.\nUsing pipes, subset the Laminaria data to include regions where the blade thickness is thicker than 5 cm, retain only the columns site and region, blade weight, blade thickness. Now exit RStudio. Pretend it is three days later and revisit your analysis. Calculate the number of entries at Kommetjie and find the row with the greatest length. Do this now.\n\n\nImagine doing this daily as our analysis grows in complexity. It will very soon become quite repetitive if each day you had to retype all these lines of code. And now, six weeks into the research, attendant statistical analysis and you discover that there were some mistakes and some of the raw data were incorrect. Now everything would have to be repeated by retyping it at the command prompt. Or worse still (and bad for repetitive strain injury) doing all of it in SPSS and remembering which buttons to click and then re-clicking them. A pain. Let us avoid that altogether and do it the right way by writing an R script to automate and annotate all of this.\n\n\n\n\n\n\nNoteDealing with Missing Data\n\n\n\nThe .csv file format is usually the most robust for reading data into R. Where you have missing data (blanks), the .csv format separates these by commas. However, there can be problems with blanks if you read in a space-delimited format file. If you are having trouble reading in missing data as blanks, replace them in your spreadsheet with NA, the missing data code in R. In Excel, highlight the area of the spreadsheet that includes all the cells you need to fill with NA. Do an Edit/Replace… and leave the ‘Find what:’ textbox blank and in the ‘Replace with:’ textbox enter NA, the missing value code. Once imported into R, the NA values will be recognised as missing data.\n\n\nSo far you have calculated the mean and standard deviation of some data in the Laminaria data set. If you have not, please append those lines of code to the end of your script. You can run individual lines of code by highlighting them, pressing ctrl-Enter (cmd-Enter on a Mac). Do this.\nYour file will now look similar to this one, but of course you will have added your own notes, comments as you went along:\n\n# Day_1.R\n# Reads in some data about Laminaria collected along the Cape Peninsula\n# do various data manipulations, analyses and graphs\n# AJ Smit\n# 9 January 2020\n\n# Find the current working directory (it will be correct if a project was\n# created as instructed earlier)\ngetwd()\n\n# If the directory is wrong because you chose not to use an Rworkspace (project),\n# set your directory manually to where the script will be saved and where the data\n# are located\n# setwd(\"&lt;insert_path_here&gt;\")\n\n# Load libraries\nlibrary(tidyverse)\n\n# Load the data\nlaminaria &lt;- read_csv(here::here(\"data\", \"BCB744\", \"laminaria.csv\"))\n\n# Examine the data\nhead(laminaria, 5) # First five lines\ntail(laminaria, 2) # Last two lines\nglimpse(laminaria) # A more thorough summary\nnames(laminaria) # THe names of the columns\n\n# Subsetting data\nlaminaria %&gt;% # Tell R which dataframe to use\n  select(site, total_length) %&gt;% # Select specific columns\n  slice(56:78) # Select specific rows\n\n# How many data points do you have at Kommetjie?\nlaminaria %&gt;%\n  filter(site == \"Kommetjie\") %&gt;%\n  nrow()\n\n# The row with the greatest length\nlaminaria %&gt;% # Tell R which dataset to use\n  filter(total_length == max(total_length)) # Select row with max total length\n\nMaking sure all the latest edits in your R script have been saved, close your R session. Pretend this is now 2019, you need to revisit the analysis. Open the file you created in 2017 in RStudio. All you need to do now is highlight the file’s entire contents and hit ctrl-Enter.\n\n\n\n\n\n\nNoteStick with .csv Files\n\n\n\nThere are packages in R to read in Excel spreadsheets (e.g., .xlsx), but remember there are likely to be problems reading in formulae, graphs, macros, multiple worksheets. You recommend exporting data deliberately to .csv files (which are also commonly used in other programs). This not only avoids complications, but also allows you to unambiguously identify the data you based your analysis on. This last statement should give you the hint that it is good practice to name your .csv slightly differently each time you export it from Excel, perhaps by appending a reference to the date it was exported.\nBecause these transformations are often invisible, downstream results can no longer be unambiguously traced to their source.\n\n\n\n\n\n\n\n\nNoteRemember…\n\n\n\nFriends do not let friends use Excel.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#summary-statistics-by-variable",
    "href": "intro_r/05-workflow.html#summary-statistics-by-variable",
    "title": "5. R Workflows",
    "section": "5.1 Summary Statistics by Variable",
    "text": "5.1 Summary Statistics by Variable\nThis is all very convenient, but you may want to ask R specifically for just the mean of a particular variable. In this case, you simply need to tell R which summary statistic you are interested in, and to specify the variable to apply it to using summarise(). Add the following lines to your script and run them:\n\nlaminaria %&gt;% # Chose the dataframe\n  summarise(avg_bld_wdt = mean(blade_length)) # Calculate mean blade length\n\nOr, if you wanted to know the mean, standard deviation for the total lengths of all the plants across all sites and do:\n\nlaminaria %&gt;% # Tell R that you want to use the 'laminaria' dataframe\n  summarise(avg_stp_ln = mean(total_length), # Create a summary of the mean of the total lengths\n            sd_stp_ln = sd(total_length)) # Create a summary of the sd of the total lengths\n\nOf course, the mean, standard deviation are not the only summary statistic that R can calculate. In your script, execute max(), min(), median(), range(), sd() and var(). Do they return the values you expected? Add the following lines to your script and run them:\n\nlaminaria %&gt;% \n  summarise(avg_stp_ms = mean(stipe_mass))\n\nThe answer probably is not what you would expect. Why not? Sometimes, you need to tell R how you want it to deal with missing data. In this case, you have NAs in the named variable, and R takes the cautious approach of giving you the answer of NA, meaning that there are missing values here. This may not seem useful, but as the programmer, you can tell R to respond differently, and it will. Simply append an argument to your function call, and you will get a different response. Type:\n\nlaminaria %&gt;% \n  summarise(avg_stp_ms = mean(stipe_mass, na.rm = T))\n\nThe na.rm argument tells R to remove (or more correctly ‘strip’) NAs from the data string before calculating the mean. It now returns the correct answer. Although needing to deal explicitly with missing values in this way can be a bit painful, it does make you more aware of missing data, what the analyses in R are doing, and makes you decide explicitly how you will treat missing data.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#more-complex-calculations",
    "href": "intro_r/05-workflow.html#more-complex-calculations",
    "title": "5. R Workflows",
    "section": "5.2 More Complex Calculations",
    "text": "5.2 More Complex Calculations\nLet us say you want to calculate something that is not standard in R, say the standard error of the mean for a variable, rather than just the corresponding standard deviation. How can this be done?\nThe trick is to remember that R is a calculator, so you can use it to do maths, even complex maths (which you will not do). The formula for standard error is:\n\\[se = \\frac{var}{\\sqrt{n}}\\]\nYou know that the variance is given by var(), so all you need to do is figure out how to get n and calculate a square root. The simplest way to determine the number of elements in a variable is a call to the function nrow(), as you saw previously. You may therefore calculate standard error with one chunk of code, step by step, using the pipe. Furthermore, by using group_by() you may calculate the standard error for all sites in one go.\n\nlaminaria %&gt;% # Select 'laminaria'\n  group_by(site) %&gt;% # Group the dataframe by site\n  summarise(var_bl = var(blade_length), # Calculate variance\n            n_bl = n()) %&gt;%  # Count number of values\n  mutate(se_bl = var_bl / sqrt(n_bl)) # Calculate se\n\nWhen calculating the mean, you specified that R should strip the NAs, using the argument na.rm = TRUE. In the example above, you did not have NAs in the variable of interest. What happens if you do?\nUnfortunately, the call to the function nrow() has no arguments telling R how to treat NAs; instead, they are simply treated as elements of the variable, are therefore counted. The easiest way to resolve this problem is to strip out NAs in advance of any calculations. Add the following lines to your script and run them:\n\nlaminaria %&gt;% \n  select(stipe_mass) %&gt;% \n  summarise(n = n())\n\nthen:\n\nlaminaria %&gt;% \n  select(stipe_mass) %&gt;% \n  na.omit() %&gt;% \n  summarise(n = n())\n\nYou will notice that the function na.omit() removes NAs from the variable that is specified as its argument.\n\n\n\n\n\n\nImportantDo This Now\n\n\n\nPurpose: to demonstrate controlled use of a small family of related functions.\n\nUsing this new information, calculate the mean stipe mass, the corresponding standard error.\nCreate a new data frame from the Laminaria dataset that meets the following criteria: contains only the site column and a new column called total_length_half containing values that are half of the total_length. In this total_length_half column, there are no NAs, all values are less than 100. Hint: think about how the commands should be ordered to produce this data frame!\nUse group_by() and summarise() to find the mean(), min(), max() blade_length for each site. Also add the number of observations (hint: see ?n).\nWhat was the heaviest stipe measured in each site? Return the columns site, region, stipe_length.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#creating-and-opening-a-project",
    "href": "intro_r/05-workflow.html#creating-and-opening-a-project",
    "title": "5. R Workflows",
    "section": "9.1 Creating and Opening a Project",
    "text": "9.1 Creating and Opening a Project\nIf you have been provided with an .Rproj file (for example, Intro_R_Workshop.Rproj), activate it as follows:\n\nIn RStudio, locate the Project menu in the top right corner of the window.\nClick the menu and select Open Project…\nNavigate to the folder containing the .Rproj file and select it.\n\nRStudio will restart the session. This is expected behaviour. Any objects previously in memory will be cleared, and previously open scripts may close.\nOr, you may navigate to the .Rproj file in your file system (Windows Explorer or some other file navigation tool), and simply double click on the file. When you do this, a new RStudio window will open with the active project in focus.\nIf you are starting a new analysis from scratch:\n\nSelect File → New Project…\nChoose New Directory, then New Project\nSelect a location on your computer and give the project a meaningful name\nClick Create Project\n\nRStudio will create a new folder containing an .Rproj file and open it immediately.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#checking-that-the-project-is-active",
    "href": "intro_r/05-workflow.html#checking-that-the-project-is-active",
    "title": "5. R Workflows",
    "section": "9.2 Checking That the Project Is Active",
    "text": "9.2 Checking That the Project Is Active\nYou should always verify that a project is active before reading or writing files.\nThere are three ways to do this:\n\nVisual check: the name of the project appears in the top right corner of the RStudio window.\nFiles pane: the Files tab shows the contents of the project directory.\nConsole check: type the following in the Console or script and run it:\n\n\ngetwd()\n\nThe returned path should correspond to the folder containing the .Rproj file. If it does not, the project is not active.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#how-projects-affect-file-paths",
    "href": "intro_r/05-workflow.html#how-projects-affect-file-paths",
    "title": "5. R Workflows",
    "section": "9.3 How Projects Affect File Paths",
    "text": "9.3 How Projects Affect File Paths\nWhen a project is active, all relative file paths are interpreted with respect to the project root. This means that code such as\n\nread_csv(here::here(\"data\", \"BCB744\", \"laminaria.csv\"))\n\nwill work reliably as long as the file exists inside the project directory, regardless of where the project is stored on your computer or whose computer it is run on.\nAvoid manually changing the working directory with setwd() inside scripts. Doing so introduces hidden state and makes scripts dependent on local file paths. Projects remove the need for this entirely.\n\n\n\n\n\n\nNoteReproducibility Checklist (use This for Every task)\n\n\n\n\nWork inside an R Project (open the .Rproj and confirm with getwd()).\nUse project-relative paths (e.g., here::here(\"data\", ...)) rather than hard-coded absolute paths.\nDo not use setwd() inside scripts.\nKeep install.packages(...) out of scripts; scripts should start with library(...).\nWhen randomness is involved, set a seed (e.g., set.seed(1)) and say why.\nRecord your environment when submitting work (e.g., sessionInfo() or sessioninfo::session_info()).",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/05-workflow.html#what-is-expected-at-this-stage",
    "href": "intro_r/05-workflow.html#what-is-expected-at-this-stage",
    "title": "5. R Workflows",
    "section": "9.4 What Is Expected at This Stage",
    "text": "9.4 What Is Expected at This Stage\nAt this point, your work will probably not expect you to design elaborate project structures or manage complex dependencies. But you need to recognise R Projects and understand their relationship with the physical location of files on your computer’s file system. You should know how to open a project, confirm that it is active, and understand that scripts and data assume such a context exists. Reproducibility depends on this relationship being fixed.\nLater in the your career, you might be expected to understand how projects interact with reporting, version control, and larger workflows. For now, treat the project as the outer container of your analysis, a place where everything that matters should live.\n\n\n\n\n\n\nNoteOrganising R Projects\n\n\n\nFor every R Project, set up a separate directory in your file system that includes the scripts, data files, outputs. You should have your personal, clearly-developed philosophy that dictate where and how you store files on your computer — this is very basic, like managing rooms in your house. Each room has a purpose.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "5. R Workflows"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html",
    "href": "intro_r/07-faceting.html",
    "title": "7. Faceting Figures",
    "section": "",
    "text": "So far we have only looked at single panel figures. But as you may have guessed by now, ggplot2 is capable of creating any sort of data visualisation that a human mind could conceive. This may seem like a grandiose assertion, but we will see if we cannot convince you of it by the end of this course. For now however, let us just take our understanding of the usability of ggplot2 two steps further by first learning how to facet a single figure, and then stitch different types of figures together into a grid. In order to aid us in this process we will make use of an additional package, ggpubr. The purpose of this package is to provide a bevy of additional tools that researchers commonly make use of in order to produce publication-quality figures. Note that library(ggpubr) will not work on your computer if you have not yet installed the package.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#line-graph",
    "href": "intro_r/07-faceting.html#line-graph",
    "title": "7. Faceting Figures",
    "section": "3.1 Line Graph",
    "text": "3.1 Line Graph\n\nline_1 &lt;- ggplot(data = ChickWeight, aes(x = Time, y = weight, colour = Diet)) +\n  geom_point() +\n  geom_line(aes(group = Chick)) +\n  labs(x = \"Days\", y = \"Mass (g)\") +\n  theme_minimal()\nline_1\n\n\n\n\n\n\n\nFigure 2: Line graph for the progression of chicken weights (g) over time (days) based on four different diets.\n\n\n\n\n\n\n\n\n\n\n\nNoteFaceting vs Grouping\n\n\n\nGrouping (group = Chick) keeps all data in one panel and tells ggplot2 which observations belong together for a geometry. Faceting (facet_wrap(~Diet)) splits the data into multiple panels before drawing anything. Use grouping when the comparison is clearest in a single coordinate system. Use faceting when each subgroup deserves its own panel, and you want direct visual comparison across panels.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#smooth-gam-model",
    "href": "intro_r/07-faceting.html#smooth-gam-model",
    "title": "7. Faceting Figures",
    "section": "3.2 Smooth (GAM) Model",
    "text": "3.2 Smooth (GAM) Model\n\nlm_1 &lt;- ggplot(data = ChickWeight, aes(x = Time, y = weight, colour = Diet)) +\n  geom_point() +\n  geom_smooth(method = \"gam\") +\n  labs(x = \"Days\", y = \"Mass (g)\") +\n  theme_minimal()\nlm_1\n\n\n\n\n\n\n\nFigure 3: GAM smooths for the progression of chicken weights (g) over time (days) based on four different diets.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#histogram",
    "href": "intro_r/07-faceting.html#histogram",
    "title": "7. Faceting Figures",
    "section": "3.3 Histogram",
    "text": "3.3 Histogram\n\n# Note that we are using 'ChickLast', not 'ChickWeight'\nhistogram_1 &lt;- ggplot(data = ChickLast, aes(x = weight)) +\n  geom_histogram(aes(fill = Diet), position = \"dodge\", binwidth = 100) +\n  labs(x = \"Final Mass (g)\", y = \"Count\") +\n  theme_minimal()\nhistogram_1\n\n\n\n\n\n\n\nFigure 4: Histogram showing final chicken weights (g) by diet.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#boxplot",
    "href": "intro_r/07-faceting.html#boxplot",
    "title": "7. Faceting Figures",
    "section": "3.4 Boxplot",
    "text": "3.4 Boxplot\n\n# Note that we are using 'ChickLast', not 'ChickWeight'\nbox_1 &lt;- ggplot(data = ChickLast, aes(x = Diet, y = weight)) +\n  geom_boxplot(aes(fill = Diet)) +\n  labs(x = \"Diet\", y = \"Final Mass (g)\") +\n  theme_minimal()\nbox_1\n\n\n\n\n\n\n\nFigure 5: Violin plot showing the distribution of final chicken weights (g) by diet.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/09-mapping.html",
    "href": "intro_r/09-mapping.html",
    "title": "9. Mapping with ggplot2",
    "section": "",
    "text": "Target map example for this chapter.\n\n\n\n“Graphical excellence is that which gives the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.”\n— Edward Tufte\n\n\n“Here be dragons.”\n— Unknown\n\nYesterday you learned how to create ggplot2 figures, change their aesthetics, labels, colour palettes, and facet/arrange them. Now you are going to look at how to create maps. In maps, it helps to distinguish fill (area) from colour (line). Polygons and rasters are usually filled, while borders and paths are typically just coloured lines.\nMost of the work that you will perform as environmental/biological scientists involves going out to a location and sampling information there. Sometimes only once, and sometimes over a period of time. All of these different sampling methods lend themselves to different types of figures. One of those, collection of data at different points, is best shown with maps. As you will see over the course of Day 3, creating maps in ggplot2 is very straight forward and is extensively supported. For that reason you are going to have plenty of time to also learn how to do some more advanced things. Your goal in this chapter is to produce the figure below.\n\n\n\nToday’s goal.\n\n\n\n1 Using Prepared Data\nBefore you begin let us go ahead and load the packages you will need, as well as several dataframes required to make the final product.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\n\n# Load data\nload(here::here(\"data\", \"BCB744\", \"south_africa_coast.Rdata\"))\nload(here::here(\"data\", \"BCB744\", \"sa_provinces.RData\"))\nload(here::here(\"data\", \"BCB744\", \"rast_annual.Rdata\"))\nload(here::here(\"data\", \"BCB744\", \"MUR.Rdata\"))\nload(here::here(\"data\", \"BCB744\", \"MUR_low_res.RData\"))\n\n# Choose which SST product you would like to use\nsst &lt;- MUR_low_res\n# OR\nsst &lt;- MUR\n\n# The colour palette we will use for ocean temperature\ncols11 &lt;- c(\"#004dcd\", \"#0068db\", \"#007ddb\", \"#008dcf\", \"#009bbc\",\n            \"#00a7a9\", \"#1bb298\", \"#6cba8f\", \"#9ac290\", \"#bec99a\")\n\n\n\n2 A New Concept?\nThe idea of creating a map in R may be daunting to some, but remember that a basic map is nothing more than a simple figure with an x, y axis. We tend to think of maps as different from other scientific figures, whereas in reality they are created the exact same way. The epistemic consequence of this is important: maps are built from the same grammar of graphics, but they carry extra assumptions about geography.\nBefore we move on, keep this distinction in mind:\n\nData coordinates are just numbers. They only become geographic when we interpret them as longitude and latitude.\nGeographic coordinates imply scale, distance, and distortion. The moment we treat x and y as lon/lat, we accept projection‑related assumptions.\n\nThis is why coord_equal() becomes important (Section 5). It preserves a 1:1 relationship between units on the axes so the geometry is not subtly stretched as in Figure 4.\nLet us look at the difference between normal data as dots, and geographic data as dots.\nChicken dots:\n\nggplot(data = ChickWeight, aes(x = Time, y = weight)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 1: Dot plot of chicken weight data.\n\n\n\n\n\nSouth African coast dots:\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 2: Dot plot off South African coast.\n\n\n\n\n\nDoes that look familiar? Notice how the x and y axis tick labels look the same as any map you would see in an atlas. This is because they are. But this is not a great way to create a map. Rather it is better to represent the land mass with a polygon. With ggplot2 this is a simple task.\n\n\n\n\n\n\nNoteCheckpoint\n\n\n\nBefore scrolling, try swapping lon and lat. What happens to the coastline? This is a fast way to diagnose coordinate order mistakes.\n\n\n\n\n3 Land Mask\nNow that you have seen that a map is nothing more than a bunch of dots and shapes on specific points along the x and y axes you are going to look at the steps you would take to build a more complex map. Do not worry if this seems daunting at first. You are going to take this step by step and ensure that each step is made clear along the way. The first step is to create a polygon.\nWhy a polygon? Polygons encode adjacency, continuity, and enclosure — properties that points cannot represent. This matters because land is not a collection of disconnected points; it is a continuous surface with a boundary (but this boundary does not have to correspond to geographical borders, although they can and often do).\nNote that you create an aesthetic argument inside of geom_polygon() and not ggplot() because some of the steps you will take later on will not accept the group aesthetic. Remember, whatever aesthetic arguments we put inside of ggplot() will be inserted into all of our other geom_...() lines of code.\n\n\n\n\n\n\nNoteWhy group Goes in geom_polygon()\n\n\n\nTry moving aes(group = group) into the main ggplot() call. Later layers (e.g., geom_raster()) will inherit it and may produce warnings or odd behaviour. Keeping group local to geom_polygon() makes the intent explicit and prevents accidental inheritance.\n\n\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) # The land mask\n\n\n\n\n\n\n\nFigure 3: Land mask of South Africa as a polygon.\n\n\n\n\n\n\n\n4 Borders\nThe first thing you will add is the province borders as seen in Figure Figure 4. Notice how you only add one more line of code to do this.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) # The province borders\n\n\n\n\n\n\n\nFigure 4: South Africa with province borders added.\n\n\n\n\n\n\n\n5 Force lon/lat Extent\nUnfortunately when you added our borders it increased the plotting area of our map past what you would like. To correct that you will need to explicitly state the borders you want. This is also where aspect ratio matters most: without coord_equal(), your coastline will be subtly stretched.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) + \n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0) # Force lon/lat extent\n\n\n\n\n\n\n\nFigure 5: The map, but with the extra bits snipped off.\n\n\n\n\n\n\n\n6 Ocean Temperature\nThis is starting to look pretty fancy, but it would be nicer if there was some colour involved. So let us add the ocean temperature. Again, this will only require one more line of code. Starting to see a pattern? But what is different this time, why? Here we are filling areas (ocean cells), not colouring borders, so we map to fill instead of colour.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) + # The ocean temperatures\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\n\n\n\n\nFigure 6: Ocean temperature (°C) visualised as an ice cream spill.\n\n\n\n\n\nThat looks… odd. Why do the colours look like someone melted a big bucket of ice cream in the ocean? This is because the colours you see in this figure are the default colours for discrete values in ggplot2. If you want to change them we may do so easily by adding yet one more line of code.\n\n\n\n\n\n\nNoteDiagnostic Heuristic\n\n\n\nIf a continuous field looks blocky or cartoonish, check whether your variable has been discretised upstream. If it has, you will get a discrete palette and a stepped legend.\n\n\n\n\n\n\n\n\nNotePalette Consistency Is Important\n\n\n\nggplot2 enforces a consistent palette across layers so that the same colour always means the same thing. This is a principle of perceptual coherence.\n\n\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) + # Set the colour palette\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\n\n\n\n\nFigure 7: Ocean temperatures (°C) around South Africa.\n\n\n\n\n\nThere is a colour palette that would make Jacques Cousteau swoon. When you set the fill palette for a figure in ggplot2 you must use that fill palette for all other instances of those types of values, too. What this means is that any other discrete values that will be filled in, like the ocean cells above, must use the same fill palette (there are some technical exceptions to this rule that you will not cover in this course). You normally want ggplot2 to use consistent palettes anyway, but it is important to note that this constraint exists. Let us see what I mean. Next you will add the coastal pixels to our figure with one more line of code. You will not change anything else. Note how ggplot2 changes the fill of the coastal pixels to match the ocean fill automatically.\n\nggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  geom_tile(data = rast_annual, aes(x = lon, y = lat, fill = bins), \n            colour = \"white\", size = 0.1) + # The coastal temperature values\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0)\n\n\n\n\n\n\n\nFigure 8: Map of South Africa showing in situ temperatures (°C) as pixels along the coast.\n\n\n\n\n\n\n\n7 Final Touches\nYou used geom_tile() instead of geom_rast() to add the coastal pixels above so that you could add those little white boxes around them.\nWhy tiles? geom_tile() and geom_raster() are best for gridded data (continuous fields sampled on a grid). Points are best for discrete observations. Choosing the geometry tells the reader what kind of data you believe you have.\nThis figure is looking pretty great now. And it only took a few rows of code to put it all together! The last step is to add several more lines of code that will control for all of the little things you want to change about the appearance of the figure. Each little thing that is changed below is annotated for your convenience.\n\n\n\n\n\n\nNoteDesign Choices Are Not Defaults\n\n\n\nAdding borders, grids, or labels is a rhetorical decision. Ask yourself: does this layer help the scientific story, or just add clutter?\n\n\n\n\n\n\n\n\nNoteFailure Modes to Watch for\n\n\n\n\nSwapped lon/lat → the map appears mirrored or rotated.\nNo coord_equal() → coastlines look stretched.\nMismatched extents → rasters appear shifted or cropped.\n\n\n\n\n\n\n\n\n\nNoteWorkflow Tip\n\n\n\nBuild the map in this order: data → geometry → scale → coordinate system → theme. Styling comes last.\n\n\n\nfinal_map &lt;- ggplot(data = south_africa_coast, aes(x = lon, y = lat)) +\n  geom_raster(data = sst, aes(fill = bins)) +\n  geom_polygon(colour = \"black\", fill = \"grey70\", aes(group = group)) +\n  geom_path(data = sa_provinces, aes(group = group)) +\n  geom_tile(data = rast_annual, aes(x = lon, y = lat, fill = bins), \n            colour = \"white\", size = 0.1) +\n  scale_fill_manual(\"Temp. (°C)\", values = cols11) +\n  coord_equal(xlim = c(15, 34), ylim = c(-36, -26), expand = 0) +\n  scale_x_continuous(position = \"top\") + # Put x axis labels on top of figure\n  theme(axis.title = element_blank(), # Remove the axis labels\n        legend.text = element_text(size = 7), # Change text size in legend\n        legend.title = element_text(size = 7), # Change legend title text size\n        legend.key.height = unit(0.3, \"cm\"), # Change size of legend\n        legend.background = element_rect(colour = \"white\"), # Add legend background\n        legend.justification = c(1, 0), # Change position of legend\n        legend.position = c(0.55, 0.4) # Fine tune position of legend\n        )\nfinal_map\n\n\n\n\n\n\n\nFigure 9: The cleaned up map of South Africa. Resplendent with coastal and ocean temperatures (°C).\n\n\n\n\n\nThat is a very clean looking map so go ahead and save it on your local disk.\n\nggsave(plot = final_map, \"figures/map_complete.pdf\", height = 6, width = 9)\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {9. {Mapping} with **Ggplot2**},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/09-mapping.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 9. Mapping with **ggplot2**. http://samos-r.netlify.app/intro_r/09-mapping.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "9. Mapping with **ggplot2**"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#learning-goals",
    "href": "intro_r/11-mapping_rnaturalearth.html#learning-goals",
    "title": "11. Mapping with Natural Earth",
    "section": "1.1 Learning Goals",
    "text": "1.1 Learning Goals\nBy the end of this chapter you should be able to:\n\nExplain what an sf object is (features, attributes, geometry column, CRS).\nSubset and transform spatial features without losing geometry.\nProduce layered maps in base R and ggplot2 with explicit spatial choices.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#workflow-overview",
    "href": "intro_r/11-mapping_rnaturalearth.html#workflow-overview",
    "title": "11. Mapping with Natural Earth",
    "section": "1.2 Workflow Overview",
    "text": "1.2 Workflow Overview\nWhen you are working independently, aim for this repeatable sequence:\n\nLoad data and inspect the geometry and CRS.\nSubset features (rows) and attributes (columns) intentionally.\nCrop or transform geometry only when you mean to.\nPlot in base R or ggplot2, then refine.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#web-resources-for-spatial-work-in-r",
    "href": "intro_r/11-mapping_rnaturalearth.html#web-resources-for-spatial-work-in-r",
    "title": "11. Mapping with Natural Earth",
    "section": "1.3 Web Resources for Spatial Work in R",
    "text": "1.3 Web Resources for Spatial Work in R\nThese are reference texts, not required reading. Use them as technical back-up when you get stuck or want to go deeper.\n\n\n\n\n\n\nNoteWeb Resources About Spatial Methods in R\n\n\n\n\n\n\nAuthor\nTitle\n\n\n\n\nSpatial R\n\n\n\nEdzer Pebesma\nSimple Features for R\n\n\nEdzer Pebesma, Roger Bivand\nSpatial Data Science with applications in R\n\n\nRobin Lovelace et al.\nGeocomputation with R\n\n\nManuel Gimond\nIntro to GIS and Spatial Analysis\n\n\nWasser et al.\nIntroduction to Geospatial Raster and Vector Data with R\n\n\nTaro Mieno\nR as GIS for Economists",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#what-to-aspire-to",
    "href": "intro_r/11-mapping_rnaturalearth.html#what-to-aspire-to",
    "title": "11. Mapping with Natural Earth",
    "section": "1.4 What to Aspire to",
    "text": "1.4 What to Aspire to\nPlease see the snooty maps at Making Static Maps in R. The skills you acquire in this module should eventually allow you to do similar maps.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#define-a-working-extent",
    "href": "intro_r/11-mapping_rnaturalearth.html#define-a-working-extent",
    "title": "11. Mapping with Natural Earth",
    "section": "4.1 Define a Working Extent",
    "text": "4.1 Define a Working Extent\nFirst, I define the extent of the map region:\n\n# the full map extent:\nxmin &lt;- 12; ymin &lt;- -36.5; xmax &lt;- 40.5; ymax &lt;- -10\nxlim &lt;- c(xmin, xmax); ylim &lt;- c(ymin, ymax)\n\n# make a bounding box for cropping:\nbbox &lt;- st_bbox(c(xmin = xmin, ymin = ymin,\n  xmax = xmax, ymax = ymax))\n\n# might be useful for zooming into a smaller region (False Bay and \n# the Cape Peninsula):\nxlim_zoom &lt;- c(17.8, 19); ylim_zoom &lt;- c(-34.5, -33.2)\n\nChoosing bounds is a cartographic decision. If you clip too tightly you can hide relevant context; if you clip too loosely you dilute the focus of the map. Cropping can also cut features that cross the boundary (and cuase complications, which are very difficult to rectify), so use it deliberately.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#load-features-from-natural-earth",
    "href": "intro_r/11-mapping_rnaturalearth.html#load-features-from-natural-earth",
    "title": "11. Mapping with Natural Earth",
    "section": "5.1 Load Features from Natural Earth",
    "text": "5.1 Load Features from Natural Earth\n\n# load the countries:\nsafrica_countries &lt;- ne_countries(returnclass = 'sf',\n  continent = \"Africa\",\n  country = c(\"South Africa\", \"Mozambique\",\n    \"Namibia\", \"Zimbabwe\", \"Botswana\",\n    \"Lesotho\", \"Eswatini\"),\n  scale = \"large\")\n\nThe object now contains features (rows) with attributes (columns) plus a geometry column that stores the spatial shapes. That geometry persists unless you explicitly alter it.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#inspect-the-geometry-and-crs",
    "href": "intro_r/11-mapping_rnaturalearth.html#inspect-the-geometry-and-crs",
    "title": "11. Mapping with Natural Earth",
    "section": "5.2 Inspect the Geometry and Crs",
    "text": "5.2 Inspect the Geometry and Crs\nPause here and check what makes this different from a normal tibble.\n\nclass(safrica_countries)\n\nR&gt; [1] \"sf\"         \"data.frame\"\n\n# safrica_countries\n\n\n\n\n\n\n\nNoteThe sf Class\n\n\n\nsf indicates that the object is of class simple features. In sf language, what would be called columns (variables) in normal tidyverse speak becomes known as attributes — these are the properties of the map features. The geometry column stores the shapes; the CRS stores the spatial reference system.\n\n\n\n# Run this on your PC... the output is too voluminous for here\nglimpse(safrica_countries)\nst_crs(safrica_countries)\nunique(st_geometry_type(safrica_countries))\n\nThe CRS tells you how the coordinates should be interpreted. You do not need to manipulate CRS yet, but you do need to be aware that distance and area calculations depend on it.\nAs you can see, it is a data.frame and tbl (tibble), amongst other classes, and so you can apply many of the tidyverse functions to it, including select(), filter(), summarise() and so on. These verbs act on attributes unless you explicitly change geometry with sf functions.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#plot-quickly-in-base-r",
    "href": "intro_r/11-mapping_rnaturalearth.html#plot-quickly-in-base-r",
    "title": "11. Mapping with Natural Earth",
    "section": "5.3 Plot Quickly in Base R",
    "text": "5.3 Plot Quickly in Base R\nLet us plot the entire safrica_countries object to see all the attributes of all of the features. This kind of figure is called a choropleth map:\n\nplot(safrica_countries)\n\n\n\n\n\n\n\nFigure 1: Southern Africa countries shown as a base R choropleth.\n\n\n\n\n\n\n\n\n\n\n\nTipAttributes vs Features (select vs filter)\n\n\n\nUse select() to choose attributes (columns). Use filter() to choose features (rows). Neither operation recalculates geometry; the shapes persist unless you explicitly transform them.\n\n\nYou probably do not want to plot all of them. Let us select one:\n\nplot(safrica_countries[\"sovereignt\"])\n\n\n\n\n\n\n\nFigure 2: Southern Africa countries shaded by sovereignty.\n\n\n\n\n\nYou might achieve the same in a more familiar way:\n\nsafrica_countries |&gt; \n  select(sovereignt) |&gt; \n  plot()\n\n\n\n\n\n\n\nFigure 3: Southern Africa countries shaded by sovereignty (select).\n\n\n\n\n\nOr you may want to plot the estimate of the population size, which is contained in the attribute pop_est:\n\nsafrica_countries |&gt; \n  select(pop_est) |&gt; \n  plot()\n\n\n\n\n\n\n\nFigure 4: Southern Africa countries shaded by population estimate.\n\n\n\n\n\nThe names of the countries are in the rows of the safrica_countries object, and so they become accessible with filter(). Let us only plot some attribute for South Africa:\n\nsafrica_countries |&gt; \n  dplyr::filter(sovereignt == \"South Africa\") |&gt; \n  select(sovereignt) |&gt; \n  plot()\n\n\n\n\n\n\n\nFigure 5: South Africa feature only (sovereignty filtered).",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#dissolve-and-crop-features-geometry-changes",
    "href": "intro_r/11-mapping_rnaturalearth.html#dissolve-and-crop-features-geometry-changes",
    "title": "11. Mapping with Natural Earth",
    "section": "5.4 Dissolve and Crop Features (geometry changes)",
    "text": "5.4 Dissolve and Crop Features (geometry changes)\nThe following sequence is dense, so here is the logic before the code:\n\ngroup_by(continent) groups features so that later operations can dissolve borders.\nsummarise() collapses grouped features into a single geometry per group.\nst_crop(bbox) clips the geometry to your bounding box. This can cut features at the boundary.\nst_combine() merges the pieces into a single multipart feature.\n\nCropping is a geometric operation, but sometimes it may be used as a visual zoom. Features that fall partly inside the box will be clipped.\nYou can continue to add additional operations to create a new map:\n\nsafrica_countries_new &lt;- safrica_countries |&gt; \n  group_by(continent) |&gt; \n  summarise() |&gt; \n  st_crop(bbox) |&gt;\n  st_combine()\n\nplot(safrica_countries_new)\n\n\n\n\n\n\n\nFigure 6: Dissolved and cropped Southern Africa geometry.\n\n\n\n\n\nSo far you have relied on the base R plot function made for simple features. You can also plot the map in ggplot2 using ggplot(), which treats geometry as data rather than annotation. Here, geom_sf() draws the features and coord_sf() manages the spatial coordinates and aspect ratio. This is why coord_sf() is preferred to coord_cartesian() for maps.\n\nggplot() +\n  geom_sf(data = safrica_countries,\n    colour = \"indianred\", fill = \"beige\") +\n  coord_sf(xlim = xlim,\n           ylim = ylim)\n\n\n\n\n\n\n\nFigure 7: Southern Africa map drawn with ggplot2 and geom_sf.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/11-mapping_rnaturalearth.html#buffering-and-geometry-assumptions",
    "href": "intro_r/11-mapping_rnaturalearth.html#buffering-and-geometry-assumptions",
    "title": "11. Mapping with Natural Earth",
    "section": "5.5 Buffering and Geometry Assumptions",
    "text": "5.5 Buffering and Geometry Assumptions\nNow you can layer another feature. This is also where geometry assumptions matter: buffering in degrees is only a rough approximation for local maps.\n\nbuffer &lt;- safrica_countries_new %&gt;%\n  st_buffer(0.4)\n\nggplot() +\n  geom_sf(data = buffer, fill = \"lightblue\", col = \"transparent\") +\n  geom_sf(data = safrica_countries, colour = \"indianred\", fill = \"beige\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 8: Buffered Southern Africa geometry overlaid on the original map.\n\n\n\n\n\n\n\n\n\n\n\nWarningDistances Are Not Degrees\n\n\n\nIf a buffer value “looks right” in degrees, that is an accident of scale. For accurate distances, project your data first and buffer in metres.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "11. Mapping with Natural Earth"
    ]
  },
  {
    "objectID": "intro_r/13-tidy.html",
    "href": "intro_r/13-tidy.html",
    "title": "13. Tidy Data",
    "section": "",
    "text": "The Tidyverse is a collection of R packages designed for data manipulation, exploration, and visualisation. It is based on a philosophy of “tidy data,” which is a standardised way of organising data. The purpose of these packages is to make working with data more efficient. The core Tidyverse packages were created by Hadley Wickham, but over the last few years other individuals have added some packages to the collective, which has significantly expanded our data analytical capabilities through improved ease of use, efficiency. All packages that are built on tidy principles provide the use of a consistent set of tools across a wide range of data analysis tasks. The core Tidyverse packages can be loaded collectively by calling the tidyverse package and as we have seen throughout this workshop. The packages making up the Tidyverse are shown in Figure 1.\nlibrary(tidyverse)",
    "crumbs": [
      "Home",
      "Introduction to R",
      "13. Tidy Data"
    ]
  },
  {
    "objectID": "intro_r/13-tidy.html#pivot_longer",
    "href": "intro_r/13-tidy.html#pivot_longer",
    "title": "13. Tidy Data",
    "section": "4.1 pivot_longer()",
    "text": "4.1 pivot_longer()\n\n\n\n\n\n\nTipDiagnostic: When Do I Need pivot_longer()?\n\n\n\nAsk yourself:\n\nDo my column names encode data values? (e.g., “DEA”, “SAWS”)\nIs one variable split across multiple columns?\n\nIf yes, you likely need pivot_longer().\n\n\nThe R function pivot_longer() is a useful tool for transforming data from wide to long format. It belongs to the tidyr package (loaded with tidyverse) and allows you to reshape your dataframe by gathering multiple columns into key-value pairs. Specifically, pivot_longer() takes in a dataframe and allows you to select a set of columns that you would like to pivot into longer format, while specifying the names of the key, value columns that you want to create. The resulting data frame will have a new row for each unique combination of key and value pairs. This function is particularly useful when you need to reshape your data in order to carry out certain analyses or visualisations.\nHave a look now at SACTN2 for an example of what wide data look like, and how to fix it.\nIn SACTN2 you can see that the src column has been removed and that the temperatures are placed in columns that denote the collecting source. This may at first seem like a reasonable way to organise these data, but it is not tidy because the collecting source is one variable, and so should not take up more than one column (i.e., there are multiple observations per row). You need to gather these source columns together into one column so that the separate measurements (observations) can conform to the one observation per row rule. You do this by telling pivot_longer() the names of the columns you want to squish together. You then tell it the name of the key (names_to) column. This is the column that will contain all of the old column names we are gathering. In this case you may call it source. The last piece of this puzzle is the value (values_to) column. This is where you decide what the name of the column will be for measurements you are gathering up. In this case you may name it temperature, because you are gathering up the temperature values that were incorrectly spread out by the source of the measurements.\n\nSACTN2_tidy &lt;- pivot_longer(SACTN2, cols = c(\"DEA\", \"KZNSB\", \"SAWS\"),\n                            names_to = \"src\",\n                            values_to = \"temp\")",
    "crumbs": [
      "Home",
      "Introduction to R",
      "13. Tidy Data"
    ]
  },
  {
    "objectID": "intro_r/13-tidy.html#pivot_wider",
    "href": "intro_r/13-tidy.html#pivot_wider",
    "title": "13. Tidy Data",
    "section": "4.2 pivot_wider()",
    "text": "4.2 pivot_wider()\n\n\n\n\n\n\nTipDiagnostic: When Do I Need pivot_wider()?\n\n\n\nAsk yourself:\n\nAre multiple variables stacked in one column? (e.g., var + val)\nIs each observation split across multiple rows?\n\nIf yes, you likely need pivot_wider().\n\n\nThe function pivot_wider() is a tool for transforming data from long to wide format. It is the counterpart to the pivot_longer() function. pivot_wider() allows you to take a set of columns containing key-value pairs and convert them into a wider format, where each unique key value becomes a separate column in the resulting data frame. You can also specify a set of value columns that you want to spread across the new columns created by the key values. With pivot_wider(), you can quickly transform your data from long format into a more intuitive, wide format that is easier to work with in some applications.\nShould your data be too long for a particular application (typically a non-Tidyverse application) or your liking, meaning when individual observations are spread across multiple rows, you will need to use pivot_wider() to rectify the situation. This is generally the case when you have two or more variables stored within the same column, as you will see in SACTN3. This is not terribly common as it would require someone to put quite a bit of time into making a dataframe this way. But never say never. To spread data to become wider, you first tell R what the name of the column is that contains more than one variable, in this case the var column. You then tell R what the name of the column is that contains the values that need to be spread, in this case the val column.\n\nSACTN3_tidy1 &lt;- SACTN3 %&gt;% \n  pivot_wider(names_from = \"var\", values_from = \"val\")\n\n\n\n\n\n\n\nNotePivot Rule of Thumb (and a Quick anti-example)\n\n\n\n\npivot_longer() turns column names → values.\npivot_wider() turns values → column names.\n\nIf you use the wrong one, you either explode the data (too many rows) or flatten it (too many columns) and lose the one-observation-per-row rule.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "13. Tidy Data"
    ]
  },
  {
    "objectID": "intro_r/13-tidy.html#separate",
    "href": "intro_r/13-tidy.html#separate",
    "title": "13. Tidy Data",
    "section": "5.1 Separate",
    "text": "5.1 Separate\nLooking at SACTN4a, you see that you no longer have a site and src column. Rather these have been replaced by an index column. This is an efficient way to store these data, but it is not tidy because the site and source of each observation have now been combined into one column (variable). Remember that tidy data calls for each of the things known about the data to be its own variable. To re-create site and src columns, you must separate the index column. There are two options: separate_wider_delim() and separate_wider_position(). What does each do? First you give R the name of the column you want to separate, in this case index. Next you specify what the names of the new columns will be. Remember that because we are creating new column names, you feed these into R within inverted commas. Lastly, you should tell R where to separate the index column. If you look at the data you will see that the values you want to split up are separated with / (including a space), so that is what you need to tell R.\n\nSACTN4a_tidy &lt;- SACTN4a |&gt; \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \")",
    "crumbs": [
      "Home",
      "Introduction to R",
      "13. Tidy Data"
    ]
  },
  {
    "objectID": "intro_r/13-tidy.html#separating-dates-using-mutate",
    "href": "intro_r/13-tidy.html#separating-dates-using-mutate",
    "title": "13. Tidy Data",
    "section": "5.2 Separating Dates Using mutate()",
    "text": "5.2 Separating Dates Using mutate()\nAlthough the date column represents an example of a date date type or class (a kind of data in its own right), you might also want to split this column into its constituent parts, i.e., create separate columns for day, month, and year. In this case you can spread these components of the date vector into three columns using the mutate() function and some functions in the lubridate package (part of the tidyverse).\n\nSACTN_tidy2 &lt;- SACTN4a %&gt;% \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \") %&gt;% \n  mutate(day = lubridate::day(date),\n         month = lubridate::month(date),\n         year = lubridate::year(date))\n\nNote that when the date is split into component parts the data are no longer tidy (see below).\n\n\n\n\n\n\nWarningDates Are a Semantic Unit\n\n\n\nSplitting dates can be useful for grouping, but it also violates tidy principles by scattering one concept across multiple columns. Use this tactically and re-unite when you are done.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "13. Tidy Data"
    ]
  },
  {
    "objectID": "intro_r/13-tidy.html#unite",
    "href": "intro_r/13-tidy.html#unite",
    "title": "13. Tidy Data",
    "section": "5.3 Unite",
    "text": "5.3 Unite\nIt is not uncommon that field/lab instruments split values across multiple columns while they are making recordings. You might sometimes see this with date values where the year, month, and day values are given in different columns. There are uses for the data in this way, though it is not terribly tidy. You usually want the date of any observation to be shown in just one column. If you look at SACTN4b you will see that there is a year, month, day column. To unite() them you must first tell R what you want the united column to be labelled, in this case you will use date. You then list the columns to be united; here this is year, month, day. Lastly, decide if you want the united values to have a separator between them. The standard separator for date values is “-”.\n\nSACTN4b_tidy &lt;- SACTN4b |&gt; \n  unite(col = \"date\", year, month, day, sep = \"-\")",
    "crumbs": [
      "Home",
      "Introduction to R",
      "13. Tidy Data"
    ]
  },
  {
    "objectID": "intro_r/15-tidiest.html#grouping-by-multiple-variables",
    "href": "intro_r/15-tidiest.html#grouping-by-multiple-variables",
    "title": "15. Tidiest Data",
    "section": "2.1 Grouping by Multiple Variables",
    "text": "2.1 Grouping by Multiple Variables\nAs you may have guessed by now, grouping is not confined to a single column. One may use any number of columns to perform elaborate grouping measures. Let us look at some ways of doing this with the SACTN data.\n\n# Create groupings based on temperatures and depth\nSACTN_temp_group &lt;- SACTN %&gt;% \n  group_by(round(temp), depth)\n\n# Create groupings based on source and date\nSACTN_src_group &lt;- SACTN %&gt;% \n  group_by(src, date)\n\n# Create groupings based on date and depth\nSACTN_date_group &lt;- SACTN %&gt;% \n  group_by(date, depth)\n\nNow that you have created some grouped dataframes, let us think of some ways to summarise these data.\nWhy tidyverse grouping? It keeps your data in tidy shape, fits naturally into pipelines, and makes your assumptions about groups explicit. Base alternatives exist, but group_by() + summarise() is usually easier to read and audit.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "15. Tidiest Data"
    ]
  },
  {
    "objectID": "intro_r/15-tidiest.html#ungrouping",
    "href": "intro_r/15-tidiest.html#ungrouping",
    "title": "15. Tidiest Data",
    "section": "2.2 Ungrouping",
    "text": "2.2 Ungrouping\nOnce you level up our tidyverse skills you will routinely be grouping variables while calculating statistics. This then poses the problem of losing track of which dataframes are grouped and which are not. Grouping persists until you remove it, and that persistence can surprise you in later steps. Happily, to remove any grouping we just use ungroup(). No arguments required, just the empty function by itself. Too easy.\n\nSACTN_ungroup &lt;- SACTN_date_group %&gt;% \n  ungroup()\n\nTip: if you are done summarising, ungroup() immediately so that later steps do not inherit group structure by accident.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "15. Tidiest Data"
    ]
  },
  {
    "objectID": "intro_r/15-tidiest.html#rename-variables-columns-with-rename",
    "href": "intro_r/15-tidiest.html#rename-variables-columns-with-rename",
    "title": "15. Tidiest Data",
    "section": "7.1 Rename Variables (Columns) with rename()",
    "text": "7.1 Rename Variables (Columns) with rename()\nYou have seen that you select columns in a dataframe with select(), but if you want to rename columns you have to use, you guessed it, rename(). This functions works by first telling R the new name you would like, and then the existing name of the column to be changed. This is perhaps a bit back to front, but such is life on occasion.\n\nSACTN %&gt;% \n  rename(source = src)\n\n\n\nR&gt;            site source       date     temp depth type\nR&gt; 1  Port Nolloth    DEA 1991-02-01 11.47029     5  UTR\nR&gt; 2  Port Nolloth    DEA 1991-03-01 11.99409     5  UTR\nR&gt; 3  Port Nolloth    DEA 1991-04-01 11.95556     5  UTR\nR&gt; 4  Port Nolloth    DEA 1991-05-01 11.86183     5  UTR\nR&gt; 5  Port Nolloth    DEA 1991-06-01 12.20722     5  UTR\nR&gt; 6  Port Nolloth    DEA 1991-07-01 12.53810     5  UTR\nR&gt; 7  Port Nolloth    DEA 1991-08-01 11.25202     5  UTR\nR&gt; 8  Port Nolloth    DEA 1991-09-01 11.29208     5  UTR\nR&gt; 9  Port Nolloth    DEA 1991-10-01 11.37661     5  UTR\nR&gt; 10 Port Nolloth    DEA 1991-11-01 10.98208     5  UTR",
    "crumbs": [
      "Home",
      "Introduction to R",
      "15. Tidiest Data"
    ]
  },
  {
    "objectID": "intro_r/15-tidiest.html#create-a-new-dataframe-for-a-newly-created-variable-column-with-transmute",
    "href": "intro_r/15-tidiest.html#create-a-new-dataframe-for-a-newly-created-variable-column-with-transmute",
    "title": "15. Tidiest Data",
    "section": "7.2 Create a New Dataframe for a Newly Created Variable (Column) with transmute()",
    "text": "7.2 Create a New Dataframe for a Newly Created Variable (Column) with transmute()\nIf for whatever reason you wanted to create a new variable (column), as you would do with mutate(), but you do not want to keep the dataframe from which the new column was created, the function to use is transmute().\n\nSACTN %&gt;% \n  transmute(kelvin = temp + 273.15)\n\n\n\nR&gt;  [1] 284.6203 285.1441 285.1056 285.0118 285.3572 285.6881 284.4020 284.4421\nR&gt;  [9] 284.5266 284.1321\n\n\nThis makes a bit more sense when paired with group_by() as it will pull over the grouping variables into the new dataframe. Note that when it does this for you automatically it will provide a message in the console.\n\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  transmute(kelvin = temp + 273.15)\n\n\n\nR&gt; # A tibble: 10 × 3\nR&gt; # Groups:   site, src [1]\nR&gt;    site         src   kelvin\nR&gt;    &lt;fct&gt;        &lt;chr&gt;  &lt;dbl&gt;\nR&gt;  1 Port Nolloth DEA     285.\nR&gt;  2 Port Nolloth DEA     285.\nR&gt;  3 Port Nolloth DEA     285.\nR&gt;  4 Port Nolloth DEA     285.\nR&gt;  5 Port Nolloth DEA     285.\nR&gt;  6 Port Nolloth DEA     286.\nR&gt;  7 Port Nolloth DEA     284.\nR&gt;  8 Port Nolloth DEA     284.\nR&gt;  9 Port Nolloth DEA     285.\nR&gt; 10 Port Nolloth DEA     284.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "15. Tidiest Data"
    ]
  },
  {
    "objectID": "intro_r/15-tidiest.html#count-observations-rows-with-n",
    "href": "intro_r/15-tidiest.html#count-observations-rows-with-n",
    "title": "15. Tidiest Data",
    "section": "7.3 Count Observations (Rows) with n()",
    "text": "7.3 Count Observations (Rows) with n()\nYou have already seen this function sneak its way into a few of the code chunks in the previous session. You use n() to count any grouped variable automatically. It cannot be given any arguments, so you must organise our dataframe in order to satisfy its needs. It is the diva function of the tidyverse; however, it is terribly useful as you usually want to know how many observations your summary stats are based on.\n\n\n\n\n\n\nTipRead Grouped Output Carefully\n\n\n\nIf you see multiple rows after summarise(), you are still grouped.\nIf you use n() without grouping, it counts all rows.\n\n\nFirst you will run some stats, create a figure without documenting n. Then you will include n and see how that changes your conclusions.\n\n SACTN_n &lt;- SACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %&gt;% \n  arrange(mean_temp) %&gt;% \n  ungroup() %&gt;% \n  select(mean_temp) %&gt;% \n  unique()\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point() +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\nFigure 3: Dot plot showing range of mean temperatures for the time series in the SACTN dataset.\n\n\n\n\n\nThis looks like a pretty linear distribution of temperatures within the SACTN dataset. But now let us change the size of the dots to show how frequently each of these mean temperatures is occurring.\n\n SACTN_n &lt;- SACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %&gt;% \n  ungroup() %&gt;% \n  select(mean_temp) %&gt;% \n  group_by(mean_temp) %&gt;% \n  summarise(count = n())\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point(aes(size = count)) +\n  labs(x = \"\", y = \"Temperature (°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\nFigure 4: Dot plot showing range of mean temperatures for the time series in the SACTN dataset with the size of each dot showing the number of occurrences of each mean.\n\n\n\n\n\nYou see now when you include the count (n) of the different mean temperatures that this distribution is not so even. There appear to be humps around 17°C and 22°C. Of course, you have created dot plots here just to illustrate this point. In reality if you were interested in a distribution like this one would use a histogram, or better yet, a density polygon.\n\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(mean_temp = round(mean(temp, na.rm = T))\n            ) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(x = mean_temp)) +\n  geom_density(fill = \"seagreen\", alpha = 0.6) +\n  labs(x = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 5: Frequency distribution of mean temperature for each time series in the SACTN dataset.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "15. Tidiest Data"
    ]
  },
  {
    "objectID": "intro_r/15-tidiest.html#select-observations-rows-by-number-with-slice",
    "href": "intro_r/15-tidiest.html#select-observations-rows-by-number-with-slice",
    "title": "15. Tidiest Data",
    "section": "7.4 Select Observations (Rows) by Number with slice()",
    "text": "7.4 Select Observations (Rows) by Number with slice()\nIf you want to select only specific rows of a dataframe, rather than using some variable like you do for filter(), you use slice(). The function expects us to provide it with a series of integers as seen in the following code chunk. Try playing around with these values and see what happens\n\n# Slice a sequence of rows\nSACTN %&gt;% \n  slice(10010:10020)\n\n# Slice specific rows\nSACTN %&gt;%\n  slice(c(1,8,19,24,3,400))\n\n# Slice all rows except these\nSACTN %&gt;% \n  slice(-(c(1,8,4)))\n\n# Slice all rows except a sequence\nSACTN %&gt;% \n  slice(-(1:1000))\n\nIt is discouraged to use slice to remove or select specific rows of data as this does not discriminate against any possible future changes in ones data. Meaning that if at some point in the future new data are added to a dataset, re-running this code will likely no longer be selecting the correct rows. This is why filter() is a main function, slice() is not. This auxiliary function can however still be quite useful when combined with arrange.\nRule of thumb: if you care about reproducibility, prefer filter() over slice().\n\n# The top 5 variable sites as measured by SD\nSACTN %&gt;% \n  group_by(site, src) %&gt;% \n  summarise(sd_temp = sd(temp, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(sd_temp)) %&gt;% \n  slice(1:5)\n\nR&gt; # A tibble: 5 × 3\nR&gt;   site       src   sd_temp\nR&gt;   &lt;fct&gt;      &lt;chr&gt;   &lt;dbl&gt;\nR&gt; 1 Muizenberg SAWS     2.76\nR&gt; 2 Stilbaai   SAWS     2.72\nR&gt; 3 Mossel Bay SAWS     2.65\nR&gt; 4 De Hoop    DAFF     2.51\nR&gt; 5 Mossel Bay DEA      2.51",
    "crumbs": [
      "Home",
      "Introduction to R",
      "15. Tidiest Data"
    ]
  },
  {
    "objectID": "intro_r/15-tidiest.html#summary-functions",
    "href": "intro_r/15-tidiest.html#summary-functions",
    "title": "15. Tidiest Data",
    "section": "7.5 Summary Functions",
    "text": "7.5 Summary Functions\nThere is a near endless sea of possibilities when one starts to become comfortable with writing R code. You have seen several summary functions used thus far. Mostly in straightforward ways. But that is one of the fun things about R, the only limits to what you may create are within your mind, not the program. Here is just one example of a creative way to answer a straightforward question: ‘What is the proportion of recordings above 15°C per source?’. Note how you may refer to columns you have created within the same chunk. There is no need to save the intermediate dataframes if we choose not to.\n\nSACTN %&gt;% \n  na.omit() %&gt;% \n  group_by(src) %&gt;%\n  summarise(count = n(), \n            count_15 = sum(temp &gt; 15)) %&gt;% \n  mutate(prop_15 = count_15/count) %&gt;% \n  arrange(prop_15)\n\nR&gt; # A tibble: 7 × 4\nR&gt;   src   count count_15 prop_15\nR&gt;   &lt;chr&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;\nR&gt; 1 DAFF    641      246   0.384\nR&gt; 2 SAWS   8636     4882   0.565\nR&gt; 3 UWC      12        7   0.583\nR&gt; 4 DEA    2087     1388   0.665\nR&gt; 5 SAEON   596      573   0.961\nR&gt; 6 EKZNW   369      369   1    \nR&gt; 7 KZNSB 15313    15313   1\n\n\nThe output of grouped summaries like this is the shape most modelling and statistical functions expect: one row per unit, one column per variable. Keep that endpoint in mind as you design your pipelines.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "15. Tidiest Data"
    ]
  },
  {
    "objectID": "intro_r/17-functions.html#useful-information",
    "href": "intro_r/17-functions.html#useful-information",
    "title": "17. Functions by Chapter",
    "section": "1 Useful Information",
    "text": "1 Useful Information\n…incomplete…\n\n1.1 Operators\nThere are several operators you can use to help build expressions as shown in Table \\(\\ref{tab:operators}\\).\n\n\n1.2 Functions\nSome example functions covered so far are presented in Table \\(\\ref{tab:functions}\\).\nSome summary functions are presented in Table \\(\\ref{tab:summaries}\\).",
    "crumbs": [
      "Home",
      "Introduction to R",
      "17. Functions by Chapter"
    ]
  },
  {
    "objectID": "intro_r/19-dates.html",
    "href": "intro_r/19-dates.html",
    "title": "19. Dates",
    "section": "",
    "text": "This script covers some of the more common issues we may face while dealing with dates.\n\n\n\nDates\n\n\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(zoo)\n\n# Load data\nsad_dates &lt;- read.csv(here::here(\"data\", \"BCB744\", \"sad_dates.csv\"))\n\n\n1 Date Details\nLook at strip time format for guidance\n\n?strptime\n\nCheck the local time zone\n\nSys.timezone(location = TRUE)\n\nR&gt; [1] \"Africa/Johannesburg\"\n\n\n\n\n2 Creating Daily Dates\nCreate date columns out of the mangled date data we have loaded.\n\n# Create good date column\nnew_dates &lt;- sad_dates %&gt;%\n  mutate(new_good = as.Date(good))\n\n# Correct bad date column\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_bad = as.Date(bad, format = \"%m/%d/%y\"))\n\n# Correct ugly date column\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_ugly = seq(as.Date(\"1998-01-13\"), as.Date(\"1998-01-21\"), by = \"day\"))\n\n\n\n3 Creating Hourly Dates\nIf we want to create date values out of data that have hourly values (or smaller), we must create ‘POSIXct’ values because ‘Date’ values may not have a finer temporal resolution than one day.\n\n# Correcting good time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_good_hours = as.POSIXct(good_hours, tz = \"Africa/Mbabane\"))\n\n# Correcting bad time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_bad_hours = as.POSIXct(bad_hours, format = \"%Y-%m-%d %I:%M:%S %p\", tz = \"Africa/Mbabane\"))\n\n# Correcting bad time stamps with hours\nnew_dates &lt;- new_dates %&gt;%\n  mutate(new_ugly_hours = seq(as.POSIXct(\"1998-01-13 09:00:00\", tz = \"Africa/Mbabane\"),\n                              as.POSIXct(\"1998-01-13 17:00:00\", tz = \"Africa/Mbabane\"), by = \"hour\"))\n\nBut should not there be a function that loads dates correctly?\n\n\n4 Importing Dates in One Step\nWhy yes, yes there is. read_csv() is the way to go.\n\nsmart_dates &lt;- read_csv(here::here(\"data\", \"BCB744\", \"sad_dates.csv\"))\n\nBut why does it matter that we correct the values to dates? For starters, it affects the way our plots look/work. Let us create some random numbers for plotting, see how these compare against our date values when we create figures.\n\n# Generate random number\nsmart_dates$numbers &lt;- rnorm(9, 2, 10)\n\n# Scatterplot with correct dates\nggplot(smart_dates, aes(x = good, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n# Scatterplot with incorrect dates\nggplot(smart_dates, aes(x = bad, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n# OR\nggplot(smart_dates, aes(x = ugly, y = numbers)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\n\n\n\nFigure 1: Dates deeper 1.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Dates deeper 1.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Dates deeper 1.\n\n\n\n\n\nIf the dates are formatted correctly it also allows us to do schnazy things with the data.\n\nsmart_dates$good[4]+32\n\nR&gt; [1] \"1998-02-17\"\n\nsmart_dates$good[9]-smart_dates$good[3]\n\nR&gt; Time difference of 6 days\n\nas.Date(smart_dates$good[9]:smart_dates$good[3])\n\nR&gt; [1] \"1998-01-21\" \"1998-01-20\" \"1998-01-19\" \"1998-01-18\" \"1998-01-17\"\nR&gt; [6] \"1998-01-16\" \"1998-01-15\"\n\nsmart_dates$good[9]-10247\n\nR&gt; [1] \"1970-01-01\"\n\n\n\n\n\n\nCitationBibTeX citation:@online{a._j.2021,\n  author = {A. J. , Smit},\n  title = {19. {Dates}},\n  date = {2021-01-01},\n  url = {http://samos-r.netlify.app/intro_r/19-dates.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S (2021) 19. Dates. http://samos-r.netlify.app/intro_r/19-dates.html.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "19. Dates"
    ]
  },
  {
    "objectID": "pages/How_to_learn.html",
    "href": "pages/How_to_learn.html",
    "title": "How to Learn",
    "section": "",
    "text": "This page is a placeholder in the standalone SAMOS R site.\n\n\n\nCitationBibTeX citation:@online{a._j.,\n  author = {A. J. , Smit},\n  title = {How to {Learn}},\n  url = {http://samos-r.netlify.app/pages/How_to_learn.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S How to Learn. http://samos-r.netlify.app/pages/How_to_learn.html."
  },
  {
    "objectID": "pages/graduate_attributes.html",
    "href": "pages/graduate_attributes.html",
    "title": "Graduate Attributes",
    "section": "",
    "text": "This page is a placeholder in the standalone SAMOS R site.\nThe full version lives in the main Tangled Bank site.\n\n\n\nCitationBibTeX citation:@online{a._j.,\n  author = {A. J. , Smit},\n  title = {Graduate {Attributes}},\n  url = {http://samos-r.netlify.app/pages/graduate_attributes.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nA. J. S Graduate Attributes. http://samos-r.netlify.app/pages/graduate_attributes.html."
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-1",
    "href": "tasks/BCB744_Task_A.html#question-1",
    "title": "BCB744 Task A",
    "section": "Question 1",
    "text": "Question 1\nWhy will I get annoyed when you say in your Methods section of your paper that you used RStudio for your analysis? (/1)\nAnswer\n\n✓ I will get annoyed because RStudio is not a statistical computing software — it is an integrated development environment (IDE) for R. Stating in a Methods section that an analysis was conducted in “RStudio” conflates the software used for computation (R) with the interface used to interact with it (RStudio).\nFurther, reporting the actual software, package versions used is important for reproducibility. Stating “RStudio” obscures the fact that R (the statistical language) is what executes the computations and RStudio is just a tool that provides a user-friendly interface with features such as script editing, debugging, and visualisation conveniences.\nA proper citation in the Methods section should explicitly reference R (with its version number) and any relevant packages that were integral to the analysis. For example:\n\n\n“All analyses were conducted in R (v4.3.1; R Core Team, 2023) using the packages brms, ggplot2, and tidyverse.”\n\n\nYou can find the version number of R with the command citation(). Similarly, the citation for, say, ggplot2 can be found with citation(\"ggplot2).\nMentioning RStudio may be acceptable in passing if there is a specific reason, such as explaining how code was executed (e.g., using RMarkdown, Quarto within RStudio). However, in general, RStudio itself does not perform any computations, making its inclusion misleading and to someone who cares about precision in computational reporting, annoying.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-2",
    "href": "tasks/BCB744_Task_A.html#question-2",
    "title": "BCB744 Task A",
    "section": "Question 2",
    "text": "Question 2\nWhy is it best practice to include all packages you use in your R program explicitly? (/3)\nAnswer\n✓ Explicitly specifying R packages in your scripts ensures reproducibility, transparency, debugging efficiency, and dependency management. Consequently, you make your code robust (less prone to failure), interpretable (easy for others to read, understand) and easily executable across different environments (different people’s computers, or different kinds of operating systems).",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-3",
    "href": "tasks/BCB744_Task_A.html#question-3",
    "title": "BCB744 Task A",
    "section": "Question 3",
    "text": "Question 3\nWhat are the values after each hashed statement in the following? (/3)\n\nmass &lt;- 48 \nmass &lt;- mass * 2.0 # mass? \nage &lt;- 42\nage &lt;- age - 17 # age?\nmass_index &lt;- mass / age # mass_index?\n\nAnswer\n\nmass &lt;- 48 \nmass &lt;- mass * 2.0 # mass? \nmass # ✓\n\nR&gt; [1] 96\n\nage &lt;- 42\nage &lt;- age - 17 # age?\nage # ✓\n\nR&gt; [1] 25\n\nmass_index &lt;- mass / age # mass_index?\nmass_index # ✓\n\nR&gt; [1] 3.84",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-4",
    "href": "tasks/BCB744_Task_A.html#question-4",
    "title": "BCB744 Task A",
    "section": "Question 4",
    "text": "Question 4\nUse R to calculate some simple mathematical expressions. Assign the value of 40 to x and assign the value of 23 to y. Make z the value of x - y Display z in the console. (/3)\nAnswer\n\nx &lt;- 40\ny &lt;- 23\nz &lt;- x - y\nz  # ✓ x 3\n\nR&gt; [1] 17",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-5",
    "href": "tasks/BCB744_Task_A.html#question-5",
    "title": "BCB744 Task A",
    "section": "Question 5",
    "text": "Question 5\nExplain what this code does (below). What have you learned about writing code, and how would you apply what you have learned in the future? When would one want to use the round() function? Name a few example use cases. (/4)\n\nround(sd(apples), 2)\n\nAnswer\n\n✓ The round() function in R is used to round a numeric value to a specified number of decimal places. In this code snippet, the round() function is applied to the standard deviation of a numeric vector called apples. The second argument to round() is 2, which specifies that the standard deviation should be rounded to two decimal places.\n✓ This function highlights the importance of precision in numerical computations. Functions like round() are useful for data presentation, statistical reporting, and computational accuracy because they allow one to control the level of numerical detail in outputs, which is important in exploratory analysis, final reporting.\n✓ Using functions with clear, well-defined arguments (like specifying digits) improves code readability, reproducibility.\n✓ We also learned that we can nest function within one-another. Here, sd() is nested within round() to round the standard deviation to two decimal places.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-6",
    "href": "tasks/BCB744_Task_A.html#question-6",
    "title": "BCB744 Task A",
    "section": "Question 6",
    "text": "Question 6\nWhat is the difference between an Excel file and a CSV file? (/2)\nAnswer\n\n✓ Excel File: An Excel file is a proprietary file format used by Microsoft Excel to store data in a structured manner. It can contain multiple sheets, formulas, figures, and other features. Excel files are typically saved with the extension .xlsx (for newer versions).xls (for older versions). They are not plain text files and require specific software (like Excel) to open and edit.\n✓ CSV File: A CSV (Comma-Separated Values) file is a plain text file that stores tabular data in a simple format. Each line in a CSV file represents a row in the table, and columns are separated by commas (or sometime semi-colons). CSV files are human-readable, can be opened with any text editor or spreadsheet software. They are commonly used for data exchange between different programs and systems.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-7",
    "href": "tasks/BCB744_Task_A.html#question-7",
    "title": "BCB744 Task A",
    "section": "Question 7",
    "text": "Question 7\nWhat is the difference between a CSV and TSV file? (/2)\nAnswer\n\n✓ CSV File: A CSV (Comma-Separated Values) file is a plain text file that stores tabular data in a simple format. Each line in a CSV file represents a row in the table, and columns are separated by commas. CSV files are human-readable, can be opened with any text editor or spreadsheet software.\n✓ TSV File: A TSV (Tab-Separated Values) file is similar to a CSV file, but instead of using commas to separate columns, it uses tabs. Each line in a TSV file represents a row in the table, and columns are separated by tabs. TSV files are also human-readable, can be opened with any text editor or spreadsheet software.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-8",
    "href": "tasks/BCB744_Task_A.html#question-8",
    "title": "BCB744 Task A",
    "section": "Question 8",
    "text": "Question 8\nWhy is it important to see the file extension when working with data files? (/2)\nAnswer\n\n✓ File extensions are important because they provide information about the type of file and the software that can be used to open it. For example, .csv indicates a CSV file that can be opened with spreadsheet software or text editors, while .xlsx indicates an Excel file that requires Microsoft Excel to open.\n✓ Knowing the file extension helps in selecting the appropriate software to open the file, avoiding compatibility issues, and ensuring that the file is opened correctly. It also helps in identifying the file type quickly, especially when dealing with multiple files, file formats.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-9",
    "href": "tasks/BCB744_Task_A.html#question-9",
    "title": "BCB744 Task A",
    "section": "Question 9",
    "text": "Question 9\nUsing examples (new data), explain how the as.vector() function works when applied to matrices and arrays. How does it decide in what order to string the elements of the matrices and arrays together? (/6)\nAnswer\n\n✓ The as.vector() function in R is used to coerce an object to a vector. When applied to matrices and arrays, it flattens the object into a one-dimensional vector by concatenating the columns of the matrix, the elements of the array in a column-major order.\n✓ For matrices, the elements are concatenated column-wise, meaning that the first column is followed by the second column, and so on. For arrays, the elements are concatenated along the last dimension first, then the second-to-last dimension, and so on, until the first dimension.\n✓ The order in which the elements are strung together is determined by the storage mode of the object. In R, matrices, arrays are stored in column-major order and meaning that the elements are stored column-wise in memory. When as.vector() is applied, it follows this order to concatenate the elements into a vector.\n\n\n# Example with a matrix ✓ x 3\n\n# Create a matrix\nmat &lt;- matrix(1:6, nrow = 2)\n\n# Convert the matrix to a vector\nvec &lt;- as.vector(mat)\n\n# Display the matrix and vector\nmat\n\nR&gt;      [,1] [,2] [,3]\nR&gt; [1,]    1    3    5\nR&gt; [2,]    2    4    6\n\nvec\n\nR&gt; [1] 1 2 3 4 5 6\n\n# Example with an array\n\n# Create an array\narr &lt;- array(1:8, dim = c(2, 2, 2))\n\n# Convert the array to a vector\nvec_arr &lt;- as.vector(arr)\n\n# Display the array and vector\narr\n\nR&gt; , , 1\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,]    1    3\nR&gt; [2,]    2    4\nR&gt; \nR&gt; , , 2\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,]    5    7\nR&gt; [2,]    6    8\n\nvec_arr\n\nR&gt; [1] 1 2 3 4 5 6 7 8",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-10",
    "href": "tasks/BCB744_Task_A.html#question-10",
    "title": "BCB744 Task A",
    "section": "Question 10",
    "text": "Question 10\nUse the result produced by as.vector() (your own data) and assemble three new arrays with a different combinations of dimensions. Show the dimensions and lengths of the new arrays. (/14)\nAnswer\n\n# ✓ Create a vector\nvec &lt;- 1:12\n\n# ✓ Display the vector\nvec\n\nR&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n# ✓ Convert the vector to an array with different dimensions\nnew_arr1 &lt;- array(vec, dim = c(2, 3, 2))\n\n# ✓ Display the new array\nnew_arr1\n\nR&gt; , , 1\nR&gt; \nR&gt;      [,1] [,2] [,3]\nR&gt; [1,]    1    3    5\nR&gt; [2,]    2    4    6\nR&gt; \nR&gt; , , 2\nR&gt; \nR&gt;      [,1] [,2] [,3]\nR&gt; [1,]    7    9   11\nR&gt; [2,]    8   10   12\n\n# ✓ Check the dimensions of the new array\ndim(new_arr1)\n\nR&gt; [1] 2 3 2\n\n# Check the class of the new array\nclass(new_arr1)\n\nR&gt; [1] \"array\"\n\n# Check the structure of the new array\nstr(new_arr1)\n\nR&gt;  int [1:2, 1:3, 1:2] 1 2 3 4 5 6 7 8 9 10 ...\n\n# ✓ Check the length (number of elements) of the new array\nlength(new_arr1)\n\nR&gt; [1] 12\n\n# Check the number of dimensions of the new array\nlength(dim(new_arr1))\n\nR&gt; [1] 3\n\n# ✓ A variation of `vec` with different dimensions\nnew_arr2 &lt;- array(vec, dim = c(3, 2, 2))\n\n# ✓ Display the new array\nnew_arr2\n\nR&gt; , , 1\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,]    1    4\nR&gt; [2,]    2    5\nR&gt; [3,]    3    6\nR&gt; \nR&gt; , , 2\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,]    7   10\nR&gt; [2,]    8   11\nR&gt; [3,]    9   12\n\n# ✓ Check the dimensions of the new array\ndim(new_arr2)\n\nR&gt; [1] 3 2 2\n\n# ✓ Check the length (number of elements) of the new array\nlength(new_arr2)\n\nR&gt; [1] 12\n\n# ✓ ✓ ✓ ✓ A third variation of `vec` with different dimensions\nnew_arr3 &lt;- array(vec, dim = c(2, 2, 3))\nnew_arr3\n\nR&gt; , , 1\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,]    1    3\nR&gt; [2,]    2    4\nR&gt; \nR&gt; , , 2\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,]    5    7\nR&gt; [2,]    6    8\nR&gt; \nR&gt; , , 3\nR&gt; \nR&gt;      [,1] [,2]\nR&gt; [1,]    9   11\nR&gt; [2,]   10   12\n\nlength(new_arr3)\n\nR&gt; [1] 12\n\ndim(new_arr3)\n\nR&gt; [1] 2 2 3",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-11",
    "href": "tasks/BCB744_Task_A.html#question-11",
    "title": "BCB744 Task A",
    "section": "Question 11",
    "text": "Question 11\nWhat is the purpose of commenting code? Name at least three reasons why you should comment your code. (/3)\nAnswer\n\n✓ Commenting code makes your code more readable, understandable, and maintainable. Comments provide context, explanations, and documentation about the code, helping you (and your future self), others understand the purpose of the code and the logic behind it, and how it works.\n✓ Comments can also serve as reminders, placeholders, or to-do lists for future work. They help in debugging, troubleshooting, and modifying code by providing insights into the code structure, functionality.\n✓ Commenting code is a good practice in programming and data analysis because it promotes collaboration, knowledge sharing, and code quality. It is essential for effective communication, ensuring that the codebase remains comprehensible and usable over time.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-12",
    "href": "tasks/BCB744_Task_A.html#question-12",
    "title": "BCB744 Task A",
    "section": "Question 12",
    "text": "Question 12\nWhy am I pedantic about using commas and periods correctly in my code? Name some use cases of commas and periods. (/3)\nAnswer\n\n✓ The SI system of units uses commas and periods in a specific way: periods are used for decimal points, while commas are used to separate thousands.\n✓ Using commas and periods correctly in your code is important for readability, clarity, and consistency. Commas are used to separate elements in a vector, list or arguments in a function call.\n✓ Incorrect usage of commas and periods can lead to syntax errors, logical errors, or unexpected behaviour in your code. It can make the code difficult to understand, debug, and maintain, especially for others who read, work with the code.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-13",
    "href": "tasks/BCB744_Task_A.html#question-13",
    "title": "BCB744 Task A",
    "section": "Question 13",
    "text": "Question 13\nCreate a script to read in the file crops.xlsx (via a CSV file that you prepare beforehand) and assign its content to the object crops. Display the content of the dataframe. (/3)\nAnswer\n\n✓ First convert the Excel file to a CSV file using Excel or an online converter such as ChatGPT.\n\n\n# Read the CSV file into R \ncrops &lt;- read.csv(\"crops.csv\") # ✓ \nhead(crops) # ✓",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-14",
    "href": "tasks/BCB744_Task_A.html#question-14",
    "title": "BCB744 Task A",
    "section": "Question 14",
    "text": "Question 14\nSave the newly-created object to a CSV file called crops2.csv within your workspace. (/1)\nAnswer\n\n# Save the object to a CSV file\nwrite.csv(crops, \"crops2.csv\", row.names = FALSE) # ✓",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-15",
    "href": "tasks/BCB744_Task_A.html#question-15",
    "title": "BCB744 Task A",
    "section": "Question 15",
    "text": "Question 15\nWhat purpose can the naming of a newly-created dataframe serve? Name at least five reasons. (/5)\nAnswer\n\nNaming a newly-created dataframe serves several purposes:\n\n✓ Clarity: A descriptive name can help you and others understand the content or purpose of the dataframe.\n✓ Readability: A well-chosen name makes the code more readable and easier to follow.\n✓ Documentation: The name can serve as a form of documentation, providing context, information about the dataframe.\n✓ Organisation: Naming conventions can help organise and manage dataframes in a project or analysis.\n✓ Consistency: Consistent naming practices across dataframes improve code consistency and maintainability.\n✓ Debugging: A meaningful name can aid in debugging and troubleshooting code.\n✓ Reusability: A good name can make the dataframe more reusable in different parts of the code or in other projects.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-16",
    "href": "tasks/BCB744_Task_A.html#question-16",
    "title": "BCB744 Task A",
    "section": "Question 16",
    "text": "Question 16\nUsing annotated R code, demonstrate your understanding of the various ways to look inside of the crops object. Show at least five different ways to inspect the dataframe. (/5)\nAnswer\n\n# ✓ Display the structure of the dataframe\nstr(crops)\n\nR&gt; tibble [96 × 4] (S3: tbl_df/tbl/data.frame)\nR&gt;  $ density   : num [1:96] 1 2 1 2 1 2 1 2 1 2 ...\nR&gt;  $ block     : chr [1:96] \"north\" \"east\" \"south\" \"west\" ...\nR&gt;  $ fertilizer: chr [1:96] \"A\" \"A\" \"A\" \"A\" ...\nR&gt;  $ mass      : num [1:96] 4823 4832 4801 4836 4821 ...\n\n# ✓ Display the first few rows of the dataframe\nhead(crops)\n\nR&gt; # A tibble: 6 × 4\nR&gt;   density block fertilizer  mass\nR&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\nR&gt; 1       1 north A          4823.\nR&gt; 2       2 east  A          4832.\nR&gt; 3       1 south A          4801.\nR&gt; 4       2 west  A          4836.\nR&gt; 5       1 north A          4821.\nR&gt; 6       2 east  A          4811.\n\n# ✓ Display the last few rows of the dataframe\ntail(crops)\n\nR&gt; # A tibble: 6 × 4\nR&gt;   density block fertilizer  mass\nR&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\nR&gt; 1       1 south C          4822.\nR&gt; 2       2 west  C          4828.\nR&gt; 3       1 north C          4848.\nR&gt; 4       2 east  C          4836.\nR&gt; 5       1 south C          4836.\nR&gt; 6       2 west  C          4820.\n\n# ✓ Display the summary statistics of the dataframe\nsummary(crops)\n\nR&gt;     density       block            fertilizer             mass     \nR&gt;  Min.   :1.0   Length:96          Length:96          Min.   :4773  \nR&gt;  1st Qu.:1.0   Class :character   Class :character   1st Qu.:4803  \nR&gt;  Median :1.5   Mode  :character   Mode  :character   Median :4819  \nR&gt;  Mean   :1.5                                         Mean   :4818  \nR&gt;  3rd Qu.:2.0                                         3rd Qu.:4828  \nR&gt;  Max.   :2.0                                         Max.   :4873\n\n# ✓ Display the dimensions of the dataframe\ndim(crops)\n\nR&gt; [1] 96  4\n\n# ✓ Display the column names of the dataframe\ncolnames(crops)\n\nR&gt; [1] \"density\"    \"block\"      \"fertilizer\" \"mass\"\n\n# ✓ Glimpse into the dataframe\nglimpse(crops)\n\nR&gt; Rows: 96\nR&gt; Columns: 4\nR&gt; $ density    &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,…\nR&gt; $ block      &lt;chr&gt; \"north\", \"east\", \"south\", \"west\", \"north\", \"east\", \"south\",…\nR&gt; $ fertilizer &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\nR&gt; $ mass       &lt;dbl&gt; 4823.367, 4832.113, 4801.044, 4836.293, 4820.559, 4811.111,…\n\n# ✓ Look at the names\nnames(crops)\n\nR&gt; [1] \"density\"    \"block\"      \"fertilizer\" \"mass\"\n\n# ✓ Use the skimr package to get a summary of the dataframe\nskimr::skim(crops)\n\n\nData summary\n\n\nName\ncrops\n\n\nNumber of rows\n96\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nblock\n0\n1\n4\n5\n0\n4\n0\n\n\nfertilizer\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndensity\n0\n1\n1.50\n0.50\n1.00\n1.00\n1.50\n2.00\n2.00\n▇▁▁▁▇\n\n\nmass\n0\n1\n4817.56\n18.09\n4772.53\n4802.68\n4818.72\n4827.99\n4873.23\n▂▅▇▃▁\n\n\n\n\n# ✓ Display the data types of the columns\nsapply(crops, class)\n\nR&gt;     density       block  fertilizer        mass \nR&gt;   \"numeric\" \"character\" \"character\"   \"numeric\"",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-17",
    "href": "tasks/BCB744_Task_A.html#question-17",
    "title": "BCB744 Task A",
    "section": "Question 17",
    "text": "Question 17\nExplain what you see inside the file. What are the columns? What are the rows? What are the data types? (/5)\nAnswer\n\n✓ The crops dataframe contains the columns density, block, fertiliser, mass. The rows represent individual observations or measurements of crop mass under different conditions.\n✓ The density column likely represents the density of the crop, block represents the experimental block, fertiliser represents the type of fertiliser used, mass represents the mass of the crop.\n✓ The data types of the columns can be inferred from the output of str(crops) and sapply(crops, class). For example, density and mass are numeric or integer, while block and fertiliser are characters or factors.\n✓ The density ranges from a minimum of 1 to a maximum of 2. The fertiliser is a factor with levels A, B, C, the block is a character vector with levels north, south, east, west. The mass ranges from a minimum of 4773 to a maximum of 4873.\n✓ There are 96 rows in the dataframe, representing 96 observations, measurements.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-18",
    "href": "tasks/BCB744_Task_A.html#question-18",
    "title": "BCB744 Task A",
    "section": "Question 18",
    "text": "Question 18\nExplain in words what the pipe operator %&gt;% does in R. How does it make your code more readable? (/3)\nAnswer\n\n✓ The pipe operator %&gt;% (or |&gt;) in R is used to chain together multiple functions or operations in a sequence. It takes the output of one function and passes it as the first argument to the next function, allowing you to create a pipeline of operations.\n✓ The pipe operator makes your code more readable by breaking down complex operations into a series of simpler steps. It helps in avoiding nested function calls, improves code clarity, and reduces the need for intermediate variables.\n✓ In this way you can write code in a more linear and intuitive way, following the flow of data transformations from one step to the next. This makes it easier to understand the logic of the code, the sequence of operations being performed.",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_A.html#question-19",
    "href": "tasks/BCB744_Task_A.html#question-19",
    "title": "BCB744 Task A",
    "section": "Question 19",
    "text": "Question 19\nUsing the various tidyverse functions, calculate the mean ± SD for the crop mass within each combination of block and fertiliser of the crops dataset. (/5)\nAnswer\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n# Calculate the mean ± SD for crop mass within each combination of block and fertiliser\ncrops %&gt;%\n  group_by(block, fertilizer) %&gt;%\n  summarise(mean_mass = mean(mass), sd_mass = sd(mass))\n\nR&gt; # A tibble: 12 × 4\nR&gt; # Groups:   block [4]\nR&gt;    block fertilizer mean_mass sd_mass\nR&gt;    &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\nR&gt;  1 east  A              4826.    16.6\nR&gt;  2 east  B              4817.    19.9\nR&gt;  3 east  C              4834.    13.8\nR&gt;  4 north A              4803.    19.4\nR&gt;  5 north B              4813.    12.1\nR&gt;  6 north C              4823.    14.6\nR&gt;  7 south A              4800.    12.7\nR&gt;  8 south B              4809.    17.6\nR&gt;  9 south C              4819.    13.7\nR&gt; 10 west  A              4812.    16.4\nR&gt; 11 west  B              4822.    11.4\nR&gt; 12 west  C              4832.    20.2",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task A"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_Bonus.html#question-1",
    "href": "tasks/BCB744_Task_Bonus.html#question-1",
    "title": "BCB744 Bonus Task",
    "section": "2.1 Question 1",
    "text": "2.1 Question 1\nPlease recreate the figure, above. You are welcome to reuse the code found on the website. Using this figure as starting point, do the following:\nWhen plotting the earthquakes, include only the earthquake data for earthquakes of magnitude greater than the 75th percentile. Add a point for five of your favourite South Pacific island nations. Ensure the point is correctly associated with the island name, that the map is correctly labelled and has a title, and it is as close to publication quality as you can make it. Your script needs to show all the steps (thoroughly annotated) leading to the final figure.\nMarks will also be assigned for the overall aethetic appearance of the map. Feel free to be creative, but ensure the final product remains publication quality. (\\30)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Bonus Task"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_Bonus.html#question-2",
    "href": "tasks/BCB744_Task_Bonus.html#question-2",
    "title": "BCB744 Bonus Task",
    "section": "2.2 Question 2",
    "text": "2.2 Question 2\nSuccessfully completing on of the options available in this task will earn you a bonus of up to 8 or 10% onto your CA mark.\nYou have until 31 March 2025 to complete it.\n\nA map that is worthy of display will become a large format poster to display in the BCB Department. Your name displayed next to it will immortalise you for continued fame and glory amongst future BCB students.\nThe winner of each category of map (hypometric and non-hypsometric) will also get a box of Lindt chocolate.\n\nOption 1 [up to 10% bonus]: Create a hypsometric map based on these examples\n \nThe maps show the locations of linefish catches along the SA coast as per a DFFE dataset. I do not expect that you add these data points as you do not have access to this dataset. However, the location of the 58 coastal sections indicated by circles can be plotted using the data provided here. You are also welcome to create a map of any topographically-interesting region on Earth, but be sure to include a few data points of some kind to draw our attention to some interesting features, statistics. Be creative!\nSince I think a few of you might actually accomplish this, best add a few improvements to it to make your map even better than mine, stand out from that of your peers. There can be only one winner in each category and the best one wins (although everyone can benefit from the bonus marks).\nWarning: You will need a fairly beefy computer to accomplish this task.\nOption 2 [up to 8% bonus]: Create an artistic map of your choice\nAlternatively, if you cannot access a powerful computer, for a bonus of up to 8% onto your CAM, create any (non-hypsometric) map of your choice of any region on Earth. Make something that you would be proud to display as a large format poster. The map may draw attention to an interesting regional geophysical, ecological, or socio-ecological (etc.) phenomena, or it may simply showcase your unique (but tasteful!) artistic ability. Show me some examples of what you wish to create before you start to avoid wasting your time on something too simple, entirely tasteless. There are many examples of beautiful maps on the internet that you may use as source of inspiration.\nWhichever option you choose, please also submit your code together with the final product in a well-described Quarto .html document. Explain each step of the way, describe the rationale for the approach you take.\nGood luck!",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Bonus Task"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_D.html#question-1",
    "href": "tasks/BCB744_Task_D.html#question-1",
    "title": "BCB744 Bonus Task",
    "section": "2.1 Question 1",
    "text": "2.1 Question 1\nWhat are the key principles of tidy data? (/3)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Bonus Task"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_D.html#question-2",
    "href": "tasks/BCB744_Task_D.html#question-2",
    "title": "BCB744 Bonus Task",
    "section": "2.2 Question 2",
    "text": "2.2 Question 2\nUsing the untidy data (SACTN2) and the tidy data (SACTN2_tidy), create line graphs, one for each of DEA, SAWS, and KZNSB, showing a time series of temperature. Ensure you have a column of three figures (ncol = 1). Use the fewest number of lines of code possible. You should end up with two graphs, each with three panels. (/13)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Bonus Task"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_F.html#question-1",
    "href": "tasks/BCB744_Task_F.html#question-1",
    "title": "BCB744 Task F",
    "section": "2.1 Question 1",
    "text": "2.1 Question 1\nPlease report back on Task F.1 presented in the lecture. Write up formal Methods and Results sections. (/15)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task F"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_F.html#question-2",
    "href": "tasks/BCB744_Task_F.html#question-2",
    "title": "BCB744 Task F",
    "section": "2.2 Question 2",
    "text": "2.2 Question 2\nPlease refer to the two-sided two-sample t-test in the lecture. It is recreated here:\n\n# random normal data\nset.seed(666)\nr_two &lt;- data.frame(dat = c(rnorm(n = 20, mean = 4, sd = 1),\n                            rnorm(n = 20, mean = 5, sd = 1)),\n                    sample = c(rep(\"A\", 20), rep(\"B\", 20)))\n\n# perform t-test\n# note how we set the `var.equal` argument to TRUE because we know \n# our data has the same SD (they are simulated as such!)\nt.test(dat ~ sample, data = r_two, var.equal = TRUE)\n\nR&gt; \nR&gt;  Two Sample t-test\nR&gt; \nR&gt; data:  dat by sample\nR&gt; t = -1.9544, df = 38, p-value = 0.05805\nR&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0\nR&gt; 95 percent confidence interval:\nR&gt;  -1.51699175  0.02670136\nR&gt; sample estimates:\nR&gt; mean in group A mean in group B \nR&gt;        4.001438        4.746584\n\n\n\nRepeat this analyses using the Welch’s t.test(). (/5)\nRepeat your analysis, above, using the even more old-fashioned Equation 4 in the lecture. Show the code, talk us through the step you followed to read the p-values off the table of t-statistics. (/10)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task F"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_F.html#question-3",
    "href": "tasks/BCB744_Task_F.html#question-3",
    "title": "BCB744 Task F",
    "section": "2.3 Question 3",
    "text": "2.3 Question 3\nPlease report back on Task F.3 presented in the lecture. Write up formal Methods and Results sections. (/15)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task F"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_F.html#question-4",
    "href": "tasks/BCB744_Task_F.html#question-4",
    "title": "BCB744 Task F",
    "section": "2.4 Question 4",
    "text": "2.4 Question 4\nPlease report back the analysis and results for Task F.4. in the lecture. Write up formal Methods and Results sections. (/15)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task F"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_H.html#question-1",
    "href": "tasks/BCB744_Task_H.html#question-1",
    "title": "BCB744 Task G",
    "section": "2.1 Question 1",
    "text": "2.1 Question 1\n\nExamine the content of the regression model object sparrows.lm produced in the linear regression chapter. Explain the meaning of the components within, and tell us how they relate to the model summary produced by summary(sparrows.lm). (/5)\nUsing the values inside of the model object, write some R code to show how you can reconstruct the observed values for the dependent variable from the residuals, the fitted values. (/5)\nFit a linear regression through the model residuals (use sparrows.lm). Explain your findings. (/5)\nSimilarly, fit a linear regression through the fitted values. Explain. (/5)",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_H.html#question-2",
    "href": "tasks/BCB744_Task_H.html#question-2",
    "title": "BCB744 Task G",
    "section": "2.2 Question 2",
    "text": "2.2 Question 2\nFind your own two datasets and do a full regression analysis on it. Briefly describe the data and the reason for their existence. Start with setting the appropriate hypotheses. Follow with an EDA, make some exploratory figures, fit the linear model, make a figure with the fitted linear model, provide diagnostic plots to test assumptions, and present the output in a Results section suitable for publication. (/20)\n\n2.2.1 Rubric\nCriterion | Excellent (Full Marks) | Partial Credit | Absent / Poor | Marks |\n— — — — — | — — — — — — — — — –| — — — — — — — | — — — — — — –| — — — –|\n1. Dataset Choice and Justification (/2) | Two datasets are clearly described, relevant, and non-trivial; rationale for use is coherent, shows critical thought. | Datasets are described but may be simplistic or rationale is weakly justified. | Datasets are vague and trivial, or arbitrarily chosen; little to no justification. | /2 |\n2. Hypothesis Framing (/2) | Null and alternative hypotheses are clearly stated in statistical terms and contextualised to the data. | Hypotheses are present but somewhat vague or generic. | Hypotheses are missing, poorly framed, or irrelevant. | /2 |\n3. Exploratory Data Analysis (/3) | EDA is systematic and insightful, summarising variable distributions, identifying patterns, and flagging potential issues (e.g., collinearity, missingness). | Some descriptive statistics, plots are provided but interpretation is limited or scattered. | Minimal or no EDA; data analysed without exploration. | /3 |\n4. Exploratory Figures (/2) | Visuals (e.g., scatterplots, histograms) are well-chosen, clearly labelled, and aid interpretation. | Plots are included but may be unclear, redundant, or poorly formatted. | Plots are missing, irrelevant. | /2 |\n5. Model Specification and Fitting (/3) | Linear model is appropriate for the data and research question; code and output are clearly reported. | Model is mostly appropriate but poorly justified or inconsistently executed. | Model is ill-suited, poorly fitted, or lacks documentation. | /3 |\n6. Visualisation of Fitted Model (/2) | Plot with fitted regression line is clear, with aesthetic attention to axis labels, units, and legends; enhances interpretation. | A fitted model is plotted but poorly presented, not fully interpretable. | No fitted model plot or plot is meaningless. | /2 |\n7. Diagnostic Checks (/3) | At least two appropriate diagnostic plots (e.g., residuals vs fitted, QQ-plot) are shown, interpreted in light of linear model assumptions (normality and etc.). | Diagnostics shown but with weak interpretation or partially inappropriate diagnostics. | Diagnostics missing, or plots are included without explanation. | /3 |\n8. Written Results Section (/3) | Results are reported in a concise, publication-ready format with appropriate terminology (e.g., coefficient estimates, p-values, R²), clear narrative flow. | Results are understandable but lack polish and completeness, or clarity; some technical terms used inaccurately. | Results are disorganised, incorrectly interpreted, or copied without synthesis. | /3 |\nTotal: /20",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "tasks/BCB744_Task_H.html#question-3",
    "href": "tasks/BCB744_Task_H.html#question-3",
    "title": "BCB744 Task G",
    "section": "3.1 Question 3",
    "text": "3.1 Question 3\nFind your own two datasets and do a full correlation analysis on it. Briefly describe the data and the reason for their existence. Start with setting the appropriate hypotheses. Follow with an EDA, make some exploratory figures, fit the correlation, make figures with the fitted correlation line, provide diagnostic plots to test assumptions, and present the output in a Results section suitable for publication. (/20)\n\n3.1.1 Rubric\nCriterion | Excellent (Full Marks) | Partial Credit | Absent / Poor | Marks |\n— — — — — | — — — — — — — — — –| — — — — — — — | — — — — — — –| — — — –|\n1. Dataset Choice and Justification (/2) | Two variables (from one or more datasets) are clearly described and justified as candidates for correlation analysis; rationale is thoughtful and contextually informed. | Variables are chosen and described but rationale is vague or unconvincing. | Variable selection appears arbitrary or trivial; little or no justification given. | /2 |\n2. Hypothesis Framing (/2) | Null and alternative hypotheses are explicitly stated and aligned with the correlation analysis (e.g., H₀: ρ = 0). Contextual meaning is clearly explained. | Hypotheses are present but poorly articulated, lacking in contextual relevance. | Hypotheses are missing or incorrect, or misaligned with the analysis. | /2 |\n3. Exploratory Data Analysis (/3) | EDA includes summary statistics, variable distribution inspection, and consideration of linearity, monotonicity. Potential issues (e.g. or outliers) are noted. | EDA is attempted but lacks depth or overlooks important features such as skewness or linearity. | No meaningful EDA is performed before conducting correlation. | /3 |\n4. Exploratory Figures (/2) | Appropriate visualisation (e.g., scatterplot with smoothing line, marginal histograms) is clear or labelled, and supports interpretation. | Plot is included but unclear, poorly formatted, or not well interpreted. | No plot provided, or plot is irrelevant, uninformative. | /2 |\n5. Correlation Method and Calculation (/3) | Correlation method is appropriate to data characteristics (Pearson/Spearman chosen with justification). Code and output are correct and clearly reported. | Method is used correctly but without justification or with some reporting issues. | Correlation is applied blindly or incorrectly; code or output is missing. | /3 |\n6. Significance and Effect Size (/2) | p-value and correlation coefficient (r or ρ) are reported with interpretation of both statistical and practical significance. | Results are reported but not clearly interpreted or contextualised. | Misinterpretation of p-value or correlation coefficient; missing output. | /2 |\n7. Assumption Checking and Discussion (/3) | Addresses assumptions of correlation method (e.g., normality, linearity, absence of outliers), supported by appropriate plots, discussion. | Some assumptions discussed or partially checked; reasoning may be unclear. | No discussion or evidence of assumption checking. | /3 |\n8. Written Results Section (/3) | Results are presented in a clear, concise, publication-ready format, with technical correctness, logical flow from EDA to conclusion. | Results are readable but disorganised or use imprecise language; conclusions may not follow cleanly from evidence. | Results are unclear and incorrect, or unstructured; poor communication of findings. | /3 |\nTotal: /20",
    "crumbs": [
      "Home",
      "Tasks",
      "BCB744 Task G"
    ]
  },
  {
    "objectID": "intro_r/06-graphics.html#first-plot-a-classic-temperaturesalinity-ts-view",
    "href": "intro_r/06-graphics.html#first-plot-a-classic-temperaturesalinity-ts-view",
    "title": "6. Graphics with ggplot2",
    "section": "4.1 First plot: a classic Temperature–Salinity (T–S) view",
    "text": "4.1 First plot: a classic Temperature–Salinity (T–S) view\nIn oceanography, temperature and salinity are often plotted against each other to reveal water-mass structure. Here we do a simple version of that idea using surface climatology (0 m) for February.\n\nwoa %&gt;%\n  filter(month == 2, depth_m == 0, variable %in% c(\"temperature\", \"salinity\")) %&gt;%\n  select(lon, lat, variable, value) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;%\n  ggplot(aes(x = salinity, y = temperature)) +\n  geom_point(alpha = 0.35, size = 0.8) +\n  labs(x = \"Salinity (PSU)\", y = \"Temperature (°C)\")\n\n\n\n\n\n\n\nFigure 1: WOA18 surface (0 m) February climatology: temperature vs salinity.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "6. Graphics with **ggplot2**"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#line-graph-a-latitudinal-transect-style-summary",
    "href": "intro_r/07-faceting.html#line-graph-a-latitudinal-transect-style-summary",
    "title": "7. Faceting Figures",
    "section": "3.1 Line Graph (a latitudinal transect style summary)",
    "text": "3.1 Line Graph (a latitudinal transect style summary)\n\nline_1 &lt;- woa_feb_temp_depths %&gt;%\n  group_by(depth_m, lat) %&gt;%\n  summarise(value = mean(value, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = lat, y = value, colour = factor(depth_m))) +\n  geom_line(linewidth = 0.8) +\n  labs(x = \"Latitude (°N)\", y = \"Temperature (°C)\", colour = \"Depth (m)\") +\n  theme_minimal()\nline_1\n\n\n\n\n\n\n\nFigure 2: Mean February temperature by latitude, shown separately for several depths.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#smooth-model-loess",
    "href": "intro_r/07-faceting.html#smooth-model-loess",
    "title": "7. Faceting Figures",
    "section": "3.2 Smooth Model (LOESS)",
    "text": "3.2 Smooth Model (LOESS)\n\nsm_1 &lt;- woa_feb_temp_depths %&gt;%\n  ggplot(aes(x = lat, y = value, colour = factor(depth_m))) +\n  geom_point(alpha = 0.15, size = 0.6) +\n  geom_smooth(se = FALSE, method = \"loess\", linewidth = 0.8) +\n  labs(x = \"Latitude (°N)\", y = \"Temperature (°C)\", colour = \"Depth (m)\") +\n  theme_minimal()\nsm_1\n\n\n\n\n\n\n\nFigure 3: Same idea, but using a smooth to emphasise broad structure (demonstration).",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#histogram-distribution-of-a-chemical-variable",
    "href": "intro_r/07-faceting.html#histogram-distribution-of-a-chemical-variable",
    "title": "7. Faceting Figures",
    "section": "3.3 Histogram (distribution of a chemical variable)",
    "text": "3.3 Histogram (distribution of a chemical variable)\n\nhistogram_1 &lt;- woa_feb_surf %&gt;%\n  filter(variable == \"nitrate\") %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 40, fill = \"grey60\", colour = \"white\") +\n  labs(x = \"Nitrate (µmol/kg)\", y = \"Count\") +\n  theme_minimal()\nhistogram_1\n\n\n\n\n\n\n\nFigure 4: Surface February nitrate distribution (WOA18 climatology).",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "intro_r/07-faceting.html#boxplot-compare-depths",
    "href": "intro_r/07-faceting.html#boxplot-compare-depths",
    "title": "7. Faceting Figures",
    "section": "3.4 Boxplot (compare depths)",
    "text": "3.4 Boxplot (compare depths)\n\nbox_1 &lt;- woa_feb_temp_depths %&gt;%\n  ggplot(aes(x = factor(depth_m), y = value)) +\n  geom_boxplot(fill = \"cornsilk\", outlier.alpha = 0.15) +\n  labs(x = \"Depth (m)\", y = \"Temperature (°C)\") +\n  theme_minimal()\nbox_1\n\n\n\n\n\n\n\nFigure 5: Temperature distributions by depth (February climatology).",
    "crumbs": [
      "Home",
      "Introduction to R",
      "7. Faceting Figures"
    ]
  },
  {
    "objectID": "index.html#pivot_longer",
    "href": "index.html#pivot_longer",
    "title": "13. Tidy Data",
    "section": "18.1 pivot_longer()",
    "text": "18.1 pivot_longer()\n\n\n\n\n\n\nTipDiagnostic: When Do I Need pivot_longer()?\n\n\n\nAsk yourself:\n\nDo my column names encode data values? (e.g., “DEA”, “SAWS”)\nIs one variable split across multiple columns?\n\nIf yes, you likely need pivot_longer().\n\n\nThe R function pivot_longer() is a useful tool for transforming data from wide to long format. It belongs to the tidyr package (loaded with tidyverse) and allows you to reshape your dataframe by gathering multiple columns into key-value pairs. Specifically, pivot_longer() takes in a dataframe and allows you to select a set of columns that you would like to pivot into longer format, while specifying the names of the key, value columns that you want to create. The resulting data frame will have a new row for each unique combination of key and value pairs. This function is particularly useful when you need to reshape your data in order to carry out certain analyses or visualisations.\nHave a look now at SACTN2 for an example of what wide data look like, and how to fix it.\nIn SACTN2 you can see that the src column has been removed and that the temperatures are placed in columns that denote the collecting source. This may at first seem like a reasonable way to organise these data, but it is not tidy because the collecting source is one variable, and so should not take up more than one column (i.e., there are multiple observations per row). You need to gather these source columns together into one column so that the separate measurements (observations) can conform to the one observation per row rule. You do this by telling pivot_longer() the names of the columns you want to squish together. You then tell it the name of the key (names_to) column. This is the column that will contain all of the old column names we are gathering. In this case you may call it source. The last piece of this puzzle is the value (values_to) column. This is where you decide what the name of the column will be for measurements you are gathering up. In this case you may name it temperature, because you are gathering up the temperature values that were incorrectly spread out by the source of the measurements.\n{r code-tidy-2, eval=FALSE} SACTN2_tidy &lt;- pivot_longer(SACTN2, cols = c(\"DEA\", \"KZNSB\", \"SAWS\"),                             names_to = \"src\",                             values_to = \"temp\")",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#pivot_wider",
    "href": "index.html#pivot_wider",
    "title": "13. Tidy Data",
    "section": "18.2 pivot_wider()",
    "text": "18.2 pivot_wider()\n\n\n\n\n\n\nTipDiagnostic: When Do I Need pivot_wider()?\n\n\n\nAsk yourself:\n\nAre multiple variables stacked in one column? (e.g., var + val)\nIs each observation split across multiple rows?\n\nIf yes, you likely need pivot_wider().\n\n\nThe function pivot_wider() is a tool for transforming data from long to wide format. It is the counterpart to the pivot_longer() function. pivot_wider() allows you to take a set of columns containing key-value pairs and convert them into a wider format, where each unique key value becomes a separate column in the resulting data frame. You can also specify a set of value columns that you want to spread across the new columns created by the key values. With pivot_wider(), you can quickly transform your data from long format into a more intuitive, wide format that is easier to work with in some applications.\nShould your data be too long for a particular application (typically a non-Tidyverse application) or your liking, meaning when individual observations are spread across multiple rows, you will need to use pivot_wider() to rectify the situation. This is generally the case when you have two or more variables stored within the same column, as you will see in SACTN3. This is not terribly common as it would require someone to put quite a bit of time into making a dataframe this way. But never say never. To spread data to become wider, you first tell R what the name of the column is that contains more than one variable, in this case the var column. You then tell R what the name of the column is that contains the values that need to be spread, in this case the val column.\n{r code-tidy-3, eval=FALSE} SACTN3_tidy1 &lt;- SACTN3 %&gt;%    pivot_wider(names_from = \"var\", values_from = \"val\")\n\n\n\n\n\n\nNotePivot Rule of Thumb (and a Quick anti-example)\n\n\n\n\npivot_longer() turns column names → values.\npivot_wider() turns values → column names.\n\nIf you use the wrong one, you either explode the data (too many rows) or flatten it (too many columns) and lose the one-observation-per-row rule.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#separate",
    "href": "index.html#separate",
    "title": "13. Tidy Data",
    "section": "19.1 Separate",
    "text": "19.1 Separate\nLooking at SACTN4a, you see that you no longer have a site and src column. Rather these have been replaced by an index column. This is an efficient way to store these data, but it is not tidy because the site and source of each observation have now been combined into one column (variable). Remember that tidy data calls for each of the things known about the data to be its own variable. To re-create site and src columns, you must separate the index column. There are two options: separate_wider_delim() and separate_wider_position(). What does each do? First you give R the name of the column you want to separate, in this case index. Next you specify what the names of the new columns will be. Remember that because we are creating new column names, you feed these into R within inverted commas. Lastly, you should tell R where to separate the index column. If you look at the data you will see that the values you want to split up are separated with / (including a space), so that is what you need to tell R.\nSACTN4a_tidy &lt;- SACTN4a |&gt; \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \")",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#separating-dates-using-mutate",
    "href": "index.html#separating-dates-using-mutate",
    "title": "13. Tidy Data",
    "section": "19.2 Separating Dates Using mutate()",
    "text": "19.2 Separating Dates Using mutate()\nAlthough the date column represents an example of a date date type or class (a kind of data in its own right), you might also want to split this column into its constituent parts, i.e., create separate columns for day, month, and year. In this case you can spread these components of the date vector into three columns using the mutate() function and some functions in the lubridate package (part of the tidyverse).\nSACTN_tidy2 &lt;- SACTN4a %&gt;% \n  separate_wider_delim(index, names = c(\"site\", \"src\"), delim = \"/ \") %&gt;% \n  mutate(day = lubridate::day(date),\n         month = lubridate::month(date),\n         year = lubridate::year(date))\nNote that when the date is split into component parts the data are no longer tidy (see below).\n\n\n\n\n\n\nWarningDates Are a Semantic Unit\n\n\n\nSplitting dates can be useful for grouping, but it also violates tidy principles by scattering one concept across multiple columns. Use this tactically and re-unite when you are done.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#unite",
    "href": "index.html#unite",
    "title": "13. Tidy Data",
    "section": "19.3 Unite",
    "text": "19.3 Unite\nIt is not uncommon that field/lab instruments split values across multiple columns while they are making recordings. You might sometimes see this with date values where the year, month, and day values are given in different columns. There are uses for the data in this way, though it is not terribly tidy. You usually want the date of any observation to be shown in just one column. If you look at SACTN4b you will see that there is a year, month, day column. To unite() them you must first tell R what you want the united column to be labelled, in this case you will use date. You then list the columns to be united; here this is year, month, day. Lastly, decide if you want the united values to have a separator between them. The standard separator for date values is “-”.\nSACTN4b_tidy &lt;- SACTN4b |&gt; \n  unite(col = \"date\", year, month, day, sep = \"-\")",
    "crumbs": [
      "Home"
    ]
  }
]